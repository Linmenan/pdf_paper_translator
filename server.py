import os
import json
import base64
import uvicorn
import fitz  # PyMuPDF
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
from fastapi.responses import FileResponse
import prompts
import asyncio
from fastapi.responses import StreamingResponse
# å¼•å…¥æ ¸å¿ƒåº“
import workflow_utils as wf

app = FastAPI()

# --- 1. å…¨å±€é…ç½® (ç¡¬ç¼–ç è·¯å¾„) ---
# ç”¨æˆ·ä¸å¯ä¿®æ”¹ï¼Œå‰ç«¯ä¹Ÿä¸æ˜¾ç¤º
CONFIG = {
    "pdf_dir": "./academic_papers",
    "extract_dir": "./extracted_output",
    "llm_dir": "./llm_output",
    "vis_dir": "./vis_output"
}


# å®šä¹‰è¯·æ±‚ä½“æ¨¡å‹ (åœ¨æ–‡ä»¶ä¸Šæ–¹ä½ç½®)
class FeedbackUpdateModel(BaseModel):
    filename: str
    id: int
    hint: str

class FeedbackRerunModel(BaseModel):
    filename: str

# è‡ªåŠ¨åˆ›å»ºç›®å½•
for d in CONFIG.values():
    os.makedirs(d, exist_ok=True)

# æŒ‚è½½é™æ€èµ„æº
app.mount("/static", StaticFiles(directory="static"), name="static")
app.mount("/output", StaticFiles(directory="vis_output"), name="output")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.get("/")
async def read_index():
    # è¿™æ ·è®¿é—® http://localhost:8000 å°±ä¼šç›´æ¥æ˜¾ç¤ºç½‘é¡µ
    return FileResponse("static/index.html")

# --- æ•°æ®æ¨¡å‹ ---
class LayoutData(BaseModel):
    page_index: int
    items: List[Dict[str, Any]] # åŒ…å« rect, type, id, role

class SaveLayoutRequest(BaseModel):
    filename: str
    layout_data: Dict[str, List[Dict[str, Any]]] # Keyæ˜¯é¡µç å­—ç¬¦ä¸²

# --- API: é¦–é¡µæç¤ºè¯ ---
@app.get("/api/config/prompts")
def get_prompts_config():
    """è¿”å›åç«¯çœŸå®ä½¿ç”¨çš„ Promptï¼Œä¾›å‰ç«¯é¢„è§ˆ"""
    return {
        "meta": prompts.SYSTEM_PROMPT_META,
        "header": prompts.SYSTEM_PROMPT_HEADER,
        "body": prompts.SYSTEM_PROMPT_BODY,
        "asset": prompts.SYSTEM_PROMPT_ASSET,
        "correction": prompts.SYSTEM_PROMPT_CORRECTION
    }

# --- API: é¦–é¡µè·å–é¡¹ç›®åˆ—è¡¨ ---
@app.get("/api/papers")
def list_papers():
    """æ‰«æç›®å½•ï¼Œè¿”å›æ‰€æœ‰PDFåŠå…¶ç²¾ç¡®çŠ¶æ€ (ç¼“å­˜çŠ¶æ€ä¼˜å…ˆ)"""
    papers = []
    if not os.path.exists(CONFIG["pdf_dir"]):
        return []
    
    for f in os.listdir(CONFIG["pdf_dir"]):
        if f.lower().endswith(".pdf"):
            raw_name = wf.sanitize_filename(f)
            status = "æœªå¼€å§‹"
            
            # è·¯å¾„å®šä¹‰
            report_path = os.path.join(CONFIG["vis_dir"], raw_name, f"{raw_name}_Report.html")
            result_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_result.txt")
            cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
            context_path = os.path.join(CONFIG["extract_dir"], f"{raw_name}_context.txt")
            
            # --- çŠ¶æ€åˆ¤å®šé€»è¾‘ (ä¼˜å…ˆçº§è°ƒæ•´) ---
            
            # 1. æœ€é«˜ä¼˜å…ˆçº§ï¼šæ£€æŸ¥ Cache JSON çš„å®Œæˆåº¦
            # åªè¦ Cache å­˜åœ¨ï¼Œå°±ä»¥ Cache å†…éƒ¨çš„ä»»åŠ¡çŠ¶æ€ä¸ºå‡†
            is_cache_valid = False
            if os.path.exists(cache_path):
                try:
                    with open(cache_path, 'r', encoding='utf-8') as cf:
                        data = json.load(cf)
                        tasks = data.get("tasks", [])
                        total = len(tasks)
                        success_count = sum(1 for t in tasks if t.get("status") == "success")
                        
                        if total > 0:
                            is_cache_valid = True
                            if success_count < total:
                                # åªè¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼Œæ— è®ºæ˜¯å¦æœ‰ Reportï¼Œéƒ½ç®—â€œç¿»è¯‘ä¸­â€
                                status = f"ç¿»è¯‘ä¸­ ({success_count}/{total})"
                            else:
                                # å…¨éƒ¨å®Œæˆ
                                if os.path.exists(report_path):
                                    status = "å·²å®Œæˆ"
                                else:
                                    status = "ç¿»è¯‘å®Œæˆ"
                except Exception as e:
                    print(f"Error reading cache for {raw_name}: {e}")
            
            # 2. å¦‚æœ Cache ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œæ‰é™çº§æ£€æŸ¥å…¶ä»–æ–‡ä»¶
            if not is_cache_valid:
                if os.path.exists(report_path):
                    # åªæœ‰åœ¨æ²¡æœ‰ active cache çš„æƒ…å†µä¸‹ï¼Œæ‰è®¤ä¸ºæ—§ Report æœ‰æ•ˆ
                    # (è¿™é€šå¸¸å‘ç”Ÿåœ¨ä½ æ‰‹åŠ¨åˆ é™¤äº† llm_output ä½†ä¿ç•™äº† vis_output çš„æƒ…å†µ)
                    status = "å·²å®Œæˆ" 
                elif os.path.exists(result_path):
                    status = "ç¿»è¯‘å®Œæˆ"
                elif os.path.exists(context_path):
                    status = "å·²æå–"
            
            papers.append({
                "filename": f,
                "raw_name": raw_name,
                "status": status
            })
    return papers

# --- API: è·å– PDF æŸä¸€é¡µçš„å›¾ç‰‡ (ç”¨äºå‰ç«¯ Canvas èƒŒæ™¯) ---
@app.get("/api/pdf/{filename}/page/{page_idx}")
def get_pdf_page_image(filename: str, page_idx: int):
    pdf_path = os.path.join(CONFIG["pdf_dir"], filename)
    if not os.path.exists(pdf_path):
        raise HTTPException(404, "PDF not found")
    
    doc = fitz.open(pdf_path)
    if page_idx < 0 or page_idx >= len(doc):
        raise HTTPException(400, "Page index out of range")
        
    page = doc[page_idx]
    # 2å€ç¼©æ”¾ä»¥ä¿è¯å‰ç«¯æ¸…æ™°åº¦
    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2)) 
    img_data = pix.tobytes("png")
    base64_str = base64.b64encode(img_data).decode('utf-8')
    
    return {
        "image": f"data:image/png;base64,{base64_str}",
        "width": page.rect.width,
        "height": page.rect.height,
        "total_pages": len(doc)
    }

# --- API: åŠ è½½/ä¿å­˜ å¸ƒå±€ä¿¡æ¯ (JSON) ---
@app.get("/api/layout/{filename}")
def load_layout(filename: str):
    raw_name = wf.sanitize_filename(filename)
    json_path = os.path.join(CONFIG["extract_dir"], raw_name, "layout_config.json")
    if os.path.exists(json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

@app.post("/api/layout/save")
def save_layout(req: SaveLayoutRequest):
    raw_name = wf.sanitize_filename(req.filename)
    target_dir = os.path.join(CONFIG["extract_dir"], raw_name)
    os.makedirs(target_dir, exist_ok=True)
    json_path = os.path.join(target_dir, "layout_config.json")
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(req.layout_data, f, ensure_ascii=False, indent=2)
    return {"status": "success"}

# [æ–°å¢] API: è·å–æå–åçš„ä»»åŠ¡åˆ—è¡¨ (Step 2 ä½¿ç”¨)
@app.get("/api/extract/{filename}")
def get_extract_data(filename: str):
    raw_name = wf.sanitize_filename(filename)
    
    # 1. å°è¯•è¯»å–ç°æœ‰çš„ç¼“å­˜ (è¿›åº¦ä¼˜å…ˆ)
    cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data 
        except Exception as e:
            print(f"Cache read error: {e}")
            
    # 2. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œå°è¯•ä» Context å®æ—¶æ„å»º (å…œåº•æ–¹æ¡ˆ)
    context_path = os.path.join(CONFIG["extract_dir"], f"{raw_name}_context.txt")
    if os.path.exists(context_path):
        try:
            with open(context_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # [æ ¸å¿ƒä¿®å¤] è¿™é‡Œå¿…é¡»æ¥æ”¶ 4 ä¸ªè¿”å›å€¼ï¼Œå¦åˆ™ä¼šå‘ç”Ÿé”™ä½æˆ–æŠ¥é”™
            # æ—§ä»£ç : tasks, ref_map_str, _ = ... (é”™è¯¯åœ°æŠŠ refs èµ‹ç»™äº† ref_map_str)
            # æ–°ä»£ç :
            tasks, refs, layout, ref_map_str = wf.build_initial_tasks(content)
            
            # æ„é€ ä¸´æ—¶å¯¹è±¡è¿”å›
            return {
                "tasks": tasks,
                "ref_map": ref_map_str, # ç°åœ¨è¿™é‡Œæ˜¯æ­£ç¡®çš„æ˜ å°„è¡¨
                "raw_references": refs,
                "is_temp": True
            }
        except Exception as e:
            # æ‰“å°è¯¦ç»†é”™è¯¯æ–¹ä¾¿è°ƒè¯•
            print(f"Build tasks error: {e}")
            raise HTTPException(500, f"Failed to build from context: {str(e)}")
            
    # 3. éƒ½æ²¡æœ‰
    raise HTTPException(404, "Data not found. Please run Step 1 Extract first.")
    
# --- API: è§¦å‘å·¥ä½œæµ ---
def _run_extract_task(pdf_path, extract_dir, vis_dir):
    # è°ƒç”¨ä¿®æ”¹åçš„ utilsï¼Œskip_ui=True
    wf.extract_text_and_save_assets_smart(pdf_path, extract_dir, vis_dir, skip_ui=True)

@app.post("/api/workflow/extract/{filename}")
def trigger_extract(filename: str):
    pdf_path = os.path.join(CONFIG["pdf_dir"], filename)
    
    try:
        # ç›´æ¥è¿è¡Œï¼Œä¸å†ä½¿ç”¨ background_tasks.add_task
        # è¿™ä¼šé˜»å¡è¯·æ±‚ç›´åˆ°æå–å®Œæˆï¼ˆé€šå¸¸å‡ ç§’åˆ°åå‡ ç§’ï¼‰
        _run_extract_task(pdf_path, CONFIG["extract_dir"], CONFIG["vis_dir"])
        return {"status": "success", "msg": "æå–å®Œæˆ"}
    except Exception as e:
        print(f"Extraction failed: {e}")
        # è¿”å› 500 é”™è¯¯ï¼Œå‰ç«¯ catch åˆ°åä¼šå¼¹çª—æç¤º
        raise HTTPException(status_code=500, detail=f"æå–å¤±è´¥: {str(e)}")

def _run_translate_task(context_path, result_path, cache_path):
    wf.run_smart_analysis(context_path, result_path, cache_path=cache_path)

@app.post("/api/workflow/translate/{filename}")
def trigger_translate(filename: str, background_tasks: BackgroundTasks):
    raw_name = wf.sanitize_filename(filename)
    
    # [æ–°å¢] ç¡®ä¿æ¸…é™¤ä¸Šæ¬¡å¯èƒ½é—ç•™çš„åœæ­¢æ ‡å¿—
    wf.clear_stop(raw_name)
    
    ctx_path = os.path.join(CONFIG["extract_dir"], f"{raw_name}_context.txt")
    res_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_result.txt")
    cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
    
    background_tasks.add_task(_run_translate_task, ctx_path, res_path, cache_path)
    return {"status": "started", "msg": "LLM ç¿»è¯‘ä»»åŠ¡å·²å¯åŠ¨"}

# 2. [æ–°å¢] åœæ­¢æ¥å£
@app.post("/api/workflow/stop/{filename}")
def stop_translate(filename: str):
    raw_name = wf.sanitize_filename(filename)
    wf.request_stop(raw_name) # è®¾ç½®æ ‡å¿—ä½
    return {"status": "success", "msg": "å·²å‘é€åœæ­¢ä¿¡å·"}

async def event_generator(raw_name):
    """SSE ç”Ÿæˆå™¨ï¼šç›‘å¬ Cache æ–‡ä»¶å˜åŒ–å¹¶æ¨é€"""
    cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
    last_mod_time = 0
    
    while True:
        if os.path.exists(cache_path):
            try:
                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œæœ‰å˜åŒ–æ‰è¯»å–
                current_mod_time = os.path.getmtime(cache_path)
                if current_mod_time > last_mod_time:
                    last_mod_time = current_mod_time
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        tasks = data.get("tasks", [])
                        # æ¨é€ JSON å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¿…é¡»æ˜¯ 'data: ...\n\n'
                        yield f"data: {json.dumps(tasks)}\n\n"
                        
                        # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆï¼Œå¦‚æœæ˜¯ï¼Œå‘é€ç»“æŸä¿¡å·
                        if tasks and all(t.get("status") == "success" for t in tasks):
                            yield "event: close\ndata: done\n\n"
                            break
            except Exception as e:
                print(f"SSE Error: {e}")
        
        # æ¯ 1 ç§’æ£€æŸ¥ä¸€æ¬¡æ–‡ä»¶ï¼ˆè¿™æ˜¯åç«¯æ£€æŸ¥ï¼Œæ¯” HTTP è¯·æ±‚è½»é‡å¾—å¤šï¼‰
        await asyncio.sleep(1)

@app.get("/api/stream/translation/{filename}")
async def stream_translation_progress(filename: str):
    raw_name = wf.sanitize_filename(filename)
    return StreamingResponse(event_generator(raw_name), media_type="text/event-stream")

@app.post("/api/workflow/generate_report/{filename}")
def generate_report(filename: str):
    raw_name = wf.sanitize_filename(filename)
    res_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_result.txt")
    vis_base = os.path.join(CONFIG["vis_dir"], raw_name)
    
    try:
        report_path = wf.generate_html_report(res_path, vis_base)
        # è¿”å›ç›¸å¯¹è·¯å¾„ä¾›å‰ç«¯ iframe è®¿é—®
        rel_path = f"/output/{raw_name}/{raw_name}_Report.html"
        return {"status": "success", "url": rel_path}
    except Exception as e:
        return {"status": "error", "msg": str(e)}

# 3. æ·»åŠ æ–°çš„ API è·¯ç”± 
# [æ–°å¢] ä¿å­˜ç”¨æˆ·çº é”™åé¦ˆ
@app.post("/api/feedback/update")
def update_feedback(data: FeedbackUpdateModel):
    # æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶çš„æ˜¯ raw_nameï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ HTML é‡Œæ³¨å…¥çš„å°±æ˜¯ raw_name
    raw_name = data.filename 
    task_id = data.id
    user_hint = data.hint
    
    cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
    
    if not os.path.exists(cache_path):
        return {"status": "error", "msg": "Cache file not found"}
        
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
            
        found = False
        for task in cache_data.get("tasks", []):
            if task["id"] == task_id:
                # æ ¸å¿ƒé€»è¾‘ï¼šä¿å­˜ hintï¼Œæ¸…ç©º transï¼Œæ ‡è®°ä¸º pending
                task['old_trans'] = task.get('trans', '') # å¤‡ä»½æ—§è¯‘æ–‡
                task['user_hint'] = user_hint
                task['status'] = 'pending' # æ ‡è®°ä¸ºå¾…é‡è¯‘
                task['trans'] = ""         # æ¸…ç©ºè¯‘æ–‡
                found = True
                break
        
        if found:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            return {"status": "success"}
        else:
            return {"status": "error", "msg": "Task ID not found"}
            
    except Exception as e:
        return {"status": "error", "msg": str(e)}

# [æ–°å¢] è§¦å‘é‡è¯‘å¹¶æ›´æ–°æŠ¥å‘Š
# å®šä¹‰ä¸€ä¸ªåå°åŒ…è£…å‡½æ•°ï¼Œè·‘å®Œç¿»è¯‘åç«‹å³é‡æ–°ç”ŸæˆæŠ¥å‘Š
def _run_rerun_task(context_path, result_path, cache_path, vis_dir):
    # 1. è¿è¡Œ LLM (åªè·‘ pending çš„ä»»åŠ¡)
    wf.run_smart_analysis(context_path, result_path, cache_path=cache_path)
    # 2. ç¿»è¯‘ç»“æŸåï¼Œç«‹å³é‡æ–°ç”Ÿæˆ HTML (ç¡®ä¿æ­£åˆ™é“¾æ¥å’Œæ ·å¼åº”ç”¨)
    wf.generate_html_report(result_path, vis_dir)

@app.post("/api/feedback/rerun")
def rerun_feedback(data: FeedbackRerunModel, background_tasks: BackgroundTasks):
    raw_name = wf.sanitize_filename(data.filename)
    
    # æ„é€ è·¯å¾„
    context_path = os.path.join(CONFIG["extract_dir"], f"{raw_name}_context.txt")
    result_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_result.txt")
    cache_path = os.path.join(CONFIG["llm_dir"], f"{raw_name}_llm_cache.json")
    vis_dir = os.path.join(CONFIG["vis_dir"], raw_name)
    
    # [ä¿®æ”¹] ä¹‹å‰æ˜¯ç›´æ¥è¿è¡Œå¹¶ç­‰å¾…ï¼Œç°åœ¨æ”¹ä¸ºæ·»åŠ åˆ°åå°ä»»åŠ¡
    # è¿™æ ·å‰ç«¯å¯ä»¥ç«‹å³æ”¶åˆ°å“åº”ï¼Œå¹¶å¼€å§‹ SSE ç›‘å¬
    wf.clear_stop(raw_name) # æ¸…é™¤ä¹‹å‰çš„åœæ­¢æ ‡å¿—
    background_tasks.add_task(_run_rerun_task, context_path, result_path, cache_path, vis_dir)
    
    return {"status": "started", "msg": "åå°é‡è¯‘ä»»åŠ¡å·²å¯åŠ¨"}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Web æœåŠ¡: http://localhost:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)