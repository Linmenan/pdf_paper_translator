<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><title>Zhang_CarPlanner_Consistent_Auto-regressive_Trajectory_Planning_for_Large-Scale_Reinforcement_Learning_in_CVPR_2025_paper - Interactive Mode</title><style>
    :root { --primary: #2c3e50; --accent: #3498db; --bg: #f8f9fa; --border: #e0e0e0; --edit-bg: #fff3e0; --edit-border: #ffb74d; --edit-hover: #ffe0b2; }
    body { font-family: "Segoe UI", sans-serif; margin: 0; background: var(--bg); padding-bottom: 100px; scroll-behavior: smooth; }
    .container { max-width: 1200px; margin: 0 auto; background: #fff; box-shadow: 0 0 20px rgba(0,0,0,0.05); border: 2px solid transparent; transition: border 0.3s; }
    
    /* === çº é”™æ¨¡å¼è§†è§‰çŠ¶æ€ (New) === */
    /* 1. å®¹å™¨è¾¹æ¡†å˜æ©™è‰²ï¼Œæç¤ºâ€œæ­£åœ¨ç¼–è¾‘çŠ¶æ€â€ */
    body.feedback-mode .container { border: 2px solid var(--edit-border); box-shadow: 0 0 30px rgba(255, 166, 0, 0.15); }
    
    /* 2. åªæœ‰ã€è¯‘æ–‡æ ¼å­ã€‘å˜è‰²ï¼Œå…¶ä»–ä¿æŒç™½è‰² */
    body.feedback-mode .col-trans { 
        cursor: pointer; 
        background-color: var(--edit-bg) !important; /* æš–é»„è‰²èƒŒæ™¯ */
        border-left: 2px solid transparent;
    }
    
    /* 3. æ‚¬åœæ•ˆæœåŠ æ·± */
    body.feedback-mode .col-trans:hover { 
        background-color: var(--edit-hover) !important; /* æ›´æ·±çš„é»„è‰² */
        border-left: 2px solid #e67e22;
    }
    
    /* 4. æç¤ºå¾½ç«  */
    .hint-badge { display: none; margin-top: 10px; padding: 5px 10px; background: #fff3cd; border: 1px solid #ffeeba; color: #856404; font-size: 0.85em; border-radius: 4px; }
    body.feedback-mode .hint-badge.has-hint { display: block !important; animation: slideDown 0.3s; }
    
    .asset-img { max-width: 100%; height: auto; display: block; margin: 0 auto; }
    .asset-card { background: #fff; max-width: 95%; margin: 0 auto; border-radius: 8px; padding: 10px; text-align: center; }
    .ref-section { padding: 40px; background: #fff; border-top: 2px solid #eee; }
    .ref-content { font-family: "Times New Roman", serif; color: #444; line-height: 1.6; }
    .ref-anchor { color: var(--accent); font-weight: bold; }
    
    .meta-section { padding: 40px; text-align: center; background: #fff; }
    .meta-title-en { font-size: 1.8em; color: #2c3e50; font-weight: 700; }
    .meta-title-zh { font-size: 1.6em; color: #34495e; font-weight: 400; }
    .meta-author-en { font-style: italic; color: #7f8c8d; }
    .meta-author-zh { color: #16a085; font-weight: bold; }
    
    .toolbar { position: fixed; top: 20px; right: 20px; background: #fff; padding: 10px 20px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border-radius: 8px; z-index: 999; display: flex; gap: 10px; align-items: center; }
    .btn { padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; font-weight: bold; transition: 0.2s; }
    .btn-primary { background: var(--accent); color: #fff; }
    .btn-danger { background: #e74c3c; color: #fff; }
    .btn-success { background: #27ae60; color: #fff; }
    .btn:disabled { background: #ccc; cursor: not-allowed; }
    
    .row-container { border-bottom: 1px solid var(--border); }
    .row { display: flex; }
    .col-src, .col-trans { flex: 1; padding: 20px; transition: all 0.3s; }
    .col-src { border-right: 1px solid var(--border); color: #555; background: #fff; }
    
    .feedback-panel { background: #f1f8ff; padding: 15px 20px; border-top: 1px solid #d6eaf8; display: none; }
    .feedback-header { font-weight: bold; color: #2c3e50; margin-bottom: 5px; font-size: 0.9em; }
    .feedback-input { width: 100%; height: 60px; padding: 8px; border: 1px solid #bdc3c7; border-radius: 4px; font-family: inherit; margin-bottom: 5px; }
    .status-saved { color: #27ae60; font-weight: bold; margin-left: 10px; display: none; }
    .asset-row { background: #f4f4f4; padding: 20px; display: block; scroll-margin-top: 80px; transition: background 0.5s; }
    
    a.fig-link, a.tab-link, a.eq-link, a.ref-link { color: var(--accent); text-decoration: none; border-bottom: 1px dotted var(--accent); cursor: pointer; }
    a.fig-link:hover, a.tab-link:hover { background: #eaf6ff; }
    
    @keyframes slideDown { from { opacity: 0; transform: translateY(-5px); } to { opacity: 1; transform: translateY(0); } }
    @keyframes highlight-pulse { 0% { background: #fff3cd; } 100% { background: #f4f4f4; } }
    .highlight-asset { animation: highlight-pulse 2s ease-out; }
    </style></head>
    <body>
    
    <div class="toolbar">
        <div id="status-text" style="margin-right: 10px; color: #666;">æµè§ˆæ¨¡å¼</div>
        <button class="btn btn-primary" id="toggle-btn" onclick="toggleFeedbackMode()">è¿›å…¥çº é”™æ¨¡å¼</button>
        <button class="btn btn-success" id="run-btn" onclick="triggerRerun()" style="display:none;">ğŸš€ åº”ç”¨ä¿®æ”¹å¹¶é‡è¯‘</button>
    </div>
    
    <div class="container"><div class="meta-section"><h1 class="meta-title-en">CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving</h1><h1 class="meta-title-zh"></h1><div class="meta-author-en">Dongkun Zhang1,2 Jiaming Liang2 Ke Guo2 Sha Lu1 Qi Wang2 Rong Xiong1,! Zhenwei Miao2,â€  Yue Wang1 1Zhejiang University 2Cainiao Network 1 {zhangdongkun, lusha, rxiong, ywang24}@zju.edu.cn 2 {liangjiaming.ljm, muguo.gk, ruifeng.wq, zhenwei.mzw}@alibaba-inc.com</div><div class="meta-author-zh">å¼ ä¸œå¤1,2 æä½³æ˜2 éƒ­å‡¯2 åˆ˜è1 ç‹å¥‡2 ç†Šè£1,! è‹—æŒ¯å¨2,â€  æ±ªè¶Š1 1æµ™æ±Ÿå¤§å­¦ 2èœé¸Ÿç½‘ç»œ 1 {zhangdongkun, lusha, rxiong, ywang24}@zju.edu.cn 2 {liangjiaming.ljm, muguo.gk, ruifeng.wq, zhenwei.mzw}@alibaba-inc.com</div></div><hr class="meta-divider"><div class="main-content"><div class="row-container" id="task-1"><div class="row text-row"><div class="col-src">[[HEADER: Abstract]]</div><div class="col-trans" id="trans-1"><b>æ‘˜è¦</b><div id="badge-1" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 1)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('1', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-2"><div class="row text-row"><div class="col-src">Trajectory planning is vital for autonomous driving, ensuring safe and efï¬cient navigation in complex environments. While recent learning-based methods, particularly reinforcement learning (RL), have shown promise in speciï¬c scenarios, RL planners struggle with training inefï¬ciencies and managing large-scale, real-world driving scenarios. In this paper, we introduce CarPlanner, a Consistent autoregressive Planner that uses RL to generate multi-modal trajectories.

The auto-regressive structure enables efï¬cient large-scale RL training, while the incorporation of consistency ensures stable policy learning by maintaining coherent temporal consistency across time steps. Moreover, CarPlanner employs a generation-selection framework with an expert-guided reward function and an invariant-view module, simplifying RL training and enhancing policy performance.</div><div class="col-trans" id="trans-2">è½¨è¿¹è§„åˆ’å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ï¼Œç¡®ä¿åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å®‰å…¨é«˜æ•ˆçš„å¯¼èˆªã€‚è™½ç„¶è¿‘æœŸåŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨ç‰¹å®šåœºæ™¯ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†RLè§„åˆ’å™¨åœ¨è®­ç»ƒæ•ˆç‡å’Œç®¡ç†å¤§è§„æ¨¡ã€çœŸå®ä¸–ç•Œçš„é©¾é©¶åœºæ™¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CarPlannerï¼Œè¿™æ˜¯ä¸€ç§ä¸€è‡´çš„è‡ªå›å½’è§„åˆ’å™¨ï¼Œåˆ©ç”¨RLç”Ÿæˆå¤šæ¨¡æ€è½¨è¿¹ã€‚

è‡ªå›å½’ç»“æ„ä½¿å¾—å¤§è§„æ¨¡RLè®­ç»ƒæ›´åŠ é«˜æ•ˆï¼Œè€Œä¸€è‡´æ€§æœºåˆ¶é€šè¿‡ä¿æŒæ—¶é—´æ­¥é•¿ä¹‹é—´çš„è¿è´¯æ€§æ¥ç¡®ä¿ç¨³å®šçš„ç­–ç•¥å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒCarPlanneré‡‡ç”¨äº†ä¸€ç§ç”Ÿæˆ-é€‰æ‹©æ¡†æ¶ï¼Œå¹¶ç»“åˆäº†ä¸“å®¶å¼•å¯¼çš„å¥–åŠ±å‡½æ•°å’Œä¸å˜è§†å›¾æ¨¡å—ï¼Œç®€åŒ–äº†RLè®­ç»ƒå¹¶æå‡äº†ç­–ç•¥æ€§èƒ½ã€‚<div id="badge-2" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 2)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('2', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-3"><div class="row text-row"><div class="col-src">Extensive analysis demonstrates that our proposed RL framework effectively addresses the challenges of training efï¬ciency and performance enhancement, positioning CarPlanner as a promising solution for trajectory planning in autonomous driving. To the best of our knowledge, we are the ï¬rst to demonstrate that the RL-based planner can surpass both IL- and rule-based state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset nuPlan.

Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA approaches within this demanding dataset.</div><div class="col-trans" id="trans-3">å¹¿æ³›çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶æœ‰æ•ˆåœ°è§£å†³äº†è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½æå‡çš„æŒ‘æˆ˜ï¼Œå°†CarPlannerå®šä½ä¸ºè‡ªä¸»é©¾é©¶ä¸­è½¨è¿¹è§„åˆ’çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†nuPlanä¸Šè¯æ˜åŸºäºRLçš„è§„åˆ’å™¨èƒ½å¤Ÿè¶…è¶ŠIL-å’Œè§„åˆ™åŸºçš„ç°æœ‰æœ€ä½³æ–¹æ³•ï¼ˆSOTAsï¼‰çš„ç ”ç©¶å›¢é˜Ÿã€‚

æˆ‘ä»¬çš„CarPlanneråœ¨è¿™ä¸ª demanding æ•°æ®é›†ä¸­è¶…è¿‡äº†åŸºäºRLã€ILå’Œè§„åˆ™çš„æœ€ä½³æ–¹æ³•ã€‚<div id="badge-3" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 3)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('3', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-4"><div class="row text-row"><div class="col-src">[[HEADER: 1. Introduction]]</div><div class="col-trans" id="trans-4"><b>1. å¼•è¨€</b><div id="badge-4" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 4)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('4', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_1</div><img src="./assets/Figure_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 1. Frameworks for multi-step trajectory generation. (a) Initialization-reï¬nement that generates an initial trajectory and reï¬nes it iteratively. (b) Vanilla auto-regressive models that decode subsequent poses sequentially. (c) Our consistent auto-regressive model that integrates time-consistent mode information.</div><div class="asset-desc-zh">å›¾1. å¤šæ­¥è½¨è¿¹ç”Ÿæˆæ¡†æ¶ã€‚ (a) åˆå§‹åŒ–-ç»†åŒ–ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆåˆå§‹è½¨è¿¹å¹¶è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚ (b) ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒä»¬æŒ‰é¡ºåºè§£ç åç»­å§¿æ€ã€‚ (c) æˆ‘ä»¬çš„ä¸€è‡´æ€§è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒæ•´åˆäº†æ—¶é—´ä¸€è‡´æ¨¡å¼ä¿¡æ¯ã€‚</div></div></div></div><div class="row-container" id="task-5"><div class="row text-row"><div class="col-src">Trajectory planning [36] is essential in autonomous driving, utilizing outputs from perception and trajectory prediction modules to generate future poses for the ego vehicle. A controller tracks this planned trajectory, producing control commands for closed-loop driving. Recently, learningbased trajectory planning has garnered attention due to its

potential to automate algorithm iteration, eliminate tedious rule design, and ensure safety and comfort in diverse realworld scenarios [36].

Most existing researches [11, 16, 29] employ imitation learning (IL) to align planned trajectories with those of human experts. However, this approach suffers from distribution shift [28] and causal confusion [8]. Reinforcement learning (RL) offers a potential solution, addressing these challenges and providing richer supervision through reward functions.</div><div class="col-trans" id="trans-5">è½¨è¿¹è§„åˆ’<a href="#ref-36" class="ref-link">[36]</a>åœ¨è‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ï¼Œå®ƒåˆ©ç”¨æ„ŸçŸ¥å’Œè½¨è¿¹é¢„æµ‹æ¨¡å—çš„è¾“å‡ºæ¥ç”Ÿæˆ ego è½¦è¾†çš„æœªæ¥å§¿æ€ã€‚æ§åˆ¶å™¨è·Ÿè¸ªè®¡åˆ’çš„è½¨è¿¹ï¼Œä»è€Œäº§ç”Ÿé—­ç¯é©¾é©¶æ‰€éœ€çš„æ§åˆ¶å‘½ä»¤ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå­¦ä¹ çš„è½¨è¿¹è§„åˆ’å› å…¶èƒ½å¤Ÿè‡ªåŠ¨åŒ–ç®—æ³•è¿­ä»£ã€æ¶ˆé™¤ç¹ççš„æ‰‹åŠ¨è§„åˆ™è®¾è®¡å¹¶ç¡®ä¿åœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„å®‰å…¨æ€§å’Œèˆ’é€‚æ€§è€Œå—åˆ°å…³æ³¨<a href="#ref-36" class="ref-link">[36]</a>ã€‚

å¤§å¤šæ•°ç°æœ‰ç ”ç©¶<a href="#ref-11" class="ref-link">[11, 16, 29]</a>é‡‡ç”¨æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ¥ä½¿è®¡åˆ’çš„è½¨è¿¹ä¸äººç±»ä¸“å®¶çš„è½¨è¿¹å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šé­å—åˆ†å¸ƒåç§»<a href="#ref-28" class="ref-link">[28]</a>å’Œå› æœæ··æ·†<a href="#ref-8" class="ref-link">[8]</a>çš„é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¥–åŠ±å‡½æ•°è§£å†³äº†è¿™äº›é—®é¢˜å¹¶æä¾›äº†æ›´ä¸°å¯Œçš„ç›‘ç£ã€‚<div id="badge-5" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 5)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('5', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-6"><div class="row text-row"><div class="col-src">Although RL shows effectiveness in domains such as games [34], robotics [19], and language models [25], it still struggles with training inefï¬ciencies and performance issues in the large-scale driving task. To the extent of our knowledge, no RL methods have yet achieved competitive results on large-scale open datasets such as nuPlan [2], which features diverse real-world scenarios.

Thus, this paper aims to tackle two key challenges in RL for trajectory planning: 1) training inefï¬ciency and 2) poor performance. Training inefï¬ciency arises from the fact that RL typically operates in a model-free setting, necessitating an inefï¬cient simulator running on a CPU to repeatedly roll out a policy for data collection. To overcome this challenge, we propose an efï¬cient model-based approach utilizing neural networks as transition models.

Our method is optimized for execution on hardware accelerators such as GPUs, rendering our time cost comparable to that of ILbased methods.</div><div class="col-trans" id="trans-6">å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯¸å¦‚æ¸¸æˆ<a href="#ref-34" class="ref-link">[34]</a>ã€æœºå™¨äººå­¦<a href="#ref-19" class="ref-link">[19]</a>å’Œè¯­è¨€æ¨¡å‹<a href="#ref-25" class="ref-link">[25]</a>ç­‰é¢†åŸŸæ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œä½†åœ¨å¤§è§„æ¨¡é©¾é©¶ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½é—®é¢˜çš„æŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰è¿˜æ²¡æœ‰ä»»ä½•RLæ–¹æ³•èƒ½å¤Ÿåœ¨nuPlan<a href="#ref-2" class="ref-link">[2]</a>è¿™æ ·çš„å¤§å‹å¼€æ”¾æ•°æ®é›†ä¸Šå–å¾—ä¸ä¹‹ç«äº‰çš„ç»“æœï¼Œè€ŒnuPlanåˆ™åŒ…å«äº†å¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚

å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ åœ¨è½¨è¿¹è§„åˆ’ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œ2ï¼‰æ€§èƒ½ä¸ä½³ã€‚è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„åŸå› åœ¨äºRLé€šå¸¸åœ¨ä¸€ä¸ªæ— æ¨¡å‹çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œéœ€è¦ä¾èµ–äºCPUä¸Šçš„ä¸€ä¸ªä½æ•ˆæ¨¡æ‹Ÿå™¨åå¤å±•å¼€ç­–ç•¥ä»¥æ”¶é›†æ•°æ®ã€‚ä¸ºå…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¥ç»ç½‘ç»œä½œä¸ºè¿‡æ¸¡æ¨¡å‹çš„é«˜æ•ˆåŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚

æˆ‘ä»¬çš„æ–¹æ³•é’ˆå¯¹ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆå¦‚GPUï¼‰è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—æˆ‘ä»¬çš„è®¡ç®—æ—¶é—´æˆæœ¬ä¸åŸºäºç¤ºä¾‹å­¦ä¹ ï¼ˆILï¼‰çš„æ–¹æ³•ç›¸å½“ã€‚<div id="badge-6" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 6)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('6', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-7"><div class="row text-row"><div class="col-src">To apply RL to solve the trajectory planning problem, we formulate it as a multi-step sequential decision-making task utilizing a Markov Decision Process (MDP). Existing methods that generate the trajectoryâ€  in multiple steps generally fall into two categories: initialization-reï¬nement [17, 20, 33, 45] and auto-regressive models [27, 32, 41, 46].

The ï¬rst category, illustrated in Fig. 1 (a), involves generating an initial trajectory estimate and subsequently reï¬ning it through iterative applications of RL. However, recent studies, including Gen-Drive [18], suggest that it continues to lag behind SOTA IL and rule-based planners. One notable limitation of this approach is its neglect of the temporal causality inherent in the trajectory planning task.</div><div class="col-trans" id="trans-7">ä¸ºäº†å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºè§£å†³è½¨è¿¹è§„åˆ’é—®é¢˜ï¼Œæˆ‘ä»¬å°†è¯¥é—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¤šæ­¥åºè´¯å†³ç­–ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚ç°æœ‰çš„ç”Ÿæˆå¤šæ­¥éª¤è½¨è¿¹çš„æ–¹æ³•é€šå¸¸å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šåˆå§‹åŒ–-ç»†åŒ–<a href="#ref-17" class="ref-link">[17, 20, 33, 45]</a>å’Œè‡ªå›å½’æ¨¡å‹<a href="#ref-27" class="ref-link">[27, 32, 41, 46]</a>ã€‚

ç¬¬ä¸€ç±»æ–¹æ³•ï¼Œå¦‚<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">å›¾1</a>(a)æ‰€ç¤ºï¼Œæ¶‰åŠé¦–å…ˆç”Ÿæˆä¸€ä¸ªåˆå§‹çš„è½¨è¿¹ä¼°è®¡ï¼Œå¹¶é€šè¿‡è¿­ä»£åº”ç”¨RLå¯¹å…¶è¿›è¡Œæ”¹è¿›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬Gen-Drive <a href="#ref-18" class="ref-link">[18]</a>ï¼Œè¡¨æ˜è¿™ç§æ–¹æ³•ä»ç„¶è½åäºå½“å‰æœ€ä½³çš„å³æ—¶å­¦ä¹ ï¼ˆILï¼‰å’ŒåŸºäºè§„åˆ™çš„è§„åˆ’å™¨ã€‚è¿™ç§åšæ³•çš„ä¸€ä¸ªæ˜¾è‘—å±€é™æ€§æ˜¯å®ƒå¿½è§†äº†è½¨è¿¹è§„åˆ’ä»»åŠ¡ä¸­å›ºæœ‰çš„æ—¶é—´å› æœå…³ç³»ã€‚<div id="badge-7" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 7)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('7', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-8"><div class="row text-row"><div class="col-src">Additionally, the complexity of direct optimization over highdimensional trajectory space can hinder the performance of RL algorithms. The second category consists of autoregressive models, shown in Fig. 1 (b), which generate the poses of the ego vehicle recurrently using a singlestep policy within a transition model. In this category, ego poses at all time steps are consolidated to form the overall planned trajectory.

As taking temporal causality into account, current auto-regressive models allow for interactive behaviors. However, a common limitation is their reliance on auto-regressively random sampling from action distributions to generate multi-modal trajectories. This vanilla auto-regressive procedure may compromise long-term consistency and unnecessarily expand the exploration space in RL, leading to poor performance.</div><div class="col-trans" id="trans-8">æ­¤å¤–ï¼Œç›´æ¥åœ¨é«˜ç»´è½¨è¿¹ç©ºé—´ä¸Šè¿›è¡Œä¼˜åŒ–çš„å¤æ‚æ€§å¯èƒ½ä¼šé˜»ç¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•çš„è¡¨ç°ã€‚ç¬¬äºŒç±»æ–¹æ³•åŒ…æ‹¬è‡ªå›å½’æ¨¡å‹ï¼Œå¦‚<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">å›¾1</a>(b)æ‰€ç¤ºï¼Œè¿™äº›æ¨¡å‹é€šè¿‡åœ¨ä¸€ä¸ªè½¬æ¢æ¨¡å‹å†…ä½¿ç”¨å•æ­¥ç­–ç•¥åå¤ç”Ÿæˆ ego è½¦è¾†çš„å§¿æ€ã€‚åœ¨è¿™ä¸ªç±»åˆ«ä¸­ï¼Œæ‰€æœ‰æ—¶é—´æ­¥éª¤ä¸­çš„ ego å§¿æ€è¢«åˆå¹¶ä»¥å½¢æˆæ•´ä½“è®¡åˆ’è½¨è¿¹ã€‚

è€ƒè™‘åˆ°æ—¶é—´å› æœæ€§ï¼Œå½“å‰çš„è‡ªå›å½’æ¨¡å‹èƒ½å¤Ÿæ”¯æŒäº¤äº’è¡Œä¸ºã€‚ç„¶è€Œï¼Œä¸€ä¸ªå¸¸è§çš„é™åˆ¶æ˜¯å®ƒä»¬ä¾èµ–äºä»åŠ¨ä½œåˆ†å¸ƒä¸­è‡ªå›å½’åœ°è¿›è¡Œéšæœºé‡‡æ ·æ¥ç”Ÿæˆå¤šæ¨¡æ€è½¨è¿¹ã€‚è¿™ç§åŸºæœ¬çš„è‡ªå›å½’è¿‡ç¨‹å¯èƒ½ä¼šæŸå®³é•¿æœŸä¸€è‡´æ€§ï¼Œå¹¶ä¸”æ— è°“åœ°æ‰©å¤§äº†åœ¨ RL ä¸­çš„æ¢ç´¢ç©ºé—´ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚<div id="badge-8" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 8)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('8', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-9"><div class="row text-row"><div class="col-src">To address the limitations of auto-regressive models, we introduce CarPlanner, a Consistent auto-regressive model designed for efï¬cient, large-scale RL-based Planner training (see Fig. 1 (c)). The key insight of CarPlanner is its incorporation of consistent mode representation as conditions for the auto-regressive model.

Speciï¬cally, we leverage a longitudinal-lateral decomposed mode representation, where the longitudinal mode is a scalar that captures average speeds, and the lateral mode encompasses all possible routes derived from the current state of the ego vehicle along with map information. This mode remains constant across time steps, providing stable and consistent guidance during policy sampling.</div><div class="col-trans" id="trans-9">ä¸ºäº†è§£å†³è‡ªå›å½’æ¨¡å‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†CarPlannerï¼Œè¿™æ˜¯ä¸€ç§ä¸€è‡´æ€§çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§„åˆ’å™¨è®­ç»ƒï¼ˆè§<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">å›¾1</a>(c)ï¼‰ã€‚CarPlannerçš„å…³é”®æ´å¯Ÿåœ¨äºå…¶å°†ä¸€è‡´çš„æ¨¡å¼è¡¨ç¤ºä½œä¸ºè‡ªå›å½’æ¨¡å‹çš„æ¡ä»¶ã€‚

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åˆ©ç”¨çºµå‘-æ¨ªå‘åˆ†è§£çš„æ¨¡å¼è¡¨ç¤ºæ–¹å¼ã€‚å…¶ä¸­ï¼Œçºµå‘æ¨¡å¼æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œç”¨äºæ•æ‰å¹³å‡é€Ÿåº¦ï¼›è€Œæ¨ªå‘æ¨¡å¼åˆ™æ¶µç›–äº†ä»å½“å‰egoè½¦è¾†çŠ¶æ€å‡ºå‘çš„æ‰€æœ‰å¯èƒ½è·¯å¾„ï¼Œå¹¶ç»“åˆäº†åœ°å›¾ä¿¡æ¯ã€‚è¿™ç§æ¨¡å¼åœ¨æ—¶é—´æ­¥ä¸Šä¿æŒä¸€è‡´ï¼Œä¸ºç­–ç•¥é‡‡æ ·æä¾›ç¨³å®šçš„å’Œä¸€è‡´çš„æŒ‡å¯¼ã€‚<div id="badge-9" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 9)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('9', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-10"><div class="row text-row"><div class="col-src">Furthermore, we propose a universal reward function that suits large-scale and diverse scenarios, eliminating the need for scenario-speciï¬c reward designs. This function consists of an expert-guided and task-oriented term. The ï¬rst term quantiï¬es the displacement error between the egoplanned trajectory and the expertâ€™s trajectory, which, along with the consistent mode representation, narrows down the policyâ€™s exploration space.

The second term incorporates common senses in driving tasks including the avoidance of collision and adherence to the drivable area. Additionally, we introduce an Invariant-View Module (IVM) to supply invariant-view input for policy, with the aim of providing time-agnostic policy input, easing the feature learning and embracing generalization.

To achieve this, IVM preprocesses state and lateral mode by transforming agent, map, and route information into the egoâ€™s current coordinate and by clipping information that is distant from the ego.</div><div class="col-trans" id="trans-10">æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„å¥–åŠ±å‡½æ•°ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æƒ…æ™¯ï¼Œä»è€Œæ¶ˆé™¤äº†é’ˆå¯¹ç‰¹å®šåœºæ™¯è®¾è®¡å¥–åŠ±çš„éœ€æ±‚ã€‚è¯¥å¥–åŠ±å‡½æ•°ç”±ä¸“å®¶æŒ‡å¯¼çš„ä»»åŠ¡å¯¼å‘é¡¹ç»„æˆã€‚ç¬¬ä¸€é¡¹é‡åŒ–äº†è‡ªè§„åˆ’è½¨è¿¹ä¸ä¸“å®¶è½¨è¿¹ä¹‹é—´çš„ä½ç§»è¯¯å·®ï¼Œå¹¶ç»“åˆä¸€è‡´çš„æ¨¡å¼è¡¨ç¤ºï¼Œç¼©å°äº†ç­–ç•¥çš„æ¢ç´¢ç©ºé—´ã€‚

ç¬¬äºŒé¡¹åˆ™èå…¥äº†é©¾é©¶ä»»åŠ¡ä¸­çš„å¸¸è¯†æ€§å› ç´ ï¼ŒåŒ…æ‹¬é¿å…ç¢°æ’å’Œéµå®ˆå¯è¡Œé©¶åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸å˜è§†å›¾æ¨¡å—ï¼ˆInvariant-View Module, IVMï¼‰ï¼Œä¸ºç­–ç•¥æä¾›ä¸å˜è§†å›¾è¾“å…¥ï¼Œæ—¨åœ¨æä¾›æ—¶é—´æ— å…³æ€§çš„ç­–ç•¥è¾“å…¥ï¼Œç®€åŒ–ç‰¹å¾å­¦ä¹ å¹¶ä¿ƒè¿›æ³›åŒ–ã€‚

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒIVM é¢„å¤„ç†çŠ¶æ€å’Œæ¨ªå‘æ¨¡å¼ï¼Œé€šè¿‡å°†ä»£ç†ã€åœ°å›¾å’Œè·¯çº¿ä¿¡æ¯è½¬æ¢ä¸ºè‡ªè½¦å½“å‰åæ ‡ï¼Œå¹¶è£å‰ªè¿œç¦»è‡ªè½¦çš„ä¿¡æ¯æ¥å®Œæˆã€‚<div id="badge-10" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 10)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('10', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-11"><div class="row text-row"><div class="col-src">To our knowledge, we are the ï¬rst to demonstrate that RL-based planner outperforms state-of-the-art (SOTA) IL and rule-based approaches on the challenging large-scale nuPlan dataset.

In summary, the key contributions of this paper are highlighted as follows: â€¢ We present CarPlanner, a consistent auto-regressive planner that trains an RL policy to generate consistent multi-modal trajectories. â€¢ We introduce an expert-guided universal reward function and IVM to simplify RL training and improve policy generalization, leading to enhanced closed-loop performance. â€¢ We conduct a rigorous analysis on the characteristics of IL and RL training, providing insights into their strengths and limitations, while highlighting the advantages of RL in tackling challenges such as distribution shift and causal confusion. â€¢ Our framework showcases exceptional performance, surpassing all RL-, IL-, and rule-based SOTAs on the nuPlan benchmark.</div><div class="col-trans" id="trans-11">æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªè¯æ˜åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§„åˆ’å™¨åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤§è§„æ¨¡nuPlanæ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å®ä¾‹çº§å­¦ä¹ ï¼ˆSOTA ILï¼‰å’Œè§„åˆ™åŸºç¡€æ–¹æ³•çš„ç ”ç©¶å›¢é˜Ÿã€‚

æ€»ç»“æ¥è¯´ï¼Œæœ¬æ–‡çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼š
- æˆ‘ä»¬æå‡ºäº†CarPlannerï¼Œè¿™æ˜¯ä¸€ç§ä¸€è‡´çš„è‡ªå›å½’è§„åˆ’å™¨ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªRLç­–ç•¥æ¥ç”Ÿæˆä¸€è‡´çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚
- æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸“å®¶æŒ‡å¯¼ä¸‹çš„é€šç”¨å¥–åŠ±å‡½æ•°å’ŒIVMï¼ˆå†…éƒ¨éªŒè¯æœºåˆ¶ï¼‰ï¼Œç®€åŒ–äº†RLè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜äº†ç­–ç•¥æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºäº†é—­ç¯æ€§èƒ½ã€‚
- æˆ‘ä»¬å¯¹ILå’ŒRLè®­ç»ƒçš„ç‰¹ç‚¹è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†æï¼Œæä¾›äº†å®ƒä»¬å„è‡ªä¼˜åŠ¿å’Œå±€é™æ€§çš„è§è§£ï¼Œå¹¶çªæ˜¾äº†åœ¨åº”å¯¹åˆ†å¸ƒåç§»å’Œå› æœæ··æ·†ç­‰æŒ‘æˆ˜æ–¹é¢RLçš„ä¼˜åŠ¿ã€‚
- è¯¥æ¡†æ¶å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰åŸºäºRLã€ILå’Œè§„åˆ™çš„æ–¹æ³•ã€‚<div id="badge-11" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 11)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('11', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-12"><div class="row text-row"><div class="col-src">This underscores the potential of RL in navigating complex real-world driving scenarios.</div><div class="col-trans" id="trans-12">è¿™å‡¸æ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¯¼èˆªå¤æ‚çœŸå®ä¸–ç•Œé©¾é©¶åœºæ™¯æ–¹é¢çš„æ½œåŠ›ã€‚<div id="badge-12" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 12)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('12', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-13"><div class="row text-row"><div class="col-src">[[HEADER: 2. Related Work]]</div><div class="col-trans" id="trans-13"><b>2. ç›¸å…³å·¥ä½œ</b><div id="badge-13" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 13)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('13', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-14"><div class="row text-row"><div class="col-src">[[HEADER: 2.1. Imitation-based Planning]]</div><div class="col-trans" id="trans-14"><b>2.1. åŸºäºæ¨¡ä»¿çš„è§„åˆ’</b><div id="badge-14" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 14)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('14', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_2</div><img src="./assets/Figure_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 2. CarPlanner contains four parts. (1) The non-reactive transition model takes initial state s0 as input and predicts the future trajectories of trafï¬c agents. (2) The mode selector outputs scores based on the initial state and the modes c. (3) The trajectory generator obeys an auto-regressive structure condition on the consistent mode and produces mode-aligned multi-modal trajectories. (4) The rule- augmented selector compensates the mode scores by safety, comfort, and progress metrics.</div><div class="asset-desc-zh">å›¾2. CarPlanner åŒ…å«å››ä¸ªéƒ¨åˆ†ã€‚ (1) éååº”å¼è¿‡æ¸¡æ¨¡å‹æ¥æ”¶åˆå§‹çŠ¶æ€ s0 ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹äº¤é€šä»£ç†çš„æœªæ¥è½¨è¿¹ã€‚ (2) æ¨¡å¼é€‰æ‹©å™¨æ ¹æ®åˆå§‹çŠ¶æ€å’Œæ¨¡å¼ c è¾“å‡ºåˆ†æ•°ã€‚ (3) è½¨è¿¹ç”Ÿæˆå™¨éµå¾ªä¸€è‡´æ¨¡å¼ä¸‹çš„è‡ªå›å½’ç»“æ„æ¡ä»¶ï¼Œäº§ç”Ÿæ¨¡å¼å¯¹é½çš„å¤šæ¨¡æ€è½¨è¿¹ã€‚ (4) è§„åˆ™å¢å¼ºçš„é€‰æ‹©å™¨é€šè¿‡å®‰å…¨ã€èˆ’é€‚æ€§å’Œè¿›å±•åº¦é‡è¡¥å¿æ¨¡å¼å¾—åˆ†ã€‚</div></div></div></div><div class="row-container" id="task-15"><div class="row text-row"><div class="col-src">The use of IL to train planners based on human demonstrations has garnered signiï¬cant interest recently. This approach leverages the driving expertise of experienced drivers who can safely and comfortably navigate a wide range of real-world scenarios, along with the added advantage of easily collectible driving data at scale [2, 9, 15]. Nu-

merous studies [5, 17, 29] have focused on developing innovative networks to enhance open-loop performance in this domain. However, the ultimate challenge of autonomous driving is achieving closed-loop operation, which is evaluated using driving-oriented metrics such as safety, adherence to trafï¬c rules, comfort, and progress. This reveals a signiï¬cant gap between the training and testing phases of planners.</div><div class="col-trans" id="trans-15">ä½¿ç”¨ILï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰æ ¹æ®äººç±»ç¤ºèŒƒæ¥è®­ç»ƒè§„åˆ’è€…æœ€è¿‘å¼•èµ·äº†æ˜¾è‘—çš„å…´è¶£ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ç»éªŒä¸°å¯Œçš„é©¾é©¶å‘˜çš„é©¾é©¶ä¸“é•¿ï¼Œä»–ä»¬èƒ½å¤Ÿå®‰å…¨èˆ’é€‚åœ°åº”å¯¹å„ç§çœŸå®ä¸–ç•Œçš„åœºæ™¯ï¼Œå¹¶ä¸”è¿˜å…·æœ‰æ˜“äºå¤§è§„æ¨¡æ”¶é›†é©¾é©¶æ•°æ®çš„ä¼˜åŠ¿<a href="#ref-2" class="ref-link">[2, 9, 15]</a>ã€‚è®¸å¤šç ”ç©¶<a href="#ref-5" class="ref-link">[5, 17, 29]</a>é›†ä¸­åœ¨å¼€å‘åˆ›æ–°ç½‘ç»œä»¥å¢å¼ºæ­¤é¢†åŸŸçš„å¼€ç¯æ€§èƒ½ã€‚ç„¶è€Œï¼Œè‡ªä¸»é©¾é©¶çš„æœ€ç»ˆæŒ‘æˆ˜æ˜¯å®ç°é—­ç¯æ“ä½œï¼Œè¿™é€šå¸¸é€šè¿‡è¯¸å¦‚å®‰å…¨æ€§ã€éµå®ˆäº¤é€šè§„åˆ™ã€èˆ’é€‚æ€§å’Œè¿›å±•ç­‰é©¾é©¶å¯¼å‘çš„æŒ‡æ ‡æ¥è¯„ä¼°ã€‚è¿™æ­ç¤ºäº†è§„åˆ’å™¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µä¹‹é—´çš„ä¸€ä¸ªæ˜¾è‘—å·®è·ã€‚<div id="badge-15" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: æ–‡çŒ®[2, 9, 15]ä¸éœ€è¦ä½¿ç”¨æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œåº”ä¿ç•™åŸæ ¼å¼ </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 15)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">æ–‡çŒ®[2, 9, 15]ä¸éœ€è¦ä½¿ç”¨æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œåº”ä¿ç•™åŸæ ¼å¼</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('15', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-16"><div class="row text-row"><div class="col-src">Moreover, IL is particularly vulnerable to issues such as distribution shift [28] and causal confusion [8]. The ï¬rst issue results in suboptimal decisions when the system encounters scenarios that are not represented in the training data distribution. The second issue arises when networks inadvertently capture incorrect correlations and develop shortcuts based on input information, primarily due to the reliance on imitation loss from expert demonstrations.

Despite efforts in several studies [1, 3, 4, 42] to address these challenges, the gap between training and testing remains substantial.</div><div class="col-trans" id="trans-16">æ­¤å¤–ï¼ŒILç‰¹åˆ«å®¹æ˜“å—åˆ°åˆ†å¸ƒåç§»<a href="#ref-28" class="ref-link">[28]</a>å’Œå› æœæ··æ·†<a href="#ref-8" class="ref-link">[8]</a>ç­‰é—®é¢˜çš„å½±å“ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜ä¼šå¯¼è‡´å½“ç³»ç»Ÿé‡åˆ°è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸­æœªåŒ…å«çš„åœºæ™¯æ—¶åšå‡ºæ¬¡ä¼˜å†³ç­–ã€‚ç¬¬äºŒä¸ªé—®é¢˜åˆ™å‘ç”Ÿåœ¨ç½‘ç»œæ— æ„ä¸­æ•æ‰åˆ°é”™è¯¯çš„ç›¸å…³æ€§ï¼Œå¹¶åŸºäºè¾“å…¥ä¿¡æ¯å¼€å‘æ·å¾„ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¯¹ä¸“å®¶æ¼”ç¤ºä¸­çš„æ¨¡ä»¿æŸå¤±çš„ä¾èµ–ã€‚

å°½ç®¡æœ‰å¤šé¡¹ç ”ç©¶<a href="#ref-1" class="ref-link">[1, 3, 4, 42]</a>è¯•å›¾è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†è®­ç»ƒä¸æµ‹è¯•ä¹‹é—´çš„å·®è·ä»ç„¶å¾ˆå¤§ã€‚<div id="badge-16" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 16)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('16', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-17"><div class="row text-row"><div class="col-src">[[HEADER: 2.2. RL in Autonomous Driving]]</div><div class="col-trans" id="trans-17"><b>2.2. è‡ªåŠ¨é©¾é©¶ä¸­çš„å¼ºåŒ–å­¦ä¹ </b><div id="badge-17" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 17)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('17', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-18"><div class="row text-row"><div class="col-src">In the ï¬eld of autonomous driving, RL has demonstrated its effectiveness in addressing speciï¬c scenarios such as highway driving [22, 39], lane changes [14, 23], and unprotected left turns [22, 40]. Most methods directly learn policies over the control space, which includes throttle, brake, and steering commands. Due to the high frequency of control command execution, the simulation process can be timeconsuming, and exploration can be inconsistent [40].

Several works [40, 44] have proposed learning trajectory planners with actions deï¬ned as ego-planned trajectories, which temporally extend the exploration space and improve training efï¬ciency. However, a trade-off exists between the trajectory horizon and training performance, as noted in ASAP-RL [40].</div><div class="col-trans" id="trans-18">åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«è¯æ˜èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ç‰¹å®šåœºæ™¯ï¼Œå¦‚é«˜é€Ÿå…¬è·¯é©¾é©¶<a href="#ref-22" class="ref-link">[22, 39]</a>ã€å˜é“<a href="#ref-14" class="ref-link">[14, 23]</a>å’Œæ— ä¿æŠ¤å·¦è½¬<a href="#ref-22" class="ref-link">[22, 40]</a>ã€‚å¤§å¤šæ•°æ–¹æ³•ç›´æ¥åœ¨æ§åˆ¶ç©ºé—´ä¸­å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬æ²¹é—¨ã€åˆ¹è½¦å’Œè½¬å‘å‘½ä»¤ã€‚ç”±äºæ§åˆ¶å‘½ä»¤æ‰§è¡Œé¢‘ç‡è¾ƒé«˜ï¼Œæ¨¡æ‹Ÿè¿‡ç¨‹å¯èƒ½ä¼šè€—æ—¶è¾ƒé•¿ï¼Œå¹¶ä¸”æ¢ç´¢å¯èƒ½ä¸ä¸€è‡´<a href="#ref-40" class="ref-link">[40]</a>ã€‚

ä¸€äº›ç ”ç©¶å·¥ä½œ<a href="#ref-40" class="ref-link">[40, 44]</a>æå‡ºäº†ä»¥ ego è®¡åˆ’è½¨è¿¹ä½œä¸ºåŠ¨ä½œçš„å­¦ä¹ è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼Œè¿™åœ¨æ—¶é—´ä¸Šæ‰©å±•äº†æ¢ç´¢ç©ºé—´å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ç„¶è€Œï¼Œè½¨è¿¹çš„å±•æœ›æœŸä¸è®­ç»ƒæ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡å…³ç³»ï¼Œè¿™ä¸€ç‚¹å·²åœ¨ ASAP-RL <a href="#ref-40" class="ref-link">[40]</a> ä¸­æŒ‡å‡ºã€‚<div id="badge-18" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: è¯¥æ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰å‚è€ƒæ–‡çŒ®éƒ½é”™è¯¯çš„ä½¿ç”¨äº†æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œåº”ä¿ç•™åŸæ ¼å¼ </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 18)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">è¯¥æ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰å‚è€ƒæ–‡çŒ®éƒ½é”™è¯¯çš„ä½¿ç”¨äº†æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œåº”ä¿ç•™åŸæ ¼å¼</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('18', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-19"><div class="row text-row"><div class="col-src">Increasing the trajectory horizon results in less reactive behaviors and a reduced amount of data, while a smaller trajectory horizon leads to challenges similar to those encountered in control space. Additionally, these methods typically employ a model-free setting, making them difï¬cult to apply to the complex, diverse realworld scenarios found in large-scale driving datasets. In this paper, we propose adopting a model-based formulation that can facilitate RL training on large-scale datasets.

Under this formulation, we aim to overcome the trajectory horizon trade-off by using a transition model, which can provide a preview of the world in which our policy can make multistep decisions during testing.</div><div class="col-trans" id="trans-19">å¢åŠ è½¨è¿¹é¢„æµ‹çš„æ—¶é•¿ä¼šå‡å°‘ååº”æ€§è¡Œä¸ºå¹¶é™ä½æ•°æ®é‡ï¼Œè€Œè¾ƒå°çš„è½¨è¿¹é¢„æµ‹æ—¶é•¿åˆ™ä¼šå¯¼è‡´ç±»ä¼¼äºæ§åˆ¶ç©ºé—´ä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é‡‡ç”¨æ— æ¨¡å‹è®¾ç½®ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡é©¾é©¶æ•°æ®é›†ä¸­å¤æ‚å¤šæ ·çš„ç°å®åœºæ™¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé‡‡ç”¨åŸºäºæ¨¡å‹çš„æ–¹æ³•æ¥ä¿ƒè¿›å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

åœ¨è¿™ç§æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä½¿ç”¨è½¬æ¢æ¨¡å‹å…‹æœè½¨è¿¹é¢„æµ‹æ—¶é•¿çš„æƒè¡¡é—®é¢˜ã€‚è¯¥è½¬æ¢æ¨¡å‹å¯ä»¥åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¸ºæˆ‘ä»¬çš„ç­–ç•¥æä¾›æœªæ¥ä¸–ç•Œçš„é¢„è§ˆï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥å†³ç­–ã€‚<div id="badge-19" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 19)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('19', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-20"><div class="row text-row"><div class="col-src">[[HEADER: 3. Method]]</div><div class="col-trans" id="trans-20"><b>3. æ–¹æ³•</b><div id="badge-20" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 20)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('20', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-21"><div class="row text-row"><div class="col-src">[[HEADER: 3.1. Preliminaries]]</div><div class="col-trans" id="trans-21"><b>3.1. å‰æœŸå·¥ä½œ</b><div id="badge-21" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 21)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('21', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Equation_1"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_1</div><img src="./assets/Equation_1.png" class="asset-img-raw" loading="lazy"></div></div><div class="row-container" id="task-22"><div class="row text-row"><div class="col-src">MDP is used to model sequential decision problems, formulated as a tuple âŸ¨S, A, PÏ„, R, Ï0, Î³, TâŸ©. S is the state space. A is the action space. PÏ„ : S Ã— A â†’âˆ†(S) â€  is the state transition probability. R : S Ã— A â†’R denotes the reward function and is bounded. Ï0 âˆˆâˆ†(S) is the initial state distribution. T is the time horizon and Î³ is the discount factor of future rewards.

The state-action sequence is deï¬ned as Ï„ = (s0, a0, s1, a1, . . . , sT ), where st âˆˆS and at âˆˆA are the state and action at time step t. The objective of RL is to maximize the expected return:

Vectorized state representation. State st contains map and agent information in vectorized representation [10]. Map information m includes the road network, trafï¬c lights, etc, which are represented by polylines and polygons. Agent information includes the current and past poses of ego vehicle and other trafï¬c agents, which are represented by polylines. The index of ego vehicle is 0 and the indices of trafï¬c agents range from 1 to N.</div><div class="col-trans" id="trans-22">é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ç”¨äºå»ºæ¨¡åºè´¯å†³ç­–é—®é¢˜ï¼Œå½¢å¼åŒ–ä¸ºå…ƒç»„ âŸ¨S, A, PÏ„, R, Ï0, Î³, TâŸ©ã€‚å…¶ä¸­ S æ˜¯çŠ¶æ€ç©ºé—´ã€‚A æ˜¯åŠ¨ä½œç©ºé—´ã€‚PÏ„ : S Ã— A â†’ âˆ†(S)â€  è¡¨ç¤ºçŠ¶æ€è½¬ç§»æ¦‚ç‡ã€‚R : S Ã— A â†’ R ä»£è¡¨å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä¸”æ˜¯æœ‰ç•Œçš„ã€‚Ï0 âˆˆ âˆ†(S) æ˜¯åˆå§‹çŠ¶æ€åˆ†å¸ƒã€‚T æ˜¯æ—¶é—´èŒƒå›´ï¼ŒÎ³ æ˜¯æœªæ¥å¥–åŠ±çš„æŠ˜æ‰£å› å­ã€‚

çŠ¶æ€-åŠ¨ä½œåºåˆ—å®šä¹‰ä¸º Ï„ = (s0, a0, s1, a1, ..., sT)ï¼Œå…¶ä¸­ st âˆˆ S å’Œ at âˆˆ A åˆ†åˆ«è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t çš„çŠ¶æ€å’ŒåŠ¨ä½œã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼š

çŸ¢é‡åŒ–çŠ¶æ€è¡¨ç¤ºã€‚çŠ¶æ€ st åŒ…å«åœ°å›¾å’Œä»£ç†ä¿¡æ¯çš„çŸ¢é‡åŒ–è¡¨ç¤º <a href="#ref-10" class="ref-link">[10]</a>ã€‚åœ°å›¾ä¿¡æ¯ m åŒ…æ‹¬é“è·¯ç½‘ç»œã€äº¤é€šç¯ç­‰ï¼Œè¿™äº›é€šè¿‡å¤šæ®µçº¿å’Œå¤šè¾¹å½¢æ¥è¡¨ç¤ºã€‚ä»£ç†ä¿¡æ¯åŒ…æ‹¬ ego è½¦è¾†åŠå…¶è¿‡å»ä½ç½®å’Œå…¶ä»–äº¤é€šä»£ç†å½“å‰çš„ä½ç½®ï¼Œè¿™äº›ä¹Ÿé€šè¿‡å¤šæ®µçº¿æ¥è¡¨ç¤ºã€‚ego è½¦è¾†çš„ç´¢å¼•ä¸º 0ï¼Œè€Œäº¤é€šä»£ç†çš„ç´¢å¼•èŒƒå›´ä» 1 åˆ° Nã€‚<div id="badge-22" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 22)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('22', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-23"><div class="row text-row"><div class="col-src">For each agent i, its history is denoted as si tâˆ’H:t, i âˆˆ{0, 1, . . . , N}, where H is the history time horizon.</div><div class="col-trans" id="trans-23">å¯¹äºæ¯ä¸ªä»£ç† \(i\)ï¼Œå…¶å†å²è®°å½•è¡¨ç¤ºä¸º \(s_i^{t-H:t}\), \(i \in \{0, 1, \ldots, N\}\)ï¼Œå…¶ä¸­ \(H\) æ˜¯å†å²æ—¶é—´èŒƒå›´ã€‚<div id="badge-23" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 23)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('23', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-24"><div class="row text-row"><div class="col-src">[[HEADER: 3.2. Problem Formulation]]</div><div class="col-trans" id="trans-24"><b>3.2. é—®é¢˜è¡¨è¿°</b><div id="badge-24" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 24)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('24', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Equation_2"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_2</div><img src="./assets/Equation_2.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_3"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_3</div><img src="./assets/Equation_3.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_4"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_4</div><img src="./assets/Equation_4.png" class="asset-img-raw" loading="lazy"></div></div><div class="row-container" id="task-25"><div class="row text-row"><div class="col-src">We model the trajectory planning task as a sequential decision process and decouple the auto-regressive models into policy and transition models. The key to connect trajectory planning and auto-regressive models is to deï¬ne the action as the next pose of ego vehicle, i.e., at = s0 t+1. Therefore, after forwarding the auto-regressive model, the decoded pose is collected to be the ego-planned trajectory.

Speciï¬cally, we can reduce the state-action sequence to the state sequence under this deï¬nition and vectorized representation:

The state sequence can be further formulated in an autoregressive fashion and decomposed into policy and transition model:

From Eq. (3), we can clearly identify the inherent problem associated with the typical auto-regressive approach: inconsistent behaviors across time steps arise from the policy distribution, which depends on random sampling from the action distribution.</div><div class="col-trans" id="trans-25">æˆ‘ä»¬å°†è½¨è¿¹è§„åˆ’ä»»åŠ¡å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å°†è‡ªå›å½’æ¨¡å‹æ‹†åˆ†ä¸ºç­–ç•¥æ¨¡å‹å’Œè½¬æ¢æ¨¡å‹ã€‚è¿æ¥è½¨è¿¹è§„åˆ’ä¸è‡ªå›å½’æ¨¡å‹çš„å…³é”®åœ¨äºå®šä¹‰åŠ¨ä½œegoè½¦è¾†çš„ä¸‹ä¸€å§¿æ€ï¼Œå³ \(a_t = s_{t+1}\)ã€‚å› æ­¤ï¼Œåœ¨å‰å‘ä¼ æ’­è‡ªå›å½’æ¨¡å‹åï¼Œè§£ç å¾—åˆ°çš„å§¿æ€è¢«æ”¶é›†èµ·æ¥ä½œä¸ºegoè½¦è¾†çš„è®¡åˆ’è½¨è¿¹ã€‚

å…·ä½“è€Œè¨€ï¼Œæ ¹æ®è¿™ä¸€å®šä¹‰å’ŒçŸ¢é‡åŒ–è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å°†çŠ¶æ€-åŠ¨ä½œåºåˆ—ç®€åŒ–ä¸ºçŠ¶æ€åºåˆ—ï¼š

çŠ¶æ€åºåˆ—å¯ä»¥è¿›ä¸€æ­¥ä»¥è‡ªå›å½’çš„æ–¹å¼è¿›è¡Œå½¢å¼åŒ–ï¼Œå¹¶åˆ†è§£ä¸ºç­–ç•¥æ¨¡å‹å’Œè½¬æ¢æ¨¡å‹ï¼š

ä»å…¬å¼ï¼ˆ<a href="#Equation_3" class="eq-link" onclick="highlightAsset('Equation_3'); return false;">Equation3ï¼‰</a>ä¸­å¯ä»¥çœ‹å‡ºï¼Œå…¸å‹çš„è‡ªå›å½’æ–¹æ³•æ‰€å›ºæœ‰çš„é—®é¢˜ï¼šæ—¶é—´æ­¥ä¹‹é—´çš„è¡Œä¸ºä¸ä¸€è‡´æ˜¯ç”±ä¾èµ–äºåŠ¨ä½œåˆ†å¸ƒçš„éšæœºé‡‡æ ·çš„ç­–ç•¥åˆ†å¸ƒå¼•èµ·çš„ã€‚<div id="badge-25" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: Eq. (3)åº”è¯¥è¯‘ä¸ºå…¬å¼ï¼ˆEquation3ï¼‰ï¼Œè€Œä¸æ˜¯å…¬å¼(3) </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 25)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">Eq. (3)åº”è¯¥è¯‘ä¸ºå…¬å¼ï¼ˆEquation3ï¼‰ï¼Œè€Œä¸æ˜¯å…¬å¼(3)</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('25', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-26"><div class="row text-row"><div class="col-src">To solve the above problem, we introduce consistent mode information c that remains unchanged across time steps into the auto-regressive fashion:

'

Since we focus on the ego trajectory planning, the consistent mode c does not impact transition model.

This consistent auto-regressive formulation deï¬ned in Eq. (4) reveals a generation-selection framework where the mode selector scores each mode based on the initial state s0 and the trajectory generator generates multi-modal trajectories via sampling from the mode-conditioned policy. Non-reactive transition model. The transition model formulated in Eq. (4) needs to be employed in every time step since it produces the poses of trafï¬c agents at time step t + 1 based on current state st.</div><div class="col-trans" id="trans-26">ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åœ¨æ—¶é—´æ­¥é•¿ä¸Šä¿æŒä¸å˜çš„ä¸€è‡´æ¨¡å¼ä¿¡æ¯ cï¼Œå°†å…¶çº³å…¥è‡ªå›å½’æ¨¡å‹ä¸­ï¼š

ç”±äºæˆ‘ä»¬ä¸“æ³¨äº ego è½¨è¿¹è§„åˆ’ï¼Œä¸€è‡´æ¨¡å¼ c ä¸ä¼šå½±å“è¿‡æ¸¡æ¨¡å‹ã€‚

è¿™ç§ä¸€è‡´çš„è‡ªå›å½’å…¬å¼ï¼ˆ<a href="#Equation_4" class="eq-link" onclick="highlightAsset('Equation_4'); return false;">Equation 4ï¼‰</a>å®šä¹‰äº†ä¸€ä¸ªç”Ÿæˆ-é€‰æ‹©æ¡†æ¶ï¼Œåœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œæ¨¡å¼é€‰æ‹©å™¨æ ¹æ®åˆå§‹çŠ¶æ€ s0 å¯¹æ¯ä¸ªæ¨¡å¼è¿›è¡Œè¯„åˆ†ï¼Œè½¨è¿¹ç”Ÿæˆå™¨é€šè¿‡ä»æ¡ä»¶äºæ¨¡å¼çš„ç­–ç•¥ä¸­é‡‡æ ·æ¥ç”Ÿæˆå¤šæ¨¡æ€è½¨è¿¹ã€‚éååº”æ€§è¿‡æ¸¡æ¨¡å‹ã€‚æ ¹æ® <a href="#Equation_4" class="eq-link" onclick="highlightAsset('Equation_4'); return false;">Equation 4 </a>å½¢å¼åŒ–çš„è¿‡æ¸¡æ¨¡å‹éœ€è¦åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥é•¿ä¸Šä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¼šåŸºäºå½“å‰çŠ¶æ€ st äº§ç”Ÿäº¤é€šä»£ç†åœ¨æ—¶é—´æ­¥ t+1 çš„å§¿æ€ã€‚<div id="badge-26" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 26)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('26', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-27"><div class="row text-row"><div class="col-src">In practice, this process is time-consuming and we do not observe a performance improvement by using this transition model, therefore, we use trajectory predictors P(s1:N 1:T |s0) as non-reactive transition model that produces all future poses of trafï¬c agents in one shot given initial state s0.</div><div class="col-trans" id="trans-27">åœ¨å®é™…æ“ä½œä¸­ï¼Œè¿™ä¸€è¿‡ç¨‹è€—æ—¶è¾ƒé•¿ï¼Œå¹¶ä¸”æˆ‘ä»¬å¹¶æœªè§‚å¯Ÿåˆ°ä½¿ç”¨æ­¤è¿‡æ¸¡æ¨¡å‹èƒ½å¸¦æ¥æ€§èƒ½æå‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨è½¨è¿¹é¢„æµ‹å™¨ \( P(s_1:N, 1:T | s_0) \) ä½œä¸ºéååº”æ€§è¿‡æ¸¡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç»™å®šåˆå§‹çŠ¶æ€ \( s_0 \) çš„æƒ…å†µä¸‹ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æœªæ¥çš„å§¿æ€ã€‚<div id="badge-27" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 27)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('27', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-28"><div class="row text-row"><div class="col-src">[[HEADER: 3.3. Planner Architecture]]</div><div class="col-trans" id="trans-28"><b>3.3. è®¡åˆ’å™¨æ¶æ„</b><div id="badge-28" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 28)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('28', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-29"><div class="row text-row"><div class="col-src">The framework of our proposed CarPlanner is illustrated in Fig. 2, comprising four key components: 1) the nonreactive transition model, 2) the mode selector, 3) the trajectory generator, and 4) the rule-augmented selector.

Our planner operates within a generation-selection framework. Given an initial state s0 and all possible Nmode modes, the trajectory selector evaluates and assigns scores to each mode. The trajectory generator then produces Nmode trajectories that correspond to their respective modes. For trajectory generator, the initial state s0 is replicated Nmode times, each associated with one of the Nmode modes, effectively creating Nmode parallel worlds.

The policy is executed within these previewed worlds. During the policy rollout, a trajectory predictor acts as the state transition model, generating future poses of trafï¬c agents across all time horizons.</div><div class="col-trans" id="trans-29">æˆ‘ä»¬æå‡ºçš„CarPlannerçš„æ¡†æ¶å¦‚<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">å›¾2</a>æ‰€ç¤ºï¼ŒåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼š1) éååº”æ€§è½¬ç§»æ¨¡å‹ï¼Œ2) æ¨¡å¼é€‰æ‹©å™¨ï¼Œ3) è½¨è¿¹ç”Ÿæˆå™¨ï¼Œä»¥åŠ4) è§„åˆ™å¢å¼ºçš„é€‰æ‹©å™¨ã€‚

æˆ‘ä»¬çš„è§„åˆ’å™¨åœ¨ç”Ÿæˆ-é€‰æ‹©æ¡†æ¶ä¸‹è¿ä½œã€‚ç»™å®šåˆå§‹çŠ¶æ€s0å’Œæ‰€æœ‰å¯èƒ½çš„Nmodeæ¨¡å¼ï¼Œè½¨è¿¹é€‰æ‹©å™¨è¯„ä¼°å¹¶ä¸ºæ¯ä¸ªæ¨¡å¼åˆ†é…åˆ†æ•°ã€‚éšåï¼Œè½¨è¿¹ç”Ÿæˆå™¨äº§ç”Ÿä¸å„è‡ªæ¨¡å¼å¯¹åº”çš„Nmodeæ¡è½¨è¿¹ã€‚å¯¹äºè½¨è¿¹ç”Ÿæˆå™¨è€Œè¨€ï¼Œåˆå§‹çŠ¶æ€s0è¢«å¤åˆ¶Nmodeæ¬¡ï¼Œæ¯æ¬¡ä¸ä¸€ä¸ªä¸åŒçš„Nmodeæ¨¡å¼å…³è”ï¼Œä»è€Œåˆ›å»ºäº†Nmodeä¸ªå¹³è¡Œä¸–ç•Œã€‚

ç­–ç•¥åœ¨è¿™äº›é¢„è§ˆçš„ä¸–ç•Œä¸­æ‰§è¡Œã€‚åœ¨ç­–ç•¥å±•å¼€è¿‡ç¨‹ä¸­ï¼Œè½¨è¿¹é¢„æµ‹å™¨ä½œä¸ºçŠ¶æ€è½¬ç§»æ¨¡å‹ï¼Œç”Ÿæˆæ‰€æœ‰æ—¶é—´èŒƒå›´å†…çš„äº¤é€šä»£ç†çš„æœªæ¥å§¿æ€ã€‚<div id="badge-29" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 29)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('29', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-30"><div class="row text-row"><div class="col-src">[[HEADER: 3.3.1. Non-reactive Transition Model]]</div><div class="col-trans" id="trans-30"><b>3.3.1. éååº”æ€§è½¬æ¢æ¨¡å‹</b><div id="badge-30" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 30)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('30', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-31"><div class="row text-row"><div class="col-src">3.3.1. Non-reactive Transition Model This module takes the initial state s0 as input and outputs the future trajectories of trafï¬c agents. The initial state is processed by agent and map encoders, followed by a selfattention Transformer encoder [38] to fuse the agent and map features. The agent features are then decoded into future trajectories. Agent and map encoders. The state s0 contains both map and agent information. The map information m consists of Nm,1 polylines and Nm,2 polygons.

The polylines describe lane centers and lane boundaries, with each polyline containing 3Np points, where 3 corresponds to the lane center, the left boundary, and the right boundary. Each point is with dimension Dm = 9 and includes the following attributes: x, y, heading, speed limit, and category. When concatenated, the points of the left and right boundaries together with the center point yield a dimension of Nm,1 Ã— Np Ã— 3Dm.</div><div class="col-trans" id="trans-31">3.3.1 éååº”æ€§è¿‡æ¸¡æ¨¡å‹

è¯¥æ¨¡å—ä»¥åˆå§‹çŠ¶æ€ \(s_0\) ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºäº¤é€šä»£ç†çš„æœªæ¥è½¨è¿¹ã€‚åˆå§‹çŠ¶æ€é¦–å…ˆç»è¿‡ä»£ç†ç¼–ç å™¨å’Œåœ°å›¾ç¼–ç å™¨å¤„ç†ï¼Œç„¶åé€šè¿‡è‡ªæ³¨æ„åŠ›å˜æ¢å™¨ç¼–ç å™¨ <a href="#ref-38" class="ref-link">[38]</a> æ¥èåˆä»£ç†å’Œåœ°å›¾ç‰¹å¾ã€‚ä¹‹åï¼Œä»£ç†ç‰¹å¾è¢«è§£ç ä¸ºæœªæ¥çš„è½¨è¿¹ã€‚

**ä»£ç†ç¼–ç å™¨å’Œåœ°å›¾ç¼–ç å™¨**

çŠ¶æ€ \(s_0\) åŒ…å«åœ°å›¾ä¿¡æ¯å’Œä»£ç†ä¿¡æ¯ã€‚åœ°å›¾ä¿¡æ¯ \(m\) ç”± \(N_{m,1}\) æ¡å¤šè¾¹çº¿å’Œ \(N_{m,2}\) ä¸ªå¤šè¾¹å½¢ç»„æˆã€‚

å¤šè¾¹çº¿æè¿°è½¦é“ä¸­å¿ƒå’Œè¾¹ç•Œï¼Œæ¯æ¡å¤šè¾¹çº¿åŒ…å« \(3N_p\) ä¸ªç‚¹ï¼Œå…¶ä¸­ 3 è¡¨ç¤ºè½¦é“ä¸­å¿ƒã€å·¦ä¾§è¾¹ç•Œå’Œå³ä¾§è¾¹ç•Œã€‚æ¯ä¸ªç‚¹çš„ç»´åº¦ä¸º \(D_m = 9\)ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å±æ€§ï¼šx åæ ‡ã€y åæ ‡ã€èˆªå‘è§’ã€é€Ÿåº¦é™åˆ¶ä»¥åŠç±»åˆ«ã€‚å½“å°†è¿™äº›ç‚¹è¿æ¥èµ·æ¥æ—¶ï¼Œå·¦ä¾§å’Œå³ä¾§è¾¹ç•Œçš„ç‚¹ä¸ä¸­å¿ƒç‚¹ä¸€èµ·æ„æˆäº†ä¸€ä¸ªç»´åº¦ä¸º \(N_{m,1} \times N_p \times 3D_m\) çš„ç‰¹å¾å‘é‡ã€‚<div id="badge-31" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 31)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('31', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-32"><div class="row text-row"><div class="col-src">We leverage a PointNet [26] to extract features from the points of each polyline, resulting in a dimensionality of Nm,1 Ã— D, where D represents the feature dimension. The polygons represent intersections, crosswalks, stop lines, etc, with each polygon containing Np points. We utilize another PointNet to extract features from the points of each polygon, producing a dimension of Nm,2 Ã— D.

We then concatenate the features from both polylines and polygons to form the overall map features, resulting in a dimension of Nm Ã— D. The agent information A consists of N agents, where each agent maintains poses for the past H time steps. Each pose is with dimension Da = 10 and includes the following attributes: x, y, heading, velocity, bounding box, time step, and category. Consequently, the agent information has a dimension of N Ã— H Ã— Da.

We apply another PointNet to extract features from the poses of each agent, yielding an agent feature dimension of N Ã— D.</div><div class="col-trans" id="trans-32">æˆ‘ä»¬åˆ©ç”¨PointNet <a href="#ref-26" class="ref-link">[26]</a> ä»æ¯ä¸ªå¤šè¾¹çº¿çš„ç‚¹ä¸­æå–ç‰¹å¾ï¼Œç»“æœç»´åº¦ä¸ºNm,1 Ã— Dï¼Œå…¶ä¸­Dè¡¨ç¤ºç‰¹å¾ç»´åº¦ã€‚å¤šè¾¹å½¢ä»£è¡¨äº¤å‰å£ã€äººè¡Œæ¨ªé“ã€åœæ­¢çº¿ç­‰ï¼Œæ¯ä¸ªå¤šè¾¹å½¢åŒ…å«Npä¸ªç‚¹ã€‚æˆ‘ä»¬å†ä½¿ç”¨å¦ä¸€ä¸ªPointNetä»æ¯ä¸ªå¤šè¾¹å½¢çš„ç‚¹ä¸­æå–ç‰¹å¾ï¼Œç”Ÿæˆä¸€ä¸ªç»´åº¦ä¸ºNm,2 Ã— Dã€‚

ç„¶åæˆ‘ä»¬å°†å¤šè¾¹çº¿å’Œå¤šè¾¹å½¢çš„ç‰¹å¾è¿›è¡Œæ‹¼æ¥ï¼Œå½¢æˆæ•´ä½“åœ°å›¾ç‰¹å¾ï¼Œç»“æœç»´åº¦ä¸ºNm Ã— Dã€‚ä»£ç†ä¿¡æ¯Aç”±Nä¸ªä»£ç†ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªä»£ç†ç»´æŠ¤è¿‡å»Hä¸ªæ—¶é—´æ­¥çš„å§¿åŠ¿ã€‚æ¯ä¸ªå§¿åŠ¿çš„ç»´åº¦Da = 10ï¼Œå¹¶åŒ…æ‹¬ä»¥ä¸‹å±æ€§ï¼šx, y, æ–¹å‘ã€é€Ÿåº¦ã€è¾¹ç•Œæ¡†ã€æ—¶é—´æ­¥å’Œç±»åˆ«ã€‚å› æ­¤ï¼Œä»£ç†ä¿¡æ¯çš„ç»´åº¦ä¸ºN Ã— H Ã— Daã€‚

æˆ‘ä»¬å†åº”ç”¨ä¸€ä¸ªPointNetä»æ¯ä¸ªä»£ç†çš„å§¿æ€ä¸­æå–ç‰¹å¾ï¼Œç”Ÿæˆä»£ç†ç‰¹å¾ç»´åº¦ä¸ºN Ã— Dã€‚<div id="badge-32" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 32)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('32', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-33"><div class="row text-row"><div class="col-src">[[HEADER: 3.3.2. Mode Selector]]</div><div class="col-trans" id="trans-33"><b>3.3.2. æ¨¡å¼é€‰æ‹©å™¨</b><div id="badge-33" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 33)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('33', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-34"><div class="row text-row"><div class="col-src">This module takes s0 and longitudinal-lateral decomposed mode information as input and outputs the probability of each mode. The number of modes Nmode = NlatNlon. Route-speed decomposed mode. To capture the longitudinal behaviors, we generate Nlon modes that represent the average speed of the trajectory associated with each mode. Each longitudinal mode clon,j is deï¬ned as a scalar value of j Nlon , repeated across a dimension D.

As a result, the dimensionality of the longitudinal modes is Nlon Ã— D. For lateral behaviors, we identify Nlat possible routes from the map using a graph search algorithm. These routes correspond to the lanes available for the ego vehicle. The dimensionality of these routes is Nlat Ã— Nr Ã— Dm. We employ another PointNet to aggregate the features of the Nr points along each route, producing a lateral mode with a dimension of Nlat Ã— D.</div><div class="col-trans" id="trans-34">è¯¥æ¨¡å—ä»¥s0å’Œçºµå‘-æ¨ªå‘åˆ†è§£æ¨¡å¼ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªæ¨¡å¼çš„æ¦‚ç‡ã€‚æ¨¡å¼çš„æ•°é‡Nmode = NlatÃ—Nlonã€‚è·¯å¾„-é€Ÿåº¦åˆ†è§£æ¨¡å¼ã€‚ä¸ºäº†æ•æ‰çºµå‘è¡Œä¸ºï¼Œæˆ‘ä»¬ç”Ÿæˆäº†Nlonä¸ªæ¨¡å¼æ¥è¡¨ç¤ºä¸æ¯ä¸ªæ¨¡å¼ç›¸å…³çš„è½¨è¿¹çš„å¹³å‡é€Ÿåº¦ã€‚æ¯ä¸ªçºµå‘æ¨¡å¼clon,jè¢«å®šä¹‰ä¸ºä¸€ä¸ªåœ¨ç»´åº¦Dä¸Šé‡å¤å‡ºç°çš„æ ‡é‡å€¼jã€‚

å› æ­¤ï¼Œçºµå‘æ¨¡å¼çš„ç»´åº¦æ˜¯Nlon Ã— Dã€‚å¯¹äºæ¨ªå‘è¡Œä¸ºï¼Œæˆ‘ä»¬ä½¿ç”¨å›¾æœç´¢ç®—æ³•ä»åœ°å›¾ä¸­è¯†åˆ«å‡ºNlatæ¡å¯èƒ½çš„è·¯å¾„ã€‚è¿™äº›è·¯å¾„å¯¹åº”äºä¾› ego è½¦è¾†ä½¿ç”¨çš„è½¦é“ã€‚è¿™äº›è·¯å¾„çš„ç»´åº¦æ˜¯Nlat Ã— Nr Ã— Dmã€‚æˆ‘ä»¬åˆä½¿ç”¨å¦ä¸€ä¸ªPointNetæ¥èšåˆæ¯æ¡è·¯å¾„ä¸ŠNrä¸ªç‚¹çš„ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªæ¨ªå‘æ¨¡å¼ï¼Œå…¶ç»´åº¦ä¸ºNlat Ã— Dã€‚<div id="badge-34" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 34)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('34', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-35"><div class="row text-row"><div class="col-src">To create a comprehensive mode representation c, we combine the lateral and longitudinal modes, resulting in a combined dimension of Nlat Ã— Nlon Ã— 2D. To align this mode information with other feature dimensions, we pass it through a linear layer, mapping it back to Nlat Ã— Nlon Ã— D. Query-based Transformer decoder. This decoder is employed to fuse the mode features with map and agent features derived from s0.

In this framework, the mode serves as the query, while the map and agent information act as the keys and values. The updated mode features are decoded through a multi-layer perceptron (MLP) to yield the scores for each mode, which are subsequently normalized using the softmax operator.</div><div class="col-trans" id="trans-35">ä¸ºäº†åˆ›å»ºä¸€ä¸ªå…¨é¢çš„æ¨¡å¼è¡¨ç¤ºcï¼Œæˆ‘ä»¬å°†æ¨ªå‘å’Œçºµå‘æ¨¡å¼ç»“åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªNlat Ã— Nlon Ã— 2Dçš„ç»¼åˆç»´åº¦ã€‚ä¸ºä½¿è¿™ç§æ¨¡å¼ä¿¡æ¯ä¸å…¶å®ƒç‰¹å¾ç»´åº¦å¯¹é½ï¼Œæˆ‘ä»¬å°†å…¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„å›Nlat Ã— Nlon Ã— Dã€‚åŸºäºæŸ¥è¯¢çš„Transformerè§£ç å™¨è¢«ç”¨äºèåˆä»s0ä¸­æå–çš„åœ°å›¾å’Œä»£ç†ä¿¡æ¯ä»¥åŠæ¨¡å¼ç‰¹å¾ã€‚

åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæ¨¡å¼ä½œä¸ºæŸ¥è¯¢ï¼Œè€Œåœ°å›¾å’Œä»£ç†ä¿¡æ¯åˆ™å……å½“é”®å’Œå€¼ã€‚æ›´æ–°åçš„æ¨¡å¼ç‰¹å¾é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰è¿›è¡Œè§£ç ï¼Œä»¥ç”Ÿæˆæ¯ä¸ªæ¨¡å¼çš„åˆ†æ•°ï¼Œéšåä½¿ç”¨softmaxæ“ä½œå¯¹è¿™äº›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ã€‚<div id="badge-35" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 35)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('35', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-36"><div class="row text-row"><div class="col-src">[[HEADER: 3.3.3. Trajectory Generator]]</div><div class="col-trans" id="trans-36"><b>3.3.3. è½¨è¿¹ç”Ÿæˆå™¨</b><div id="badge-36" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 36)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('36', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-37"><div class="row text-row"><div class="col-src">This module operates in an auto-regressive manner, recurrently decoding the next pose of the ego vehicle at, given the current state st, and consistent mode information c. Invariant-view module (IVM). Before feeding the mode and state into the network, we preprocess them to eliminate time information. For the map and agent information in state st, we select the K-nearest neighbors (KNN) to the ego current pose and only feed these into the policy.

K is set to the half of map and agent elements respectively. Regarding the routes that capture lateral behaviors, we ï¬lter out the segments where the point closest to the current pose of the ego vehicle is the starting point, retaining Kr points. In this case, Kr is set to a quarter of Nr points in one route. Finally, we transform the routes, agent, and map poses into the coordinate frame of the ego vehicle at the current time step t.</div><div class="col-trans" id="trans-37">è¯¥æ¨¡å—ä»¥è‡ªå›å½’çš„æ–¹å¼è¿è¡Œï¼Œé€’å½’åœ°è§£ç ç»™å®šå½“å‰çŠ¶æ€ \( s_t \) å’Œä¸€è‡´æ¨¡å¼ä¿¡æ¯ \( c \) ä¸‹egoè½¦è¾†çš„ä¸‹ä¸€ä¸ªå§¿æ€ã€‚åœ¨å°†æ¨¡å¼å’ŒçŠ¶æ€è¾“å…¥ç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ä»¥æ¶ˆé™¤æ—¶é—´ä¿¡æ¯ã€‚å¯¹äºçŠ¶æ€ \( s_t \) ä¸­çš„åœ°å›¾å’Œä»£ç†ä¿¡æ¯ï¼Œæˆ‘ä»¬é€‰æ‹© ego å½“å‰å§¿æ€çš„ K ä¸ªæœ€è¿‘é‚»ï¼ˆKNNï¼‰ï¼Œå¹¶ä»…å°†è¿™äº›ä¿¡æ¯é¦ˆå…¥ç­–ç•¥ä¸­ã€‚

K è¢«è®¾ç½®ä¸ºåœ°å›¾å’Œä»£ç†å…ƒç´ çš„ä¸€åŠã€‚å…³äºæ•æ‰ä¾§å‘è¡Œä¸ºçš„è·¯çº¿ï¼Œæˆ‘ä»¬è¿‡æ»¤æ‰èµ·ç‚¹æ˜¯å½“å‰ ego è½¦è¾†æœ€æ¥è¿‘ç‚¹çš„è·¯æ®µï¼Œå¹¶ä¿ç•™ \( K_r \) ç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ\( K_r \) è¢«è®¾ç½®ä¸ºæ¯æ¡è·¯çº¿ä¸­ \( N_r \) ç‚¹çš„å››åˆ†ä¹‹ä¸€ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è·¯çº¿ã€ä»£ç†å’Œåœ°å›¾å§¿æ€è½¬æ¢åˆ°å½“å‰æ—¶é—´æ­¥ \( t \) çš„ ego è½¦è¾†åæ ‡ç³»ä¸­ã€‚<div id="badge-37" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 37)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('37', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-38"><div class="row text-row"><div class="col-src">We subtract the historical time steps t âˆ’H : t from the current time step t, yielding time steps in range âˆ’H : 0.

Query-based Transformer decoder. We employ the same backbone network architecture as the mode selector, but with different query dimensions. Due to the IVM and the fact that different modes yield distinct states, the map and agent information cannot be shared among modes. As a result, we fuse information for each individual mode. Speciï¬cally, the query dimension is 1Ã—D, while the dimensions of the keys and values are (N + Nm) Ã— D. The output feature dimension remains 1 Ã— D.</div><div class="col-trans" id="trans-38">æˆ‘ä»¬ä»å½“å‰æ—¶é—´æ­¥tå‡å»å†å²æ—¶é—´æ­¥t-H : tï¼Œå¾—åˆ°æ—¶é—´èŒƒå›´ä¸ºâˆ’H : 0çš„åŒºé—´ã€‚

åŸºäºæŸ¥è¯¢çš„Transformerè§£ç å™¨ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸æ¨¡å¼é€‰æ‹©å™¨ç›¸åŒçš„éª¨å¹²ç½‘ç»œæ¶æ„ï¼Œä½†æŸ¥è¯¢ç»´åº¦ä¸åŒã€‚ç”±äºIVMä»¥åŠä¸åŒçš„æ¨¡å¼ä¼šäº§ç”Ÿä¸åŒçš„çŠ¶æ€ï¼Œåœ°å›¾å’Œä»£ç†ä¿¡æ¯ä¸èƒ½åœ¨å„æ¨¡å¼ä¹‹é—´å…±äº«ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä¸ºæ¯ä¸ªå•ç‹¬çš„æ¨¡å¼èåˆä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼ŒæŸ¥è¯¢ç»´åº¦ä¸º1Ã—Dï¼Œé”®å’Œå€¼çš„ç»´åº¦åˆ†åˆ«ä¸º(N + Nm) Ã— Dã€‚è¾“å‡ºç‰¹å¾ç»´åº¦ä¿æŒä¸º1Ã—Dã€‚<div id="badge-38" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 38)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('38', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-39"><div class="row text-row"><div class="col-src">Note that Transformer decoder can process information from multiple modes in parallel, eliminating the need to handle each mode sequentially. Policy output. The mode feature is processed by two distinct heads: a policy head and a value head. Each head comprises its own MLP to produce the parameters for the action distribution and the corresponding value estimate. We employ a Gaussian distribution to model the action distribution, where actions are sampled from this distribution during training.

In contrast, during inference, we utilize the mean of the distribution to determine the actions.</div><div class="col-trans" id="trans-39">è¯·æ³¨æ„ï¼ŒTransformerè§£ç å™¨å¯ä»¥å¹¶è¡Œå¤„ç†å¤šç§æ¨¡å¼çš„ä¿¡æ¯ï¼Œä»è€Œæ¶ˆé™¤æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å¼çš„éœ€è¦ã€‚ç­–ç•¥è¾“å‡ºæ–¹é¢ï¼Œæ¨¡å¼ç‰¹å¾ç”±ä¸¤ä¸ªä¸åŒçš„å¤´è¿›è¡Œå¤„ç†ï¼šä¸€ä¸ªç­–ç•¥å¤´å’Œä¸€ä¸ªä»·å€¼å¤´ã€‚æ¯ä¸ªå¤´éƒ½åŒ…å«è‡ªå·±çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä»¥ç”ŸæˆåŠ¨ä½œåˆ†å¸ƒçš„å‚æ•°ä»¥åŠç›¸åº”çš„ä»·å€¼ä¼°è®¡ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜æ–¯åˆ†å¸ƒæ¥å»ºæ¨¡åŠ¨ä½œåˆ†å¸ƒï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»è¯¥åˆ†å¸ƒä¸­é‡‡æ ·åŠ¨ä½œã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†å¸ƒçš„å‡å€¼æ¥ç¡®å®šåŠ¨ä½œã€‚<div id="badge-39" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 39)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('39', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-40"><div class="row text-row"><div class="col-src">[[HEADER: 3.3.4. Rule-augmented Selector]]</div><div class="col-trans" id="trans-40"><b>3.3.4. Rule-augmented Selector</b><div id="badge-40" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 40)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('40', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-41"><div class="row text-row"><div class="col-src">This module is only utilized during inference and takes as input the initial state s0, the multi-modal ego-planned trajectories, and the predicted future trajectories of agents. It calculates driving-oriented metrics such as safety, progress, comfort. A comprehensive score is obtained by the weighted sum of rule-based scores and the mode scores provided by the mode selector. The ego-planned trajectory with the highest score is selected as the output of the planner.</div><div class="col-trans" id="trans-41">è¯¥æ¨¡å—ä»…åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¢«åˆ©ç”¨ï¼Œè¾“å…¥åŒ…æ‹¬åˆå§‹çŠ¶æ€s0ã€å¤šæ¨¡æ€è‡ªæˆ‘è§„åˆ’è½¨è¿¹ä»¥åŠé¢„æµ‹çš„å…¶ä»–ä»£ç†çš„æœªæ¥è½¨è¿¹ã€‚å®ƒè®¡ç®—è¯¸å¦‚å®‰å…¨ã€è¿›å±•å’Œèˆ’é€‚åº¦ç­‰ä¸é©¾é©¶ç›¸å…³çš„æŒ‡æ ‡ã€‚é€šè¿‡åŸºäºè§„åˆ™å¾—åˆ†å’Œæ¨¡å¼é€‰æ‹©å™¨æä¾›çš„æ¨¡å¼å¾—åˆ†åŠ æƒæ±‚å’Œè·å¾—ä¸€ä¸ªç»¼åˆè¯„åˆ†ã€‚å¾—åˆ†æœ€é«˜çš„è‡ªæˆ‘è§„åˆ’è½¨è¿¹è¢«é€‰ä½œè§„åˆ’å™¨çš„è¾“å‡ºã€‚<div id="badge-41" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 41)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('41', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-42"><div class="row text-row"><div class="col-src">[[HEADER: 3.4. Training]]</div><div class="col-trans" id="trans-42"><b>3.4. è®­ç»ƒ</b><div id="badge-42" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 42)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('42', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-43"><div class="row text-row"><div class="col-src">We ï¬rst train the non-reactive transition model and freeze the weights during the training of the mode selector and trajectory generator. Instead of feeding all modes to the generator, we apply a winner-takes-all strategy, wherein a positive mode is assigned based on the ego ground-truth trajectory and serves as a condition for the trajectory generator. Mode assignment. For the lateral mode, we assign the route closest to the endpoint of ego ground-truth trajectory as the positive lateral mode.

For the longitudinal mode, we partition the longitudinal space into Nlon intervals and assign the interval containing the endpoint of the ground-truth trajectory as the positive longitudinal mode. Reward function. To handle diverse scenarios, we use the negative displacement error (DE) between the ego future pose and the ground truth as a universal reward. We also introduce additional terms to improve trajectory quality: collision rate and drivable area compliance.</div><div class="col-trans" id="trans-43">æˆ‘ä»¬é¦–å…ˆè®­ç»ƒéååº”æ€§è¿‡æ¸¡æ¨¡å‹ï¼Œå¹¶åœ¨æ¨¡å¼é€‰æ‹©å™¨å’Œè½¨è¿¹ç”Ÿæˆå™¨çš„è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“å…¶æƒé‡ã€‚è€Œä¸æ˜¯å°†æ‰€æœ‰æ¨¡å¼éƒ½è¾“å…¥ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬åº”ç”¨â€œèµ¢å®¶é€šåƒâ€ç­–ç•¥ï¼Œåœ¨æ­¤ç­–ç•¥ä¸­ï¼Œæ ¹æ®egoçš„çœŸå®åœ°é¢è½¨è¿¹åˆ†é…ä¸€ä¸ªæ­£æ¨¡å¼ï¼Œå¹¶å°†å…¶ä½œä¸ºè½¨è¿¹ç”Ÿæˆå™¨çš„æ¡ä»¶ã€‚æ¨¡å¼åˆ†é…ã€‚å¯¹äºæ¨ªå‘æ¨¡å¼ï¼Œæˆ‘ä»¬å°†ä¸egoçœŸå®åœ°é¢è½¨è¿¹ç»ˆç‚¹æœ€è¿‘çš„è·¯çº¿åˆ†é…ä¸ºæ­£æ¨ªå‘æ¨¡å¼ã€‚

å¯¹äºçºµå‘æ¨¡å¼ï¼Œæˆ‘ä»¬å°†çºµå‘ç©ºé—´åˆ’åˆ†ä¸ºNlonä¸ªåŒºé—´ï¼Œå¹¶å°†åŒ…å«çœŸå®è½¨è¿¹ç»ˆç‚¹çš„åŒºé—´åˆ†é…ä¸ºæ­£çºµå‘æ¨¡å¼ã€‚å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†å¤„ç†å„ç§åœºæ™¯ï¼Œæˆ‘ä»¬ä½¿ç”¨egoæœªæ¥å§¿æ€ä¸çœŸå®å€¼ä¹‹é—´çš„è´Ÿä½ç§»è¯¯å·®ï¼ˆDEï¼‰ä½œä¸ºé€šç”¨å¥–åŠ±ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†é¢å¤–é¡¹ä»¥æé«˜è½¨è¿¹è´¨é‡ï¼šç¢°æ’ç‡å’Œå¯é€šè¡ŒåŒºåŸŸåˆè§„æ€§ã€‚<div id="badge-43" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 43)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('43', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-44"><div class="row text-row"><div class="col-src">If the future pose collides or falls outside the drivable area, the reward is set to -1; otherwise, it is 0. Mode dropout. In some cases, there are no available routes for ego to follow. However, since routes serve as queries in Transformer, the absence of a route can lead to unstable or hazardous outputs. To mitigate this issue, we implement a mode dropout module during training that randomly masks routes to prevent over-reliance on this information. Loss function.

For the selector, we use cross-entropy loss that is the negative log-likelihood of the positive mode and a side task that regresses the ego ground-truth trajectory. For the generator, we use PPO [31] loss that consists of three parts: policy improvement, value estimation, and entropy. Full description can be found in supplementary.</div><div class="col-trans" id="trans-44">å¦‚æœæœªæ¥å§¿æ€å‘ç”Ÿç¢°æ’æˆ–è½åœ¨ä¸å¯è¡Œé©¶åŒºåŸŸä¹‹å¤–ï¼Œå¥–åŠ±å°†è¢«è®¾å®šä¸º-1ï¼›å¦åˆ™ï¼Œå¥–åŠ±ä¸º0ã€‚æ¨¡å¼ä¸¢å¼ƒã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ²¡æœ‰å¯ä¾›egoè·Ÿéšçš„æœ‰æ•ˆè·¯å¾„ã€‚ç„¶è€Œï¼Œç”±äºè·¯å¾„ä½œä¸ºTransformerä¸­çš„æŸ¥è¯¢é¡¹ï¼Œè·¯å¾„çš„ç¼ºå¤±å¯èƒ½å¯¼è‡´ä¸ç¨³å®šçš„æˆ–å±é™©çš„è¾“å‡ºã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªæ¨¡å¼ä¸¢å¼ƒæ¨¡å—ï¼Œéšæœºé®è”½è·¯å¾„ä»¥é˜²æ­¢è¿‡åº¦ä¾èµ–è¿™äº›ä¿¡æ¯ã€‚æŸå¤±å‡½æ•°ã€‚

å¯¹äºé€‰æ‹©å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œå³æ­£æ¨¡å¼çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡æ¥å›å½’egoçš„çœŸå®è½¨è¿¹ã€‚å¯¹äºç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬é‡‡ç”¨PPO <a href="#ref-31" class="ref-link">[31]</a> æŸå¤±ï¼Œè¯¥æŸå¤±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šç­–ç•¥æ”¹è¿›ã€ä»·å€¼ä¼°è®¡å’Œç†µã€‚å®Œæ•´æè¿°è¯¦è§è¡¥å……ææ–™ã€‚<div id="badge-44" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 44)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('44', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-45"><div class="row text-row"><div class="col-src">[[HEADER: 4. Experiments]]</div><div class="col-trans" id="trans-45"><b>4. å®éªŒ</b><div id="badge-45" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 45)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('45', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-46"><div class="row text-row"><div class="col-src">[[HEADER: 4.1. Experimental Setup]]</div><div class="col-trans" id="trans-46"><b>4.1. å®éªŒè®¾ç½®</b><div id="badge-46" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 46)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('46', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_1</div><img src="./assets/Table_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 1. Comparison with SOTAs in Test14-Random. Based on the type of trajectory generator, all methods are categorized into Rule, IL, and RL. The best result is in bold and the second best result is underlined.</div><div class="asset-desc-zh">è¡¨1. Test14-Random ä¸­ä¸æœ€å…ˆè¿›çš„æ–¹æ³•æ¯”è¾ƒã€‚ æ ¹æ®è½¨è¿¹ç”Ÿæˆå™¨ç±»å‹ï¼Œæ‰€æœ‰æ–¹æ³•è¢«åˆ†ä¸ºè§„åˆ™ã€IL å’Œ RL ç±»åˆ«ã€‚ æœ€ä½³ç»“æœåŠ ç²—æ˜¾ç¤ºï¼Œç¬¬äºŒå¥½çš„ç»“æœä¸‹åˆ’çº¿æ ‡æ³¨ã€‚</div></div></div></div><div class="row asset-row" id="Table_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_2</div><img src="./assets/Table_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 2. Comparison with SOTAs in Reduced-Val14 with non- reactive trafï¬c agents.</div><div class="asset-desc-zh">è¡¨2. Reduced-Val14 æ— ååº”å¼äº¤é€šä»£ç†ä¸­çš„ä¸æœ€å…ˆè¿›çš„æ–¹æ³•æ¯”è¾ƒã€‚</div></div></div></div><div class="row-container" id="task-47"><div class="row text-row"><div class="col-src">Dataset and simulator. We use nuPlan [2], a large-scale closed-loop platform for studying trajectory planning in autonomous driving, to evaluate the efï¬cacy of our method. The nuPlan dataset contains driving log data over 1,500 hours collected by human expert drivers across 4 diverse cities. It includes complex, diverse scenarios such as lane follow and change, left and right turn, traversing intersections and bus stops, roundabouts, interaction with pedestrians, etc.

As a closed-loop platform, nuPlan provides a simulator that uses scenarios from the dataset as initialization. During the simulation, trafï¬c agents are taken over by logreplay (non-reactive) or an IDM [37] policy (reactive). The ego vehicle is taken over by user-provided planners. The simulator lasts for 15 seconds and runs at 10 Hz.</div><div class="col-trans" id="trans-47">æ•°æ®é›†å’Œæ¨¡æ‹Ÿå™¨ã€‚æˆ‘ä»¬ä½¿ç”¨ nuPlan <a href="#ref-2" class="ref-link">[2]</a>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é—­ç¯å¹³å°ï¼Œç”¨äºç ”ç©¶è‡ªåŠ¨é©¾é©¶ä¸­çš„è½¨è¿¹è§„åˆ’ï¼Œæ¥è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚nuPlan æ•°æ®é›†åŒ…å«ç”±äººç±»ä¸“å®¶é©¾é©¶è€…åœ¨4ä¸ªä¸åŒåŸå¸‚æ”¶é›†çš„è¶…è¿‡1,500å°æ—¶çš„é©¾é©¶æ—¥å¿—æ•°æ®ã€‚å®ƒåŒ…æ‹¬å¤æ‚çš„å¤šæ ·åŒ–åœºæ™¯ï¼Œå¦‚è½¦é“è·Ÿéšä¸å˜æ¢ã€å·¦è½¬å’Œå³è½¬ã€ç©¿è¶Šäº¤å‰å£å’Œå…¬äº¤ç«™ã€ç¯å²›ã€è¡Œäººäº¤äº’ç­‰ã€‚

ä½œä¸ºä¸€ä¸ªé—­ç¯å¹³å°ï¼ŒnuPlan æä¾›äº†ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¨¡æ‹Ÿå™¨ä½¿ç”¨æ•°æ®é›†ä¸­çš„åœºæ™¯ä½œä¸ºåˆå§‹åŒ–ã€‚åœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­ï¼Œäº¤é€šä»£ç†ç”±æ—¥å¿—å›æ”¾ï¼ˆéååº”æ€§ï¼‰æˆ– IDM <a href="#ref-37" class="ref-link">[37]</a> ç­–ç•¥ï¼ˆååº”æ€§ï¼‰æ¥ç®¡ã€‚è‡ªä¸»è½¦è¾†åˆ™ç”±ç”¨æˆ·æä¾›çš„è§„åˆ’è€…æ¥ç®¡ã€‚æ¨¡æ‹ŸæŒç»­15ç§’ï¼Œå¹¶ä»¥æ¯ç§’10æ¬¡çš„é¢‘ç‡è¿è¡Œã€‚<div id="badge-47" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 47)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('47', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-48"><div class="row text-row"><div class="col-src">At each timestamp, the simulator queries the planner to plan a trajectory, which is tracked by an LQR controller to generate control commands to drive the ego vehicle. Benchmarks and metrics. We use two benchmarks: Test14-Random and Reduced-Val14 for comparing with other methods and analyzing the design choices within our method. The Test14-Random provided by PlanTF [4] contains 261 scenarios. The Reduced-Val14 provided by PDM [7] contains 318 scenarios.

We use the closed-loop score (CLS) provided by the ofï¬cial nuPlan devkitâ€  to assess the performance of all methods. The CLS score comprehends different aspects such as safety (S-CR, S-TTC), drivable area compliance (S-Area), progress (S-PR), comfort, etc. Based on the different behavior types of trafï¬c agents, CLS is detailed into CLS-NR (non-reactive) and CLS-R (reactive). Implementation details. We follow PDM [7] to construct our training and validation splits.</div><div class="col-trans" id="trans-48">åœ¨æ¯ä¸ªæ—¶é—´æˆ³ï¼Œæ¨¡æ‹Ÿå™¨æŸ¥è¯¢è§„åˆ’å™¨æ¥è®¡åˆ’ä¸€æ¡è½¨è¿¹ï¼Œè¯¥è½¨è¿¹ç”±LQRæ§åˆ¶å™¨è·Ÿè¸ªä»¥ç”Ÿæˆæ§åˆ¶å‘½ä»¤æ¥é©±åŠ¨egoè½¦è¾†ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåŸºå‡†ï¼šTest14-Randomå’ŒReduced-Val14æ¥è¿›è¡Œä¸å…¶ä»–æ–¹æ³•çš„æ¯”è¾ƒä»¥åŠåˆ†ææˆ‘ä»¬æ–¹æ³•ä¸­çš„è®¾è®¡é€‰æ‹©ã€‚PlanTF <a href="#ref-4" class="ref-link">[4]</a>æä¾›çš„Test14-RandomåŒ…å«261ä¸ªåœºæ™¯ï¼ŒPDM <a href="#ref-7" class="ref-link">[7]</a>æä¾›çš„Reduced-Val14åŒ…å«318ä¸ªåœºæ™¯ã€‚

æˆ‘ä»¬ä½¿ç”¨å®˜æ–¹nuPlan devkitâ€ æä¾›çš„é—­ç¯è¯„åˆ†ï¼ˆCLSï¼‰æ¥è¯„ä¼°æ‰€æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚CLSåˆ†æ•°æ¶µç›–äº†è¯¸å¦‚å®‰å…¨æ€§ï¼ˆS-CR, S-TTCï¼‰ã€å¯è¡Œé©¶åŒºåŸŸåˆè§„æ€§ï¼ˆS-Areaï¼‰ã€è¿›åº¦ï¼ˆS-PRï¼‰ã€èˆ’é€‚åº¦ç­‰ä¸åŒæ–¹é¢ã€‚æ ¹æ®äº¤é€šä»£ç†çš„ä¸åŒè¡Œä¸ºç±»å‹ï¼ŒCLSè¿›ä¸€æ­¥ç»†åˆ†ä¸ºCLS-NRï¼ˆéååº”å‹ï¼‰å’ŒCLS-Rï¼ˆååº”å‹ï¼‰ã€‚å®ç°ç»†èŠ‚ã€‚æˆ‘ä»¬éµå¾ªPDM <a href="#ref-7" class="ref-link">[7]</a>æ¥æ„å»ºæˆ‘ä»¬çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ†ã€‚<div id="badge-48" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 48)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('48', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-49"><div class="row text-row"><div class="col-src">The size of the training set is 176,218 where all available scenario types are used, with a number of 4,000 scenarios per type. The size of the validation set is 1,118 where 100 scenarios with 14 types are selected. We train all models with 50 epochs in 2 NVIDIA 3090 GPUs. The batch size is 64 per GPU. We use AdamW optimizer with an initial learning rate of 1e-4 and reduce the learning rate when the validation loss stops decreasing with a patience of 0 and decrease factor of 0.3.

For RL training, â€ https://github.com/motional/nuplan-devkit

we set the discount Î³ = 0.1 and the GAE parameter Î» = 0.9. The weights of value, policy, and entropy loss are set to 3, 100, and 0.001, respectively. The number of longitudinal modes is set to 12 and a maximum number of lateral modes are set to 5.</div><div class="col-trans" id="trans-49">è®­ç»ƒé›†çš„å¤§å°ä¸º176,218ï¼Œä½¿ç”¨äº†æ‰€æœ‰å¯ç”¨çš„æƒ…æ™¯ç±»å‹ï¼Œæ¯ç§ç±»å‹æœ‰4,000ä¸ªæƒ…æ™¯ã€‚éªŒè¯é›†çš„å¤§å°ä¸º1,118ï¼Œä»ä¸­é€‰æ‹©äº†100ä¸ªæƒ…æ™¯ï¼ŒåŒ…å«14ç§ç±»å‹ã€‚æˆ‘ä»¬åœ¨ä¸¤å—NVIDIA 3090 GPUä¸Šä»¥50ä¸ªepochsè®­ç»ƒæ‰€æœ‰æ¨¡å‹ã€‚æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°ä¸º64ã€‚æˆ‘ä»¬ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå¹¶è®¾ç½®åˆå§‹å­¦ä¹ ç‡ä¸º1e-4ï¼Œåœ¨éªŒè¯æŸå¤±åœæ­¢ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡ï¼Œè€å¿ƒå€¼è®¾ä¸º0ï¼Œå‡å°‘å› å­ä¸º0.3ã€‚

å¯¹äºRLè®­ç»ƒï¼Œhttps://github.com/motional/nuplan-devkit ä¸­è®¾ç½®æŠ˜æ‰£å› å­Î³ = 0.1å’ŒGAEå‚æ•°Î» = 0.9ã€‚ä»·å€¼æŸå¤±ã€ç­–ç•¥æŸå¤±å’Œç†µæŸå¤±çš„æƒé‡åˆ†åˆ«è®¾ä¸º3ã€100å’Œ0.001ã€‚çºµå‘æ¨¡å¼çš„æ•°é‡è®¾ç½®ä¸º12ï¼Œæœ€å¤§æ¨ªå‘æ¨¡å¼æ•°é‡è®¾ç½®ä¸º5ã€‚<div id="badge-49" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: æ–‡ç« å¤–é“¾ç½‘å€æ— éœ€ä½¿ç”¨æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œä¿ç•™åŸæ–‡å³å¯ </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 49)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">æ–‡ç« å¤–é“¾ç½‘å€æ— éœ€ä½¿ç”¨æ ‡ç­¾åŒ–è¯­è¨€åŒ…å›´ï¼Œä¿ç•™åŸæ–‡å³å¯</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('49', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-50"><div class="row text-row"><div class="col-src">[[HEADER: 4.2. Comparison with SOTAs]]</div><div class="col-trans" id="trans-50"><b>4.2. ä¸SOTAæ–¹æ³•çš„æ¯”è¾ƒ</b><div id="badge-50" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 50)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('50', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_3</div><img src="./assets/Table_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 3. Ablation studies on the design choices in RL training. Results are in Test14-random non-reactive benchmark.</div><div class="asset-desc-zh">è¡¨3. åœ¨ RL è®­ç»ƒä¸­è®¾è®¡é€‰æ‹©çš„æ¶ˆèç ”ç©¶ã€‚ ç»“æœåŸºäº Test14-random éååº”å¼åŸºå‡†ã€‚</div></div></div></div><div class="row-container" id="task-51"><div class="row text-row"><div class="col-src">SOTAs. We categorize the methods into Rule, IL, and RL based on the type of trajectory generator. (1) PDM [7] wins the nuPlan challenge 2023, its IL-based and rule-based variants are denoted as PDM-Open and PDM-Closed, respectively.

PDM-Closed follows the generation-selection framework where IDM is used to generate multiple candidate trajectories and rule-based selector considering safety, progress, and comfort is used to select the best trajectory. (2) PLUTO [3] also obeys the generation-selection framework and uses contrastive IL to incorporate various data augmentation techniques and trains the generator. (3) GenDrive [18] is a concurrent work that follows a pretrainï¬netune pipeline where IL is used to pretrain a diffusionbased planner and RL is used to ï¬netune the denoising process based on a reward model trained by AI preference.</div><div class="col-trans" id="trans-51">æœ€å…ˆè¿›æ–¹æ³•ï¼ˆSOTAsï¼‰ã€‚æˆ‘ä»¬æ ¹æ®è½¨è¿¹ç”Ÿæˆå™¨çš„ç±»å‹å°†æ–¹æ³•åˆ†ä¸ºè§„åˆ™ã€ILå’ŒRLä¸‰ç±»ã€‚ï¼ˆ1ï¼‰PDM <a href="#ref-7" class="ref-link">[7]</a> åœ¨ nuPlan æŒ‘æˆ˜èµ›2023ä¸­è·èƒœï¼Œå…¶åŸºäºILå’Œè§„åˆ™çš„æ–¹æ³•åˆ†åˆ«è¡¨ç¤ºä¸º PDM-Open å’Œ PDM-Closedã€‚

PDM-Closed éµå¾ªç”Ÿæˆ-é€‰æ‹©æ¡†æ¶ï¼Œåœ¨è¯¥æ¡†æ¶ä¸‹ä½¿ç”¨ IDM ç”Ÿæˆå¤šä¸ªå€™é€‰è½¨è¿¹ï¼Œå¹¶é‡‡ç”¨è€ƒè™‘å®‰å…¨ã€è¿›åº¦å’Œèˆ’é€‚æ€§çš„åŸºäºè§„åˆ™çš„é€‰æ‹©å™¨æ¥é€‰æ‹©æœ€ä½³è½¨è¿¹ã€‚ï¼ˆ2ï¼‰PLUTO <a href="#ref-3" class="ref-link">[3]</a> åŒæ ·éµå¾ªç”Ÿæˆ-é€‰æ‹©æ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”ILæŠ€æœ¯ç»“åˆå¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚ ï¼ˆ3ï¼‰GenDrive <a href="#ref-18" class="ref-link">[18]</a> æ˜¯ä¸€é¡¹å¹¶è¡Œå·¥ä½œï¼Œå®ƒé‡‡ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒç®¡é“ï¼Œåœ¨è¯¥ç®¡é“ä¸­ä½¿ç”¨ILå¯¹åŸºäºæ‰©æ•£çš„è§„åˆ’å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ç”±AIåå¥½è®­ç»ƒå¾—åˆ°çš„å¥–åŠ±æ¨¡å‹å¯¹å»å™ªè¿‡ç¨‹è¿›è¡Œå¾®è°ƒä»¥å®ç°RLã€‚<div id="badge-51" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 51)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('51', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-52"><div class="row text-row"><div class="col-src">Results. We compare our method with SOTAs in Test14Random and Reduced-Val14 benchmark as shown in Tab. 1 and Tab. 2. Overall, our CarPlanner demonstrates superior performance, particularly in non-reactive environments.

In the non-reactive setting, our method achieves the highest scores across all metrics, with an improvement of 4.02 and 2.15 compared to PDM-Closed and PLUTO, establishing the potential of RL and the superior performance

of our proposed framework. Moreover, CarPlanner reveals substantial improvement in the progress metric S-PR compared to PDM-Closed in Tab. 2 and comparable collision metric S-CR, indicating the ability of our method to improving driving efï¬ciency while maintaining safe driving. Importantly, we do not apply any techniques commonly used in IL such as data augmentation [3, 4] and ego-history masking [11], underscoring the intrinsic capability of our approach to solving the closed-loop task.</div><div class="col-trans" id="trans-52">ç»“æœã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸Test14Randomå’ŒReduced-Val14åŸºå‡†ä¸­çš„SOTAè¿›è¡Œæ¯”è¾ƒï¼Œå¦‚<a href="#Table_1" class="tab-link" onclick="highlightAsset('Table_1'); return false;">è¡¨1</a>å’Œ<a href="#Table_2" class="tab-link" onclick="highlightAsset('Table_2'); return false;">è¡¨2</a>æ‰€ç¤ºã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„CarPlannerå±•ç¤ºäº†æ›´ä¼˜çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éååº”æ€§ç¯å¢ƒä¸­ã€‚

åœ¨éååº”æ€§è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è·å¾—æœ€é«˜åˆ†æ•°ï¼Œç›¸è¾ƒäºPDM-Closedå’ŒPLUTOåˆ†åˆ«æé«˜äº†4.02å’Œ2.15åˆ†ï¼Œè¿™è¡¨æ˜äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ½œåŠ›ä»¥åŠæˆ‘ä»¬æ‰€æå‡ºæ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¦‚<a href="#Table_2" class="tab-link" onclick="highlightAsset('Table_2'); return false;">è¡¨2</a>æ‰€ç¤ºï¼Œåœ¨è¿›åº¦åº¦é‡S-PRæ–¹é¢ï¼ŒCarPlannerç›¸æ¯”PDM-Closedæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨ç¢°æ’åº¦é‡S-CRä¸Šè¡¨ç°ç›¸å½“ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æé«˜é©¾é©¶æ•ˆç‡çš„åŒæ—¶ä¿æŒå®‰å…¨é©¾é©¶ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨å®éªŒä¸­æœªä½¿ç”¨ILä¸­å¸¸ç”¨çš„è¯¸å¦‚æ•°æ®å¢å¼º<a href="#ref-3" class="ref-link">[3, 4]</a>å’Œè‡ªä½“å†å²é®è”½<a href="#ref-11" class="ref-link">[11]</a>ç­‰æŠ€æœ¯ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å†…åœ¨èƒ½åŠ›ä»¥è§£å†³é—­ç¯ä»»åŠ¡ã€‚<div id="badge-52" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 52)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('52', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-53"><div class="row text-row"><div class="col-src">In the reactive setting, while our method performs well, it falls slightly short of PDM-Closed. This discrepancy arises because our model was trained exclusively in nonreactive settings and has not interacted with the IDM policy used by reactive settings; as a result, our model is less robust to disturbances generated by reactive agents during testing.</div><div class="col-trans" id="trans-53">åœ¨ååº”æ€§è®¾ç½®ä¸­ï¼Œè™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æŸäº›æ–¹é¢ä»ç•¥é€ŠäºPDM-Closedã€‚è¿™ç§å·®å¼‚çš„åŸå› åœ¨äºï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…åœ¨éååº”æ€§ç¯å¢ƒä¸­è¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶æœªä¸ååº”æ€§è®¾ç½®ä¸­ä½¿ç”¨çš„IDMç­–ç•¥è¿›è¡Œäº’åŠ¨ï¼›å› æ­¤ï¼Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œå½“é‡åˆ°ç”±ååº”æ€§ä»£ç†ç”Ÿæˆçš„å¹²æ‰°æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸å¤Ÿ robustã€‚<div id="badge-53" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 53)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('53', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-54"><div class="row text-row"><div class="col-src">[[HEADER: 4.3. Ablation Studies]]</div><div class="col-trans" id="trans-54"><b>4.3. Ablation Studies</b><div id="badge-54" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 54)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('54', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-55"><div class="row text-row"><div class="col-src">We investigate the effects of different design choices in RL training. The results are shown in Tab. 3. Inï¬‚uence of reward items. When using the quality reward only, the planner tends to generate static trajectories and achieves a low progress metric. This occurs because the ego vehicle begins in a safe, drivable state, but moving forward is at risk of collisions or leaving the drivable area.

On the other hand, when the quality reward is incorporated alongside the DE reward, it leads to signiï¬cant improvements in closed-loop metrics compared to using the DE reward alone. For instance, the S-CR metric rises from 97.49 to 99.22, and the S-Area metric rises from 96.91 to 99.22. These improvements indicate that the quality reward encourages safe and comfortable behaviors. Effectiveness of IVM.</div><div class="col-trans" id="trans-55">æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒè®¾è®¡é€‰æ‹©åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä¸­çš„å½±å“ã€‚ç»“æœå¦‚<a href="#Table_3" class="tab-link" onclick="highlightAsset('Table_3'); return false;">è¡¨3</a>æ‰€ç¤ºã€‚å¥–åŠ±é¡¹çš„å½±å“ã€‚ä»…ä½¿ç”¨è´¨é‡å¥–åŠ±æ—¶ï¼Œè§„åˆ’å™¨å€¾å‘äºç”Ÿæˆé™æ€è½¨è¿¹ï¼Œå¹¶ä¸”è¿›åº¦åº¦é‡è¾ƒä½ã€‚è¿™æ˜¯å› ä¸ºegoè½¦è¾†åˆå§‹çŠ¶æ€å¤„äºå®‰å…¨å¯è¡Œé©¶çš„çŠ¶æ€ï¼Œä½†å‘å‰ç§»åŠ¨åˆ™é¢ä¸´ç¢°æ’æˆ–ç¦»å¼€å¯è¡Œé©¶åŒºåŸŸçš„é£é™©ã€‚

å¦ä¸€æ–¹é¢ï¼Œåœ¨å¼•å…¥è´¨é‡å¥–åŠ±çš„åŒæ—¶ç»“åˆä½¿ç”¨DEå¥–åŠ±ï¼Œä¸å•ç‹¬ä½¿ç”¨DEå¥–åŠ±ç›¸æ¯”ï¼Œåœ¨é—­ç¯åº¦é‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼ŒS-CRæŒ‡æ ‡ä»97.49ä¸Šå‡åˆ°99.22ï¼ŒS-AreaæŒ‡æ ‡ä»96.91ä¸Šå‡åˆ°99.22ã€‚è¿™äº›æ”¹è¿›è¡¨æ˜ï¼Œè´¨é‡å¥–åŠ±é¼“åŠ±å®‰å…¨èˆ’é€‚çš„é©¾é©¶è¡Œä¸ºã€‚IVMçš„æœ‰æ•ˆæ€§ã€‚<div id="badge-55" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 55)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('55', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-56"><div class="row text-row"><div class="col-src">The results show that the coordinate transformation and KNN techniques in IVM notably improve closed-loop metrics and generator loss. For instance, with the coordinate transformation technique, the overall closed-loop score increases from 90.78 to 94.07, and S-PR rises from 91.37 to 95.06. These improvements are attributed to the enhanced accuracy of value estimation in RL, leading to generalized driving in closed-loop.</div><div class="col-trans" id="trans-56">ç»“æœè¡¨æ˜ï¼ŒIVMä¸­çš„åæ ‡å˜æ¢å’ŒKNNæŠ€æœ¯æ˜¾è‘—æé«˜äº†é—­ç¯æŒ‡æ ‡å’Œå‘ç”µæœºæŸå¤±ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åæ ‡å˜æ¢æŠ€æœ¯åï¼Œæ•´ä½“é—­ç¯å¾—åˆ†ä¸º94.07ï¼ˆåŸä¸º90.78ï¼‰ï¼ŒS-PRå€¼ä»91.37æé«˜åˆ°95.06ã€‚è¿™äº›æ”¹è¿›å½’å› äºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä»·å€¼ä¼°è®¡ç²¾åº¦çš„æå‡ï¼Œä»è€Œå®ç°äº†æ›´é€šç”¨çš„é—­ç¯é©¾é©¶è¡Œä¸ºã€‚<div id="badge-56" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 56)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('56', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-57"><div class="row text-row"><div class="col-src">[[HEADER: 4.4. Extention to IL]]</div><div class="col-trans" id="trans-57"><b>4.4. æ‰©å±•åˆ°IL</b><div id="badge-57" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 57)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('57', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-58"><div class="row text-row"><div class="col-src">In addition to designing for RL training, we also extend the CarPlanner to incorporate IL. We conduct rigorous analysis to compare the effects of various design choices in IL and RL training, as summarized in Tab. 4. Our ï¬ndings indicate that while mode dropout and selector side task contribute to both IL and RL training, ego-history dropout and backbone sharing, often effective in IL, are less suitable for RL. Ego-history dropout.

Previous works [1, 3, 4, 11] suggest that planners trained via IL may rely too heavily on past poses and neglect environmental state information. To counter this, we combine techniques from ChauffeurNet [1] and PlanTF [4] into an ego-history dropout module, randomly masking ego history poses and current velocity to alleviate the causal confusion issue.</div><div class="col-trans" id="trans-58">é™¤äº†ä¸ºRLè®­ç»ƒè¿›è¡Œè®¾è®¡å¤–ï¼Œæˆ‘ä»¬è¿˜å°†CarPlanneræ‰©å±•ä»¥ç»“åˆILã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†ææ¥æ¯”è¾ƒå„ç§è®¾è®¡é€‰æ‹©åœ¨ILå’ŒRLè®­ç»ƒä¸­çš„æ•ˆæœï¼Œå¹¶å°†è¿™äº›ç»“æœæ€»ç»“åœ¨<a href="#Table_4" class="tab-link" onclick="highlightAsset('Table_4'); return false;">è¡¨4</a>ä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ¨¡å¼ä¸¢å¼ƒå’Œé€‰æ‹©å™¨è¾…åŠ©ä»»åŠ¡å¯¹ILå’ŒRLè®­ç»ƒéƒ½æœ‰è´¡çŒ®ï¼Œè€Œego-history dropoutå’Œéª¨å¹²å…±äº«â€”â€”å°½ç®¡åœ¨ILä¸­é€šå¸¸æœ‰æ•ˆï¼Œåœ¨RLä¸­åˆ™ä¸å¤ªé€‚ç”¨ã€‚ego-history dropoutã€‚

ä»¥å¾€çš„ç ”ç©¶<a href="#ref-1" class="ref-link">[1, 3, 4, 11]</a>è¡¨æ˜ï¼Œé€šè¿‡ILè®­ç»ƒçš„è§„åˆ’è€…å¯èƒ½ä¼šè¿‡åº¦ä¾èµ–è¿‡å»çš„å§¿æ€ä¿¡æ¯å¹¶å¿½è§†ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†ChauffeurNet <a href="#ref-1" class="ref-link">[1]</a>å’ŒPlanTF <a href="#ref-4" class="ref-link">[4]</a>ä¸­çš„æŠ€æœ¯ç»“åˆèµ·æ¥ï¼Œè®¾è®¡äº†ä¸€ä¸ªego-history dropoutæ¨¡å—ï¼Œéšæœºé®è”½egoçš„å†å²å§¿æ€å’Œå½“å‰é€Ÿåº¦ä»¥ç¼“è§£å› æœæ··æ·†çš„é—®é¢˜ã€‚<div id="badge-58" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: Tab. 4åº”è¯¥ç¿»è¯‘ä¸ºè¡¨4ï¼Œè€Œä¸æ˜¯Table_4 </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 58)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">Tab. 4åº”è¯¥ç¿»è¯‘ä¸ºè¡¨4ï¼Œè€Œä¸æ˜¯Table_4</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('58', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-59"><div class="row text-row"><div class="col-src">Our experiments conï¬rm that ego-history dropout beniï¬ts IL training, as it improves performance across closed-loop metrics like S-CR and S-Area. However, in RL training, we observe a negative impact on advantage estimation due to ego-history dropout, which signiï¬cantly affects the value part of generator loss, leading to closed-loop performance degradation.

This suggests that RL training naturally addresses the causal confusion problem inherent in IL by uncovering causal relationships that align with the reward signal, which explicitly encodes task-oriented preferences. This capability highlights the potential of RL to push the boundaries of learning-based planning. Backbone sharing. This choice, often used in IL-based multi-modal planners, promotes feature sharing across tasks to improve generalization.</div><div class="col-trans" id="trans-59">æˆ‘ä»¬çš„å®éªŒç¡®è®¤äº†è‡ªæˆ‘å†å²ä¸¢å¼ƒå¯¹ILè®­ç»ƒæœ‰ç›Šï¼Œå› ä¸ºå®ƒåœ¨å¦‚S-CRå’ŒS-Areaç­‰é—­ç¯æŒ‡æ ‡ä¸Šæé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨RLè®­ç»ƒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç”±äºè‡ªæˆ‘å†å²ä¸¢å¼ƒå¯¼è‡´çš„ä¼˜åŠ¿ä¼°è®¡å‡ºç°äº†è´Ÿé¢å½±å“ï¼Œè¿™æ˜¾è‘—å½±å“äº†ç”Ÿæˆå™¨æŸå¤±ä¸­çš„ä»·å€¼éƒ¨åˆ†ï¼Œä»è€Œå¯¼è‡´é—­ç¯æ€§èƒ½ä¸‹é™ã€‚

è¿™è¡¨æ˜ï¼ŒRLè®­ç»ƒè‡ªç„¶è§£å†³äº†ILå›ºæœ‰çš„å› æœæ··æ·†é—®é¢˜ï¼Œé€šè¿‡æ­ç¤ºä¸å¥–åŠ±ä¿¡å·ç›¸ä¸€è‡´çš„å› æœå…³ç³»æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè€Œè¯¥å¥–åŠ±ä¿¡å·æ˜ç¡®ç¼–ç äº†ä»»åŠ¡å¯¼å‘çš„åå¥½ã€‚è¿™ç§èƒ½åŠ›çªæ˜¾äº†RLåœ¨åŸºäºå­¦ä¹ çš„è§„åˆ’æ–¹é¢çš„æ½œåŠ›ã€‚ä¸»å¹²ç½‘ç»œå…±äº«ã€‚è¿™ç§é€‰æ‹©é€šå¸¸åœ¨åŸºäºILçš„å¤šæ¨¡æ€è§„åˆ’å™¨ä¸­è¢«é‡‡ç”¨ï¼Œä¿ƒè¿›äº†ä¸åŒä»»åŠ¡é—´çš„ç‰¹å¾å…±äº«ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚<div id="badge-59" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 59)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('59', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-60"><div class="row text-row"><div class="col-src">While backbone sharing helps IL by balancing losses across trajectory generator and selector, we ï¬nd it adversely affects RL training. Speciï¬cally, backbone sharing leads to higher losses for both the trajectory generator and selector in RL, indicating that gradients from each task interfere. The divergent objectives in RL for trajectory generation and selection tasks seem to conï¬‚ict, reducing overall policy performance.

Consequently, we avoid backbone sharing in our RL framework to maintain task-speciï¬c gradient ï¬‚ow and improve policy quality.</div><div class="col-trans" id="trans-60">è™½ç„¶å…±äº«éª¨å¹²ç½‘ç»œæœ‰åŠ©äºè¿ç§»å­¦ä¹ ï¼ˆILï¼‰é€šè¿‡åœ¨è½¨è¿¹ç”Ÿæˆå™¨å’Œé€‰æ‹©å™¨ä¹‹é—´å¹³è¡¡æŸå¤±ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒå¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒäº§ç”Ÿäº†ä¸åˆ©å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œå…±äº«éª¨å¹²ç½‘ç»œå¯¼è‡´è½¨è¿¹ç”Ÿæˆå™¨å’Œé€‰æ‹©å™¨çš„æŸå¤±æ›´é«˜ï¼Œè¡¨æ˜æ¯ä¸ªä»»åŠ¡çš„æ¢¯åº¦ç›¸äº’å¹²æ‰°ã€‚è½¨è¿¹ç”Ÿæˆå’Œé€‰æ‹©ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡åˆ†æ­§ä¼¼ä¹å­˜åœ¨å†²çªï¼Œä»è€Œé™ä½äº†æ•´ä½“ç­–ç•¥æ€§èƒ½ã€‚

å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¿å…ä½¿ç”¨å…±äº«éª¨å¹²ç½‘ç»œä»¥ä¿æŒç‰¹å®šäºä»»åŠ¡çš„æ¢¯åº¦æµåŠ¨å¹¶æé«˜ç­–ç•¥è´¨é‡ã€‚<div id="badge-60" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 60)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('60', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-61"><div class="row text-row"><div class="col-src">[[HEADER: 4.5. Qualitative Results]]</div><div class="col-trans" id="trans-61"><b>4.5. å®šæ€§ç»“æœ</b><div id="badge-61" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 61)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('61', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_4"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_4</div><img src="./assets/Table_4.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 4. Effect of different components on IL and RL loss using our CarPlanner. Results are in Test14-random non-reactive benchmark.</div><div class="asset-desc-zh">è¡¨4. æˆ‘ä»¬ CarPlanner ä¸­ä¸åŒç»„ä»¶å¯¹ IL å’Œ RL æŸå¤±çš„å½±å“ã€‚ ç»“æœåŸºäº Test14-random éååº”å¼åŸºå‡†ã€‚</div></div></div></div><div class="row asset-row" id="Figure_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_3</div><img src="./assets/Figure_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 3. Qualitative comparison of PDM-Closed and our method in non-reactive environments. The scenario is annotated as waiting for pedestrian to cross. In each frame shot, ego vehicle is marked as green. Trafï¬c agents are marked as sky blue. Lineplot with blue is the ego planned trajectory.</div><div class="asset-desc-zh">å›¾3. PDM-Closed å’Œæˆ‘ä»¬æ–¹æ³•åœ¨éååº”å¼ç¯å¢ƒä¸­çš„å®šæ€§æ¯”è¾ƒã€‚ åœºæ™¯æ³¨é‡Šä¸ºç­‰å¾…è¡Œäººè¿‡é©¬è·¯ã€‚ åœ¨æ¯ä¸€å¸§ä¸­ï¼Œego è½¦è¾†æ ‡è®°ä¸ºç»¿è‰²ã€‚ äº¤é€šä»£ç†æ ‡è®°ä¸ºå¤©è“è‰²ã€‚ çº¿å›¾ä»¥è“è‰²è¡¨ç¤º ego è½¦è¾†çš„è®¡åˆ’è½¨è¿¹ã€‚</div></div></div></div><div class="row-container" id="task-62"><div class="row text-row"><div class="col-src">We provide qualitative results as shown in Fig. 3. In this scenario, ego vehicle is required to execute a right turn while navigating around pedestrians. In this case, Our method shows a smooth, efï¬cient performance. From tsim = 0s to tsim = 9s, all methods wait for the pedestrians to cross the road. At tsim = 10s, an unexpected pedestrian goes back and prepares to re-cross the road. PDM-Closed is

unaware of this situation and takes an emergency stop, but it still intersects with this pedestrian. In contrast, our IL variant displays an awareness of the pedestrianâ€™s movements and consequently conducts a braking maneuver. However, it still remains close to the pedestrian. Our RL method avoids this hazard by starting up early up to tsim = 9s and achieves the highest progress and safety metrics.</div><div class="col-trans" id="trans-62">æˆ‘ä»¬æä¾›äº†å®šæ€§ç»“æœï¼Œå¦‚<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">å›¾3</a>æ‰€ç¤ºã€‚åœ¨è¿™ç§åœºæ™¯ä¸­ï¼Œegoè½¦è¾†éœ€è¦åœ¨è¡Œäººå‘¨å›´å¯¼èˆªæ—¶æ‰§è¡Œå³è½¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºäº†ä¸€ç§å¹³æ»‘ä¸”é«˜æ•ˆçš„æ€§èƒ½ã€‚ä»tsim = 0ç§’åˆ°tsim = 9ç§’ï¼Œæ‰€æœ‰æ–¹æ³•éƒ½åœ¨ç­‰å¾…è¡Œäººè¿‡é©¬è·¯ã€‚åœ¨tsim = 10ç§’æ—¶ï¼Œå‡ºç°äº†ä¸€ä¸ªæ„æƒ³ä¸åˆ°çš„è¡Œäººåœ¨é€€å›å¹¶å‡†å¤‡é‡æ–°è¿‡é©¬è·¯çš„æƒ…å†µã€‚PDM-Closedå¯¹æ­¤æƒ…å†µä¸çŸ¥æƒ…ï¼Œå¹¶é‡‡å–äº†ç´§æ€¥åœè½¦æªæ–½ï¼Œä½†ä»ä¸è¯¥è¡Œäººå‘ç”Ÿäº†ç¢°æ’ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„ILå˜ä½“èƒ½å¤Ÿæ„è¯†åˆ°è¡Œäººçš„ç§»åŠ¨ï¼Œå¹¶å› æ­¤æ‰§è¡Œäº†ä¸€æ¬¡åˆ¶åŠ¨æ“ä½œã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶é è¿‘è¡Œäººã€‚æˆ‘ä»¬çš„RLæ–¹æ³•é€šè¿‡åœ¨tsim = 9ç§’ä¹‹å‰æå‰å¯åŠ¨æ¥é¿å…è¿™ä¸€å±é™©ï¼Œå¹¶å®ç°äº†æœ€é«˜çš„è¿›åº¦å’Œå®‰å…¨æ€§æŒ‡æ ‡ã€‚<div id="badge-62" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 62)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('62', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-63"><div class="row text-row"><div class="col-src">[[HEADER: 5. Conclusion]]</div><div class="col-trans" id="trans-63"><b>5. ç»“è®º</b><div id="badge-63" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 63)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('63', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-64"><div class="row text-row"><div class="col-src">In this paper, we introduce CarPlanner, a consistent auto-regressive planner aiming at large-scale RL training. Thanks to the proposed framework, we train an RL-based planner that outperforms existing RL-, IL-, and rule-based SOTAs. Furthermore, we provide analysis indicating the characteristics of IL and RL, highlighting the potential of RL to take a further step toward learning-based planning.

Limitations and future work. RL needs delicate design and is prone to input representation. RL can overï¬t its training environment and suffer from performance drop in unseen environments [21]. Our method leverages expert-aided reward design to guide exploration. However, this approach may constrain the full potential of RL, as it inherently relies on expert demonstrations and may hinder the discovery of solutions that surpass human expertise.</div><div class="col-trans" id="trans-64">åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CarPlannerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è¿›è¡Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„ä¸€è‡´è‡ªå›å½’è§„åˆ’å™¨ã€‚å¾—ç›Šäºæ‰€æå‡ºçš„æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºRLçš„è§„åˆ’å™¨ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºäºRLã€ILå’Œè§„åˆ™çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åˆ†æä»¥è¯´æ˜ILå’ŒRLçš„ç‰¹ç‚¹ï¼Œå¹¶çªæ˜¾äº†RLå‘åŸºäºå­¦ä¹ çš„è§„åˆ’è¿ˆè¿›çš„å¯èƒ½æ€§ã€‚

å±€é™æ€§å’Œæœªæ¥å·¥ä½œã€‚å¼ºåŒ–å­¦ä¹ éœ€è¦ç²¾ç»†çš„è®¾è®¡å¹¶ä¸”å®¹æ˜“å—åˆ°è¾“å…¥è¡¨ç¤ºçš„å½±å“ã€‚å¼ºåŒ–å­¦ä¹ å¯èƒ½ä¼šåœ¨å…¶è®­ç»ƒç¯å¢ƒä¸­è¿‡æ‹Ÿåˆï¼Œå¹¶åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è¡¨ç°ä¸‹é™<a href="#ref-21" class="ref-link">[21]</a>ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸“å®¶è¾…åŠ©å¥–åŠ±è®¾è®¡æ¥å¼•å¯¼æ¢ç´¢ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯èƒ½é™åˆ¶äº†RLçš„å…¨éƒ¨æ½œåŠ›ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šä¾èµ–äºä¸“å®¶æ¼”ç¤ºï¼Œå¯èƒ½ä¼šé˜»ç¢å‘ç°è¶…è¶Šäººç±»ä¸“ä¸šçŸ¥è¯†çš„è§£å†³æ–¹æ¡ˆã€‚<div id="badge-64" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 64)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('64', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-65"><div class="row text-row"><div class="col-src">Future work aims to develop robust RL algorithms capable of overcoming these limitations, enabling autonomous exploration and generalization across diverse environments.</div><div class="col-trans" id="trans-65">æœªæ¥çš„å·¥ä½œæ—¨åœ¨å¼€å‘ç¨³å¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥å…‹æœè¿™äº›é™åˆ¶ï¼Œä»è€Œå®ç°è‡ªä¸»æ¢ç´¢å¹¶åœ¨å¤šç§ç¯å¢ƒä¸­è¿›è¡Œæ³›åŒ–ã€‚<div id="badge-65" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 65)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('65', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-66"><div class="row text-row"><div class="col-src">[[HEADER: 6. Acknowledgement]]</div><div class="col-trans" id="trans-66"><b>6. è‡´è°¢</b><div id="badge-66" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 66)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('66', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-67"><div class="row text-row"><div class="col-src">Many thanks to Jingke Wang for helpful discussions and all reviewers for improving the paper. This work was supported by Zhejiang Provincial Natural Science Foundation of China under Grant No. LD24F030001, and by the National Nature Science Foundation of China under Grant 62373322.</div><div class="col-trans" id="trans-67">æ„Ÿè°¢ Jingke Wang åœ¨è®¨è®ºä¸­æä¾›çš„å¸®åŠ©ï¼Œä»¥åŠæ‰€æœ‰å®¡ç¨¿äººå¯¹æœ¬æ–‡çš„æ”¹è¿›ã€‚æœ¬ç ”ç©¶å¾—åˆ°äº†æµ™æ±Ÿçœè‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆé¡¹ç›®ç¼–å·ï¼šLD24F030001ï¼‰å’Œå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆé¡¹ç›®ç¼–å·ï¼š62373322ï¼‰çš„æ”¯æŒã€‚<div id="badge-67" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 67)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('67', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div></div><div class="ref-section"><h2>å‚è€ƒæ–‡çŒ® (References)</h2><div class="ref-content"><br><span id="ref-1" class="ref-anchor">[1]</span> Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. In Proc. of Robotics: Science and Systems, 2019. 3, 7 <br><span id="ref-2" class="ref-anchor">[2]</span> Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 2, 6, 12 <br><span id="ref-3" class="ref-anchor">[3]</span> Jie Cheng, Yingbing Chen, and Qifeng Chen.

Pluto: Pushing the limit of imitation learning-based planning for autonomous driving. arXiv preprint arXiv:2404.14327, 2024. 3, 6, 7, 12, 13 <br><span id="ref-4" class="ref-anchor">[4]</span> Jie Cheng, Yingbing Chen, Xiaodong Mei, Bowen Yang, Bo Li, and Ming Liu. Rethinking imitation-based planners for autonomous driving. In Proc. of the IEEE Intl. Conf. on Robotics & Automation, pages 14123â€“14130. IEEE, 2024. 3, 6, 7 <br><span id="ref-5" class="ref-anchor">[5]</span> Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger.

Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), 2022. 3 <br><span id="ref-6" class="ref-anchor">[6]</span> Ignasi Clavera, Yao Fu, and Pieter Abbeel. Modelaugmented actor-critic: Backpropagating through paths. In Proc. of the Int. Conf. on Learning Representations, 2020. 15 <br><span id="ref-7" class="ref-anchor">[7]</span> Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. Parting with misconceptions about learningbased vehicle motion planning. In Proc. of the Conf.

on Robot Learning, 2023. 6 <br><span id="ref-8" class="ref-anchor">[8]</span> Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Proc. of the Advances in Neural Information Processing Systems, 32, 2019. 1, 3 <br><span id="ref-9" class="ref-anchor">[9]</span> Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proc. of the IEEE/CVF Intl. Conf.

on Computer Vision, pages 9710â€“9719, 2021. 2, 12 <br><span id="ref-10" class="ref-anchor">[10]</span> Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized representation. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 11525â€“11533, 2020. 3 <br><span id="ref-11" class="ref-anchor">[11]</span> Ke Guo, Wei Jing, Junbo Chen, and Jia Pan. CCIL: Contextconditioned imitation learning for urban driving. In Proc. of Robotics: Science and Systems, 2023.

1, 7 <br><span id="ref-12" class="ref-anchor">[12]</span> Marcel Hallgarten, Martin Stoll, and Andreas Zell. From prediction to planning with goal conditioned lane graph traversals. arXiv preprint arXiv:2302.07753, 2023. 6 <br><span id="ref-13" class="ref-anchor">[13]</span> Nicklas A Hansen, Hao Su, and Xiaolong Wang. Temporal difference learning for model predictive control. In Proc. of the Intl. Conf. on Machine Learning, pages 8387â€“ 8406. PMLR, 2022. 15 <br><span id="ref-14" class="ref-anchor">[14]</span> Xiangkun He, Haohan Yang, Zhongxu Hu, and Chen Lv.

Robust lane change decision making for autonomous vehicles: An observation adversarial reinforcement learning approach. IEEE Transactions on Intelligent Vehicles, 8(1):184â€“ 193, 2022. 3 <br><span id="ref-15" class="ref-anchor">[15]</span> John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one hours: Self-driving motion prediction dataset. In Proc. of the Conf. on Robot Learning, pages 409â€“418, 2021.

2 <br><span id="ref-16" class="ref-anchor">[16]</span> Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 17853â€“17862, 2023. 1 <br><span id="ref-17" class="ref-anchor">[17]</span> Zhiyu Huang, Haochen Liu, and Chen Lv. Gameformer: Game-theoretic modeling and learning of transformer-based interactive prediction and planning for autonomous driving. In Proc. of the IEEE/CVF Intl. Conf.

on Computer Vision, pages 3903â€“3913, 2023. 2, 3, 6, 13 <br><span id="ref-18" class="ref-anchor">[18]</span> Zhiyu Huang, Xinshuo Weng, Maximilian Igl, Yuxiao Chen, Yulong Cao, Boris Ivanovic, Marco Pavone, and Chen Lv. Gen-drive: Enhancing diffusion generative driving policies with reward modeling and reinforcement learning ï¬netuning. arXiv preprint arXiv:2410.05582, 2024. 2, 6, 12 <br><span id="ref-19" class="ref-anchor">[19]</span> Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey Levine.

How to train your robot with deep reinforcement learning: lessons we have learned. Intl. Journal of Robotics Research, 40(4-5):698â€“721, 2021. 1 <br><span id="ref-20" class="ref-anchor">[20]</span> Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 9644â€“9653, 2023. 2 <br><span id="ref-21" class="ref-anchor">[21]</span> Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim RocktÂ¨aschel.

A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artiï¬cial Intelligence Research, 76:201â€“264, 2023. 8 <br><span id="ref-22" class="ref-anchor">[22]</span> Edouard Leurent and Jean Mercat. Social attention for autonomous decision-making in dense trafï¬c. arXiv preprint arXiv:1911.12250, 2019. 3 <br><span id="ref-23" class="ref-anchor">[23]</span> Guofa Li, Yifan Yang, Shen Li, Xingda Qu, Nengchao Lyu, and Shengbo Eben Li. Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness.

Transportation research part C: emerging technologies, 134:103452, 2022. 3 <br><span id="ref-24" class="ref-anchor">[24]</span> Quanyi Li, Zhenghao Mark Peng, Lan Feng, Zhizheng Liu, Chenda Duan, Wenjie Mo, and Bolei Zhou. Scenarionet: Open-source platform for large-scale trafï¬c scenario simulation and modeling. Proc. of the Advances in Neural Information Processing Systems, 36:3894â€“3920, 2023.

12 <br><span id="ref-25" class="ref-anchor">[25]</span> Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Proc. of the Advances in Neural Information Processing Systems, 35: 27730â€“27744, 2022. 1 <br><span id="ref-26" class="ref-anchor">[26]</span> Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.

Pointnet: Deep learning on point sets for 3d classiï¬cation and segmentation. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 652â€“660, 2017. 4 <br><span id="ref-27" class="ref-anchor">[27]</span> Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals in visual multi-agent settings. In Proc. of the IEEE/CVF Intl. Conf. on Computer Vision, pages 2821â€“2830, 2019. 2 <br><span id="ref-28" class="ref-anchor">[28]</span> StÂ´ephane Ross, Geoffrey Gordon, and Drew Bagnell.

A reduction of imitation learning and structured prediction to noregret online learning. In Proceedings of the fourteenth international conference on artiï¬cial intelligence and statistics, pages 627â€“635, 2011. 1, 3 <br><span id="ref-29" class="ref-anchor">[29]</span> Oliver Scheel, Luca Bergamini, Maciej Wolczyk, BÅ‚aË™zej OsiÂ´nski, and Peter Ondruska. Urban driver: Learning to drive from real-world demonstrations using policy gradients. In Proc. of the Conf. on Robot Learning, pages 718â€“728, 2022.

1, 3, 6 <br><span id="ref-30" class="ref-anchor">[30]</span> John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. 11, 12 <br><span id="ref-31" class="ref-anchor">[31]</span> John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

6, 11 <br><span id="ref-32" class="ref-anchor">[32]</span> Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proc. of the IEEE/CVF Intl. Conf. on Computer Vision, pages 8579â€“8590, 2023. 2 <br><span id="ref-33" class="ref-anchor">[33]</span> Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.

Motion transformer with global intention localization and local movement reï¬nement. Proc. of the Advances in Neural Information Processing Systems, 35:6531â€“6543, 2022. 2 <br><span id="ref-34" class="ref-anchor">[34]</span> David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484â€“489, 2016.

1 <br><span id="ref-35" class="ref-anchor">[35]</span> Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafï¬csim: Learning to simulate realistic multiagent behaviors. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 10400â€“10409, 2021. 13 <br><span id="ref-36" class="ref-anchor">[36]</span> Ardi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro Fishman, and Naveed Muhammad. A survey of end-to-end driving: Architectures and training methods. IEEE Transactions on Neural Networks and Learning Systems, 33(4): 1364â€“1384, 2020.

1 <br><span id="ref-37" class="ref-anchor">[37]</span> Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested trafï¬c states in empirical observations and microscopic simulations. Physical review E, 62(2):1805, 2000. 6 <br><span id="ref-38" class="ref-anchor">[38]</span> Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Proc. of the Advances in Neural Information Processing Systems, 30, 2017. 4 <br><span id="ref-39" class="ref-anchor">[39]</span> Huanjie Wang, Shihua Yuan, Mengyu Guo, Xueyuan Li, and Wei Lan.

A deep reinforcement learning-based approach for autonomous driving in highway on-ramp merge. Proceedings of the Institution of Mechanical engineers, Part D: Journal of Automobile engineering, 235(10-11):2726â€“2739, 2021. 3 <br><span id="ref-40" class="ref-anchor">[40]</span> Letian Wang, Jie Liu, Hao Shao, Wenshuo Wang, Ruobing Chen, Yu Liu, and Steven L Waslander. Efï¬cient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors. In Proc. of Robotics: Science and Systems, Daegu, Republic of Korea, 2023.

3 <br><span id="ref-41" class="ref-anchor">[41]</span> Wei Wu, Xiaoxin Feng, Ziyan Gao, and Yuheng Kan. Smart:

Scalable multi-agent real-time simulation via next-token prediction. arXiv preprint arXiv:2405.15677, 2024. 2 <br><span id="ref-42" class="ref-anchor">[42]</span> Dongkun Zhang, Jiaming Liang, Sha Lu, Ke Guo, Qi Wang, Rong Xiong, Zhenwei Miao, and Yue Wang. Pep: Policyembedded trajectory planning for autonomous driving. IEEE Robotics and Automation Letters, 2024. 3, 6 <br><span id="ref-43" class="ref-anchor">[43]</span> Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, and Luc V Gool.

Real-time motion prediction via heterogeneous polyline transformer with relative pose encoding. Proc. of the Advances in Neural Information Processing Systems, 36, 2024. 13 <br><span id="ref-44" class="ref-anchor">[44]</span> Tong Zhou, Letian Wang, Ruobing Chen, Wenshuo Wang, and Yu Liu. Accelerating reinforcement learning for autonomous driving using task-agnostic and ego-centric motion skills. In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems, pages 11289â€“11296. IEEE, 2023.

3 <br><span id="ref-45" class="ref-anchor">[45]</span> Yang Zhou, Hao Shao, Letian Wang, Steven L Waslander, Hongsheng Li, and Yu Liu. Smartreï¬ne: A scenario-adaptive reï¬nement framework for efï¬cient motion prediction. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 15281â€“15290, 2024. 2 <br><span id="ref-46" class="ref-anchor">[46]</span> Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and Chun Jason Xue. Behaviorgpt: Smart agent simulation for autonomous driving with next-patch prediction.

arXiv preprint arXiv:2405.17372, 2024. 2</div></div></div>
    
    <script>
    const CURRENT_FILENAME = "Zhang_CarPlanner_Consistent_Auto-regressive_Trajectory_Planning_for_Large-Scale_Reinforcement_Learning_in_CVPR_2025_paper";
    let isFeedbackMode = false;
    let isTranslating = false;
    let eventSource = null;

    function toggleFeedbackMode() {
        if (isTranslating) return;
        isFeedbackMode = !isFeedbackMode;
        document.body.classList.toggle('feedback-mode');
        updateUI();
    }

    function updateUI() {
        const toggleBtn = document.getElementById('toggle-btn');
        const runBtn = document.getElementById('run-btn');
        const statusText = document.getElementById('status-text');
        
        if (isTranslating) {
            toggleBtn.style.display = 'none';
            runBtn.style.display = 'block';
            runBtn.textContent = "ğŸ›‘ åœæ­¢é‡è¯‘ (Running...)";
            runBtn.classList.replace('btn-success', 'btn-danger');
            runBtn.onclick = stopRerun; 
            statusText.textContent = "â³ æ­£åœ¨åå°é‡è¯‘...";
            disableClickHandlers();
        } else if (isFeedbackMode) {
            toggleBtn.style.display = 'block';
            toggleBtn.textContent = "é€€å‡ºçº é”™æ¨¡å¼";
            toggleBtn.classList.replace('btn-primary', 'btn-danger');
            
            runBtn.style.display = 'block';
            runBtn.textContent = "ğŸš€ åº”ç”¨ä¿®æ”¹å¹¶é‡è¯‘";
            runBtn.classList.replace('btn-danger', 'btn-success');
            runBtn.onclick = triggerRerun;
            
            statusText.textContent = "âœï¸ çº é”™æ¨¡å¼ï¼šç‚¹å‡»é»„è‰²åŒºåŸŸå³å¯ä¿®æ”¹";
            enableClickHandlers();
        } else {
            toggleBtn.style.display = 'block';
            toggleBtn.textContent = "è¿›å…¥çº é”™æ¨¡å¼";
            toggleBtn.classList.replace('btn-danger', 'btn-primary');
            runBtn.style.display = 'none';
            statusText.textContent = "æµè§ˆæ¨¡å¼";
            disableClickHandlers();
        }
    }

    function enableClickHandlers() {
        const rows = document.querySelectorAll('.row-container');
        rows.forEach(row => {
            const transCol = row.querySelector('.col-trans');
            if (transCol.getAttribute('data-bound')) return;
            transCol.setAttribute('data-bound', 'true');
            transCol.onclick = () => {
                if (!isFeedbackMode || isTranslating) return;
                const panel = row.querySelector('.feedback-panel');
                const isHidden = (panel.style.display === 'none' || panel.style.display === '');
                panel.style.display = isHidden ? 'block' : 'none';
            };
        });
    }

    function disableClickHandlers() {
        const panels = document.querySelectorAll('.feedback-panel');
        panels.forEach(p => p.style.display = 'none');
    }

    async function triggerRerun() {
        if (!confirm("ç¡®å®šè¦æ ¹æ®æ‚¨çš„ä¿®æ”¹å»ºè®®é‡æ–°ç¿»è¯‘å—ï¼Ÿ")) return;
        isTranslating = true;
        updateUI();
        
        try {
            const response = await fetch('/api/feedback/rerun', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ filename: CURRENT_FILENAME })
            });
            startSSE();
        } catch (err) {
            alert("è¯·æ±‚é”™è¯¯: " + err);
            isTranslating = false;
            updateUI();
        }
    }

    async function stopRerun() {
        if(!confirm("ç¡®å®šè¦ç»ˆæ­¢åå°ä»»åŠ¡å—ï¼Ÿ")) return;
        try {
            await fetch('/api/workflow/stop/' + CURRENT_FILENAME, { method: 'POST' });
            if(eventSource) eventSource.close();
            isTranslating = false;
            updateUI();
            alert("å·²å‘é€åœæ­¢ä¿¡å·ã€‚è¯·ç¨ååˆ·æ–°æŸ¥çœ‹å·²å®Œæˆçš„éƒ¨åˆ†ã€‚");
        } catch(e) {
            alert("åœæ­¢å¤±è´¥: " + e);
        }
    }

    function startSSE() {
        if(eventSource) eventSource.close();
        const url = '/api/stream/translation/' + CURRENT_FILENAME;
        eventSource = new EventSource(url);
        
        eventSource.onmessage = (e) => {
            const tasks = JSON.parse(e.data);
            const statusText = document.getElementById('status-text');
            const done = tasks.filter(t => t.status === 'success').length;
            const pending = tasks.filter(t => t.status === 'pending').length;
            statusText.textContent = `â³ é‡è¯‘ä¸­... (å‰©ä½™: ${pending})`;
        };
        
        eventSource.addEventListener('close', () => {
            eventSource.close();
            isTranslating = false;
            const statusText = document.getElementById('status-text');
            statusText.textContent = "âœ… å®Œæˆï¼æ­£åœ¨åˆ·æ–°...";
            setTimeout(() => { location.reload(); }, 1500);
        });
        
        eventSource.onerror = (err) => {
            console.warn("SSE è¿çº¿æ³¢åŠ¨", err);
        };
    }

    async function saveFeedback(taskId, btnElement) {
        const container = document.getElementById('task-' + taskId);
        const input = container.querySelector('.feedback-input');
        const hint = input.value.trim();
        const statusMsg = container.querySelector('.status-saved');
        const badge = document.getElementById('badge-' + taskId);
        
        if (!hint) { alert("è¯·è¾“å…¥æç¤º"); return; }
        
        const originalText = btnElement.textContent;
        btnElement.disabled = true;
        btnElement.textContent = "ä¿å­˜ä¸­...";
        
        try {
            const response = await fetch('/api/feedback/update', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    filename: CURRENT_FILENAME,
                    id: taskId, 
                    hint: hint 
                })
            });
            const data = await response.json();
            if (data.status === 'success') {
                statusMsg.style.display = 'inline';
                setTimeout(() => statusMsg.style.display = 'none', 2000);
                btnElement.textContent = "å·²ä¿å­˜ (å¾…é‡è¯‘)";
                
                if(badge) {
                    badge.innerHTML = `ğŸ’¡ ä¸Šæ¬¡æç¤º: ${hint} (â³ ç­‰å¾…é‡è¯‘)`;
                    badge.classList.add('has-hint');
                }
            } else {
                alert("ä¿å­˜å¤±è´¥: " + data.msg);
                btnElement.disabled = false;
                btnElement.textContent = originalText;
            }
        } catch (err) {
            alert("è¿æ¥é”™è¯¯: " + err);
            btnElement.disabled = false;
            btnElement.textContent = originalText;
        }
    }
    
    function highlightAsset(id) {
        const el = document.getElementById(id);
        if (el) {
            el.scrollIntoView({ behavior: 'smooth', block: 'center' });
            el.classList.remove('highlight-asset');
            void el.offsetWidth;
            el.classList.add('highlight-asset');
        }
    }
    </script>
    </body></html>