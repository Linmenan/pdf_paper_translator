<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><title>A_Survey_of_Reinforcement_Learning-Based_Motion_Planning_for_Autonomous_Driving_Lessons_Learned_from_a_Driving_Task_Perspective - Interactive Mode</title><style>
    :root { --primary: #2c3e50; --accent: #3498db; --bg: #f8f9fa; --border: #e0e0e0; --edit-bg: #fff3e0; --edit-border: #ffb74d; --edit-hover: #ffe0b2; }
    body { font-family: "Segoe UI", sans-serif; margin: 0; background: var(--bg); padding-bottom: 100px; scroll-behavior: smooth; }
    .container { max-width: 1200px; margin: 0 auto; background: #fff; box-shadow: 0 0 20px rgba(0,0,0,0.05); border: 2px solid transparent; transition: border 0.3s; }
    
    /* === çº é”™æ¨¡å¼è§†è§‰çŠ¶æ€ === */
    /* 1. å®¹å™¨è¾¹æ¡†å˜æ©™è‰²ï¼Œæç¤ºâ€œæ­£åœ¨ç¼–è¾‘çŠ¶æ€â€ */
    body.feedback-mode .container { border: 2px solid var(--edit-border); box-shadow: 0 0 30px rgba(255, 166, 0, 0.15); }
    
    /* 2. åªæœ‰ã€è¯‘æ–‡æ ¼å­ã€‘å˜è‰²ï¼Œå…¶ä»–ä¿æŒç™½è‰² */
    body.feedback-mode .col-trans { 
        cursor: pointer; 
        background-color: var(--edit-bg) !important; /* æš–é»„è‰²èƒŒæ™¯ */
        border-left: 2px solid transparent;
    }
    
    /* 3. æ‚¬åœæ•ˆæœåŠ æ·± */
    body.feedback-mode .col-trans:hover { 
        background-color: var(--edit-hover) !important; /* æ›´æ·±çš„é»„è‰² */
        border-left: 2px solid #e67e22;
    }
    
    /* 4. æç¤ºå¾½ç« ï¼šä»…åœ¨çº é”™æ¨¡å¼ä¸”æœ‰å†…å®¹æ—¶æ˜¾ç¤º */
    .hint-badge { display: none; margin-top: 10px; padding: 5px 10px; background: #fff3cd; border: 1px solid #ffeeba; color: #856404; font-size: 0.85em; border-radius: 4px; }
    body.feedback-mode .hint-badge.has-hint { display: block !important; animation: slideDown 0.3s; }
    
    .asset-img { max-width: 100%; height: auto; display: block; margin: 0 auto; }
    .asset-card { background: #fff; max-width: 95%; margin: 0 auto; border-radius: 8px; padding: 10px; text-align: center; }
    .ref-section { padding: 40px; background: #fff; border-top: 2px solid #eee; }
    .ref-content { font-family: "Times New Roman", serif; color: #444; line-height: 1.6; }
    .ref-anchor { color: var(--accent); font-weight: bold; }
    
    .meta-section { padding: 40px; text-align: center; background: #fff; }
    .meta-title-en { font-size: 1.8em; color: #2c3e50; font-weight: 700; }
    .meta-title-zh { font-size: 1.6em; color: #34495e; font-weight: 400; }
    .meta-author-en { font-style: italic; color: #7f8c8d; }
    .meta-author-zh { color: #16a085; font-weight: bold; }
    
    .toolbar { position: fixed; top: 20px; right: 20px; background: #fff; padding: 10px 20px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border-radius: 8px; z-index: 999; display: flex; gap: 10px; align-items: center; }
    .btn { padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; font-weight: bold; transition: 0.2s; }
    .btn-primary { background: var(--accent); color: #fff; }
    .btn-danger { background: #e74c3c; color: #fff; }
    .btn-success { background: #27ae60; color: #fff; }
    .btn:disabled { background: #ccc; cursor: not-allowed; }
    
    .row-container { border-bottom: 1px solid var(--border); }
    .row { display: flex; }
    .col-src, .col-trans { flex: 1; padding: 20px; transition: all 0.3s; }
    .col-src { border-right: 1px solid var(--border); color: #555; background: #fff; }
    
    .feedback-panel { background: #f1f8ff; padding: 15px 20px; border-top: 1px solid #d6eaf8; display: none; }
    .feedback-header { font-weight: bold; color: #2c3e50; margin-bottom: 5px; font-size: 0.9em; }
    .feedback-input { width: 100%; height: 60px; padding: 8px; border: 1px solid #bdc3c7; border-radius: 4px; font-family: inherit; margin-bottom: 5px; }
    .status-saved { color: #27ae60; font-weight: bold; margin-left: 10px; display: none; }
    .asset-row { background: #f4f4f4; padding: 20px; display: block; scroll-margin-top: 80px; transition: background 0.5s; }
    
    a.fig-link, a.tab-link, a.eq-link, a.ref-link { color: var(--accent); text-decoration: none; border-bottom: 1px dotted var(--accent); cursor: pointer; }
    a.fig-link:hover, a.tab-link:hover { background: #eaf6ff; }
    
    @keyframes slideDown { from { opacity: 0; transform: translateY(-5px); } to { opacity: 1; transform: translateY(0); } }
    @keyframes highlight-pulse { 0% { background: #fff3cd; } 100% { background: #f4f4f4; } }
    .highlight-asset { animation: highlight-pulse 2s ease-out; }
    </style></head>
    <body>
    
    <div class="toolbar">
        <div id="status-text" style="margin-right: 10px; color: #666;">æµè§ˆæ¨¡å¼</div>
        <button class="btn btn-primary" id="toggle-btn" onclick="toggleFeedbackMode()">è¿›å…¥çº é”™æ¨¡å¼</button>
        <button class="btn btn-success" id="run-btn" onclick="triggerRerun()" style="display:none;">ğŸš€ åº”ç”¨ä¿®æ”¹å¹¶é‡è¯‘</button>
    </div>
    
    <div class="container"><div class="meta-section"><h1 class="meta-title-en">A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective</h1><h1 class="meta-title-zh">åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»é©¾é©¶è¿åŠ¨è§„åˆ’ç»¼è¿°ï¼šä»é©¾é©¶ä»»åŠ¡è§†è§’å­¦åˆ°çš„ç»éªŒ</h1><div class="meta-author-en">Zhuoren Li, Guizhe Jin, Ran Yu, Zhiwen Chen, Wei Han, Nan Li, Lu Xiong, Bo Leng, Jia Hu, Ilya Kolmanovsky and Dimitar Filev</div><div class="meta-author-zh">æ Zhuoren, é‡‘ Guizhe, äº Ran, é™ˆ Zhiwen, éŸ© Wei, æ Nan, é‚ªé¾™ Lu, æ¢µ Leng, è™¾ Jia, ç§‘å°”æ›¼æ–¯åŸº Ilya å’Œ Filev Dimitar</div></div><hr class="meta-divider"><div class="main-content"><div class="row-container" id="task-1"><div class="row text-row"><div class="col-src">[[HEADER: Abstractâ€”]]</div><div class="col-trans" id="trans-1"><b>æ‘˜è¦â€”</b><div id="badge-1" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 1)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('1', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-2"><div class="row text-row"><div class="col-src">einforcement learning (RL), with its ability to explore and optimize policies in complex, dynamic decisionmaking tasks, has emerged as a promising approach to addressing motion planning (MoP) challenges in autonomous driving (AD). Despite rapid advancements in RL and AD, a systematic description and interpretation of the RL design process tailored to diverse driving tasks remains underdeveloped.

This survey provides a comprehensive review of RL-based MoP for AD, focusing on lessons from task-specific perspectives. We first outline the fundamentals of RL methodologies, and then survey their applications in MoP, analyzing scenario-specific features and task requirements to shed light on their influence on RL design choices.</div><div class="col-trans" id="trans-2">åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œç”±äºå…¶åœ¨å¤æ‚åŠ¨æ€å†³ç­–ä»»åŠ¡ä¸­æ¢ç´¢å’Œä¼˜åŒ–ç­–ç•¥çš„èƒ½åŠ›ï¼Œå·²æˆä¸ºè§£å†³è‡ªä¸»é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸè¿åŠ¨è§„åˆ’ï¼ˆMoPï¼‰æŒ‘æˆ˜çš„æœ‰å‰é€”çš„æ–¹æ³•ä¹‹ä¸€ã€‚å°½ç®¡åœ¨RLå’ŒADé¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†é’ˆå¯¹å¤šæ ·åŒ–çš„é©¾é©¶ä»»åŠ¡å®šåˆ¶åŒ–åœ°æè¿°å’Œè§£é‡ŠRLè®¾è®¡è¿‡ç¨‹ä»ç„¶ä¸å¤Ÿå®Œå–„ã€‚

æœ¬ç»¼è¿°æä¾›äº†ä¸€ç§å…¨é¢å›é¡¾åŸºäºRLçš„MoPæ–¹æ³•ä»¥åº”å¯¹ADæŒ‘æˆ˜çš„æ–¹å¼ï¼Œé‡ç‚¹å…³æ³¨ä»ç‰¹å®šä»»åŠ¡è§†è§’è·å¾—çš„ç»éªŒæ•™è®­ã€‚æˆ‘ä»¬é¦–å…ˆæ¦‚è¿°äº†RLæ–¹æ³•çš„åŸºæœ¬åŸç†ï¼Œç„¶åæ¢è®¨å…¶åœ¨MoPä¸­çš„åº”ç”¨ï¼Œåˆ†æå…·ä½“åœºæ™¯çš„ç‰¹ç‚¹åŠä»»åŠ¡éœ€æ±‚ï¼Œä»è€Œé˜æ˜è¿™äº›å› ç´ å¯¹RLè®¾è®¡é€‰æ‹©çš„å½±å“ã€‚<div id="badge-2" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 2)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('2', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-3"><div class="row text-row"><div class="col-src">Building on this analysis, we summarize key design experiences, extract insights from various driving task applications, and provide guidance for future implementations. Additionally, we examine the frontier challenges in RL-based MoP, review recent efforts to addresse these challenges, and propose strategies for overcoming unresolved issues.

Index Termsâ€”Reinforcement learning, autonomous driving, motion planning, survey.</div><div class="col-trans" id="trans-3">åŸºäºä¸Šè¿°åˆ†æï¼Œæˆ‘ä»¬æ€»ç»“äº†å…³é”®è®¾è®¡ç»éªŒï¼Œä»å„ç§é©¾é©¶ä»»åŠ¡åº”ç”¨ä¸­æå–è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„å®æ–½æä¾›æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒå¯Ÿäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿åŠ¨è§„åˆ’ï¼ˆMoPï¼‰é¢†åŸŸçš„å‰æ²¿æŒ‘æˆ˜ï¼Œå›é¡¾äº†è¿‘æœŸä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜æ‰€åšå‡ºçš„åŠªåŠ›ï¼Œå¹¶æå‡ºäº†å…‹æœæœªè§£å†³é—®é¢˜çš„ç­–ç•¥ã€‚

å…³é”®è¯â€”å¼ºåŒ–å­¦ä¹ ï¼Œè‡ªä¸»é©¾é©¶ï¼Œè¿åŠ¨è§„åˆ’ï¼Œç»¼è¿°ã€‚<div id="badge-3" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 3)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('3', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-4"><div class="row text-row"><div class="col-src">[[HEADER: I. INTRODUCTION]]</div><div class="col-trans" id="trans-4"><b>I. å¼•è¨€</b><div id="badge-4" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 4)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('4', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_1</div><img src="./assets/Figure_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Fig. 1. Search result of Web of Science until 2024: (a) topic search for RL and AD. (b) topic search for surveys for RL, AD, and RL-based AD.</div><div class="asset-desc-zh">Fig. 1. Web of Scienceæœç´¢ç»“æœè‡³2024ï¼š(a) å¯¹RLå’ŒADçš„ä¸»é¢˜æœç´¢ã€‚(b) å¯¹RLã€ADåŠåŸºäºRLçš„ADçš„ç»¼è¿°ä¸»é¢˜æœç´¢ã€‚</div></div></div></div><div class="row-container" id="task-5"><div class="row text-row"><div class="col-src">EINFORCEMENT learning (RL) is a machine learning paradigm that focuses on solving sequential decisionmaking and control challenges [1]. In contrast to supervised learning such as imitation learning (IL) [2]), where the agent directly learns a policy with labels of expert data, an RL agent generates its policy by interacting with the environment, and evaluating and iterating itself by statistically maximizing long-term rewards with its trial-and-error property [3].

The RL agent still learns a mapping between inputs and outputs rather than hidden patterns within the data. With RL methods surpassing human world champions in Go [4], Starcraft II [5], automobile racing [6], and drone racing [7], RL has been recognized as a promising approach for AD, especially for motion planning (MoP) [8], [9].</div><div class="col-trans" id="trans-5">å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œä¸“æ³¨äºè§£å†³åºåˆ—å†³ç­–å’Œæ§åˆ¶æŒ‘æˆ˜<a href="#ref-1" class="ref-link">[1]</a>ã€‚ä¸ç›‘ç£å­¦ä¹ å¦‚æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰<a href="#ref-2" class="ref-link">[2]</a>ä¸åŒï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­ä»£ç†ç›´æ¥ä»ä¸“å®¶æ•°æ®çš„æ ‡ç­¾ä¸­å­¦ä¹ ç­–ç•¥ï¼Œè€ŒRLä»£ç†é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ç”Ÿæˆå…¶ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨è¯•é”™æ€§è´¨ç»Ÿè®¡åœ°æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±æ¥è¯„ä¼°å’Œè¿­ä»£è‡ªèº«<a href="#ref-3" class="ref-link">[3]</a>ã€‚

å°½ç®¡å¦‚æ­¤ï¼ŒRLä»£ç†ä»ç„¶å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºä¹‹é—´çš„æ˜ å°„ï¼Œè€Œä¸æ˜¯æ•°æ®å†…éƒ¨éšè—çš„æ¨¡å¼ã€‚éšç€RLæ–¹æ³•åœ¨å›´æ£‹<a href="#ref-4" class="ref-link">[4]</a>ã€æ˜Ÿé™…äº‰éœ¸II<a href="#ref-5" class="ref-link">[5]</a>ã€æ±½è½¦èµ›è½¦<a href="#ref-6" class="ref-link">[6]</a>ä»¥åŠæ— äººæœºç«é€Ÿ<a href="#ref-7" class="ref-link">[7]</a>ä¸­è¶…è¶Šäººç±»ä¸–ç•Œå† å†›ï¼ŒRLå·²è¢«è®¤ä¸ºæ˜¯è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿åŠ¨è§„åˆ’ï¼ˆMoPï¼‰æ–¹é¢<a href="#ref-8" class="ref-link">[8]</a><a href="#ref-9" class="ref-link">[9]</a>ã€‚<div id="badge-5" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 5)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('5', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-6"><div class="row text-row"><div class="col-src">According to the search results from Web of Science (WOS), the number of research papers on the RL and AD topics has surged over the past decade, as shown in Fig. 1. In particular, owing to the complexity of interaction with the environment in different MoP problems [10], [11], RL has proven highly applicable to these tasks [12]. Recently, research on RL technologies applied to MoP has explored in a variety of driving tasks [13].

Most existing surveys focus on the overall technology of AD, or focus on specific functions such as localization, perception (especially object detection), communication, etc., with very few studies on MoP [14]. There are even fewer surveys summarizing RL-based MoP studies. Several references such as [3], [15] have reviewed some studies and applications of RL-based MoP for AD.</div><div class="col-trans" id="trans-6">æ ¹æ®Web of Science (WOS)çš„æœç´¢ç»“æœï¼Œè¿‡å»åå¹´å…³äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä¸»é¢˜çš„ç ”ç©¶è®ºæ–‡æ•°é‡æ€¥å‰§å¢åŠ ï¼Œå¦‚<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">å›¾1</a>æ‰€ç¤ºã€‚ç‰¹åˆ«æ˜¯ç”±äºåœ¨ä¸åŒåˆ¶é€ è¿‡ç¨‹é—®é¢˜ï¼ˆMoPï¼‰ä¸­ä¸ç¯å¢ƒäº¤äº’çš„å¤æ‚æ€§<a href="#ref-10" class="ref-link">[10]</a>ã€<a href="#ref-11" class="ref-link">[11]</a>ï¼Œå¼ºåŒ–å­¦ä¹ å·²è¢«è¯æ˜å¯¹è¿™äº›ä»»åŠ¡å…·æœ‰é«˜åº¦é€‚ç”¨æ€§<a href="#ref-12" class="ref-link">[12]</a>ã€‚æœ€è¿‘ï¼Œå…³äºåº”ç”¨äºMoPçš„RLæŠ€æœ¯çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§é©¾é©¶ä»»åŠ¡<a href="#ref-13" class="ref-link">[13]</a>ã€‚

ç°æœ‰çš„å¤§å¤šæ•°ç»¼è¿°ä¸»è¦å…³æ³¨ADçš„æ•´ä½“æŠ€æœ¯ï¼Œæˆ–è€…ä¸“æ³¨äºç‰¹å®šåŠŸèƒ½å¦‚å®šä½ã€æ„ŸçŸ¥ï¼ˆå°¤å…¶æ˜¯ç›®æ ‡æ£€æµ‹ï¼‰ã€é€šä¿¡ç­‰ï¼Œè€Œå¾ˆå°‘æœ‰ç ”ç©¶æ¶‰åŠMoP<a href="#ref-14" class="ref-link">[14]</a>ã€‚ç”šè‡³å‡ ä¹æ²¡æœ‰ç»¼è¿°æ€»ç»“åŸºäºå¼ºåŒ–å­¦ä¹ çš„MoPç ”ç©¶ã€‚ä¸€äº›å‚è€ƒæ–‡çŒ®å¦‚<a href="#ref-3" class="ref-link">[3]</a>ã€<a href="#ref-15" class="ref-link">[15]</a>å¯¹åŸºäºRLçš„MoPåœ¨ADä¸­çš„æŸäº›ç ”ç©¶å’Œåº”ç”¨è¿›è¡Œäº†å›é¡¾ã€‚<div id="badge-6" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 6)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('6', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-7"><div class="row text-row"><div class="col-src">Nevertheless, most of them focus on the perspective of categorizing RL methodologies, and do not clearly define the connection between RL and the specific driving tasks. Some surveys such as [16], [17] have tried to categorize and discuss RL-based MoP research according to driving scenarios, and provide insight into some state-of-theart RL research from a problem-driven perspective. However, their summarization is incomplete, ignoring some rare driving tasks, such as parking and racing.

Most importantly, they do not provide a detailed introduction to the scenario characteristics and task requirements corresponding to AD tasks, as well as their impact on RL model design. Moreover, the limitations and challenges identified by most surveys, such as driving safety, policy robustness, sample efficiency, and scenario generalization, have been further explored in recent years.</div><div class="col-trans" id="trans-7">ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•è®ºè¿›è¡Œåˆ†ç±»çš„è§’åº¦ä¸Šï¼Œå¹¶æ²¡æœ‰æ˜ç¡®å®šä¹‰RLä¸å…·ä½“é©¾é©¶ä»»åŠ¡ä¹‹é—´çš„è”ç³»ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ç»¼è¿°å¦‚<a href="#ref-16" class="ref-link">[16]</a>ã€<a href="#ref-17" class="ref-link">[17]</a>è¯•å›¾æ ¹æ®é©¾é©¶åœºæ™¯æ¥åˆ†ç±»å’Œè®¨è®ºåŸºäºRLçš„æ–¹æ³•ï¼Œå¹¶ä»é—®é¢˜é©±åŠ¨çš„è§†è§’æä¾›äº†éƒ¨åˆ†å‰æ²¿RLç ”ç©¶çš„è§è§£ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€»ç»“å¹¶ä¸å®Œæ•´ï¼Œå¿½ç•¥äº†æŸäº›ç¨€æœ‰çš„é©¾é©¶ä»»åŠ¡ï¼Œæ¯”å¦‚æ³Šè½¦å’Œç«é€Ÿã€‚

æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬æ²¡æœ‰è¯¦ç»†ä»‹ç»ä¸è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä»»åŠ¡ç›¸å¯¹åº”çš„æƒ…æ™¯ç‰¹å¾åŠä»»åŠ¡è¦æ±‚ï¼Œä»¥åŠè¿™äº›å› ç´ å¯¹RLæ¨¡å‹è®¾è®¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç»¼è¿°ä¸­æŒ‡å‡ºçš„ä¸€äº›é™åˆ¶å’ŒæŒ‘æˆ˜ï¼Œå¦‚é©¾é©¶å®‰å…¨æ€§ã€ç­–ç•¥é²æ£’æ€§ã€æ ·æœ¬æ•ˆç‡å’Œåœºæ™¯æ³›åŒ–ï¼Œåœ¨è¿‘å¹´æ¥å¾—åˆ°äº†è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚<div id="badge-7" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 7)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('7', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-8"><div class="row text-row"><div class="col-src">Despite the existence of several summaries of advanced theoretical approaches to RL [18], [19], [20], to the best of our knowledge, there is no review that comprehensively summarizes the application of these state-of-the-art technologies to the field of MoP for AD. With the rapid development of RL-based AD technologies in both academia and industry, holistic and thorough review of recent investigations is needed.

This article analyzes and summarizes recent advanced work from a comprehensive driving task perspective (although owing to space limitations, we are unable to include some impressive RL-based MoP papers in this article). Our study aims to systematically answer the following questions: How can RL be employed to formulate an MoP model for specific AD tasks? What are the generic design paradigms and customized adaptations of RL for various driving tasks?</div><div class="col-trans" id="trans-8">å°½ç®¡å­˜åœ¨å…³äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é«˜çº§ç†è®ºæ–¹æ³•çš„è‹¥å¹²æ€»ç»“<a href="#ref-18" class="ref-link">[18]</a>ã€<a href="#ref-19" class="ref-link">[19]</a>ã€<a href="#ref-20" class="ref-link">[20]</a>ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæ— å…¨é¢ç»¼è¿°è¿™äº›å‰æ²¿æŠ€æœ¯åœ¨ADé¢†åŸŸMoPä¸­çš„åº”ç”¨ã€‚éšç€åŸºäºRLçš„ADæŠ€æœ¯åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¿«é€Ÿå‘å±•ï¼Œå¯¹è¿‘æœŸç ”ç©¶è¿›è¡Œå…¨é¢è€Œæ·±å…¥çš„å›é¡¾æ˜¯å¿…è¦çš„ã€‚

æœ¬æ–‡ä»ç»¼åˆé©¾é©¶ä»»åŠ¡çš„è§’åº¦åˆ†æå¹¶æ€»ç»“äº†æœ€è¿‘çš„ä¸€äº›å…ˆè¿›å·¥ä½œï¼ˆå°½ç®¡ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•åœ¨æ­¤æ–‡ä¸­åŒ…å«ä¸€äº›ä»¤äººå°è±¡æ·±åˆ»çš„åŸºäºRLçš„MoPè®ºæ–‡ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿåœ°å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šå¦‚ä½•åˆ©ç”¨RLæ¥æ„å»ºç‰¹å®šADä»»åŠ¡çš„MoPæ¨¡å‹ï¼Ÿå¯¹äºå„ç§é©¾é©¶ä»»åŠ¡ï¼ŒRLçš„ä¸€èˆ¬è®¾è®¡èŒƒå¼åŠå…¶å®šåˆ¶åŒ–é€‚åº”æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ<div id="badge-8" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 8)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('8', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-9"><div class="row text-row"><div class="col-src">What are the advances addressing the current challenges for RL-based MoP? The contributions of this article include the following:

â€¢ We outline the fundamentals of RL methodologies, and then focus on their applications in MoP for AD, where various driving tasks are systematically characterized to shed light on their influence on RL design.

â€¢ We summarize several developments in RL-based MoP for AD, extract insights from various driving task applications, and provide guidance for future implementations.

â€¢ The current challenges in RL applications to MoP for AD are discussed, and beyond pointing out challenges and future directions, a comprehensive review of recent exploratory efforts to address these issues with advanced methods is undertaken. The structure of this article is shown in Fig. 2, and the remainder of it is organized as follows: Section II briefly introduces the basics of RL and RL-based MoP. Section III reviews research on RL-based MoP from a driving task perspective.</div><div class="col-trans" id="trans-9">é’ˆå¯¹åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç§»åŠ¨ç‰©ä½“è§„åˆ’ï¼ˆMoPï¼‰æ‰€é¢ä¸´çš„å½“å‰æŒ‘æˆ˜ï¼Œæœ‰å“ªäº›è¿›å±•ï¼Ÿæœ¬æ–‡çš„ç ”ç©¶è´¡çŒ®åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š

â€¢ æˆ‘ä»¬æ¦‚è¿°äº†RLæ–¹æ³•çš„åŸºæœ¬åŸç†ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä¸­çš„åº”ç”¨ï¼Œåœ¨å„ç§é©¾é©¶ä»»åŠ¡ä¸­ç³»ç»Ÿåœ°è¿›è¡Œåˆ†ç±»ä»¥æ­ç¤ºå®ƒä»¬å¯¹RLè®¾è®¡çš„å½±å“ã€‚

â€¢ æ€»ç»“äº†å‡ ç§åŸºäºRLçš„MoPåœ¨ADä¸­çš„å‘å±•ï¼Œä»ä¸åŒçš„é©¾é©¶ä»»åŠ¡åº”ç”¨ä¸­æç‚¼å‡ºè§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„å®ç°æä¾›æŒ‡å¯¼ã€‚

â€¢ è®¨è®ºäº†å°†RLåº”ç”¨äºADä¸­çš„MoPæ‰€é¢ä¸´çš„å½“å‰æŒ‘æˆ˜ï¼Œå¹¶ä¸ä»…æŒ‡å‡ºè¿™äº›æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ï¼Œè¿˜å…¨é¢å›é¡¾äº†æœ€è¿‘é‡‡ç”¨å…ˆè¿›æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜çš„åŠªåŠ›ã€‚æœ¬æ–‡ç»“æ„å¦‚<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">å›¾2</a>æ‰€ç¤ºï¼Œå…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬äºŒèŠ‚ç®€è¦ä»‹ç»äº†RLåŠå…¶åŸºäºRLçš„MoPçš„åŸºæœ¬æ¦‚å¿µï¼›ç¬¬ä¸‰èŠ‚ä»é©¾é©¶ä»»åŠ¡çš„è§’åº¦å›é¡¾äº†åŸºäºRLçš„MoPçš„ç ”ç©¶ã€‚<div id="badge-9" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 9)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('9', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-10"><div class="row text-row"><div class="col-src">Section IV discusses the lessons learned from RL-based MoP design for various driving tasks, and offers experiences and insights. Section V analyzes the current challenges in RL-based MoP and details exploratory efforts to apply advanced RL theories to address them, exploring outlooks and opportunities. Section VI concludes this article.</div><div class="col-trans" id="trans-10">ç¬¬å››èŠ‚è®¨è®ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿åŠ¨è§„åˆ’ï¼ˆMoPï¼‰è®¾è®¡åœ¨å„ç§é©¾é©¶ä»»åŠ¡ä¸­çš„ç»éªŒæ•™è®­ï¼Œå¹¶æä¾›äº†ç›¸å…³ç»éªŒå’Œè§è§£ã€‚ç¬¬äº”èŠ‚åˆ†æäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿åŠ¨è§„åˆ’å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶è¯¦ç»†æ¢è®¨äº†å¦‚ä½•åº”ç”¨å…ˆè¿›çš„RLç†è®ºæ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ¢ç´¢äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘å’Œæœºé‡ã€‚ç¬¬å…­èŠ‚æ€»ç»“äº†æœ¬æ–‡çš„ä¸»è¦å†…å®¹ã€‚<div id="badge-10" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 10)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('10', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-11"><div class="row text-row"><div class="col-src">[[HEADER: II. BASICS OF RL AND RL-BASED MOP FOR AD]]</div><div class="col-trans" id="trans-11"><b>II. å¼ºåŒ–å­¦ä¹ åŠå…¶åœ¨ADä¸­çš„å¤šç›®æ ‡ä¼˜åŒ–åŸºç¡€</b><div id="badge-11" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 11)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('11', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-12"><div class="row text-row"><div class="col-src">[[HEADER: A. Basic Theory and Algorithm of Reinforcement Learning]]</div><div class="col-trans" id="trans-12">A. å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬ç†è®ºä¸ç®—æ³•<div id="badge-12" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 12)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('12', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_2</div><img src="./assets/Figure_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Fig. 2. The schematic of the survey structure of RL-based MoP for AD.</div><div class="asset-desc-zh">Fig. 2. åŸºäºRLçš„æ–¹æ³•è®ºæ¡†æ¶å›¾ï¼Œç”¨äºADæ²»ç–—ã€‚</div></div></div></div><div class="row asset-row" id="Equation_1"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_1</div><img src="./assets/Equation_1.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_2"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_2</div><img src="./assets/Equation_2.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Figure_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_3</div><img src="./assets/Figure_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Fig. 3. RL methods with different categorization.</div><div class="asset-desc-zh">Fig. 3. ä¸åŒåˆ†ç±»ä¸‹çš„RLæ–¹æ³•ã€‚</div></div></div></div><div class="row-container" id="task-13"><div class="row text-row"><div class="col-src">Perception, action, and goal are the three key elements of RL: After perceiving information about the environment state, the RL agent can take actions to influence the environment to achieve its goal. In RL, the agent is not concerned with how to act based on expert data, rather, iterates the policy by evaluating action performance through reward signals and improves its policy to achieve its goal.

In general, the RL model can be formulated as a Markov Decision Process (MDP) [21] satisfying the Markov property: The future states depend only on the current state. Specifically, an MDP problem can be defined by a tuple < S, A, R, T , Î³ >:

â€¢ S and A denote the state and action spaces, respectively, i.e. st âˆˆS and at âˆˆA.

â€¢ T : S Ã— A â†’[0, 1], T (st+1, st, at) is the transition function from a current state-action pair (st, at) to a new state st+1 at the next time step with probability

P(st+1 | st, at), which is referred to as the environmental dynamics (system dynamics).</div><div class="col-trans" id="trans-13">æ„ŸçŸ¥ã€åŠ¨ä½œå’Œç›®æ ‡æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šåœ¨æ¥æ”¶åˆ°å…³äºç¯å¢ƒçŠ¶æ€çš„ä¿¡æ¯åï¼ŒRLä»£ç†å¯ä»¥é‡‡å–è¡ŒåŠ¨æ¥å½±å“ç¯å¢ƒä»¥å®ç°å…¶ç›®æ ‡ã€‚åœ¨RLä¸­ï¼Œä»£ç†å¹¶ä¸å…³å¿ƒå¦‚ä½•åŸºäºä¸“å®¶æ•°æ®è¿›è¡Œè¡Œä¸ºï¼Œè€Œæ˜¯é€šè¿‡å¥–åŠ±ä¿¡å·è¯„ä¼°è¡Œä¸ºè¡¨ç°å¹¶è¿­ä»£ç­–ç•¥ï¼Œä»è€Œæ”¹è¿›å…¶ç­–ç•¥ä»¥è¾¾æˆç›®æ ‡ã€‚

ä¸€èˆ¬æ¥è¯´ï¼ŒRLæ¨¡å‹å¯ä»¥å½¢å¼åŒ–ä¸ºæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰<a href="#ref-21" class="ref-link">[21]</a>ï¼šæœªæ¥çš„çŠ¶æ€ä»…ä¾èµ–äºå½“å‰çš„çŠ¶æ€ã€‚å…·ä½“è€Œè¨€ï¼Œä¸€ä¸ªMDPé—®é¢˜å¯ä»¥é€šè¿‡å…ƒç»„ < S, A, R, T , Î³ > æ¥å®šä¹‰ï¼š

- S å’Œ A åˆ†åˆ«è¡¨ç¤ºçŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œå³ st âˆˆS ä¸” at âˆˆAã€‚

- T : S Ã— A â†’<a href="#ref-0" class="ref-link">[0, 1]</a>ï¼ŒT (st+1, st, at) æ˜¯ä»å½“å‰çš„çŠ¶æ€-åŠ¨ä½œå¯¹ (st, at) åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ–°çŠ¶æ€ st+1 çš„è½¬ç§»å‡½æ•°ï¼Œå…¶æ¦‚ç‡ä¸º P(st+1 | st, at)ï¼Œè¿™è¢«ç§°ä¸ºç¯å¢ƒåŠ¨åŠ›å­¦ï¼ˆç³»ç»ŸåŠ¨åŠ›å­¦ï¼‰ã€‚<div id="badge-13" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 13)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('13', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-14"><div class="row text-row"><div class="col-src">â€¢ R : S Ã— A Ã— S â†’R is the reward function used to evaluate the agentâ€™s performance.

â€¢ Î³ âˆˆ[0, 1] denotes the discount factor for the present value of the future reward. To describe not fully observable states, the MDP problem can be extended to a partially observable MDP (POMDP) [22]. For POMDPs, an observation space O, an observation function â„¦(at, st+1, ot+1) : S â†’ O, and the probability P(ot+1|at, st+1) of observing ot+1 after the agent executed at and reached st+1.

The policy Ï€ : (at|st), maps the observed state st to a probability of an action at, which represents the driving maneuver in the AD driving task. The set of all possible policies is expressed by Î . The sequence {s0, a0, s1, a1, Â· Â· Â· , st, at, Â· Â· Â· } generated by the RL agent with the policy Ï€ is called trajectory or rollout.</div><div class="col-trans" id="trans-14">â€¢ R : S Ã— A Ã— S â†’ R æ˜¯ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“è¡¨ç°çš„å¥–åŠ±å‡½æ•°ã€‚

â€¢ Î³ âˆˆ <a href="#ref-0" class="ref-link">[0, 1]</a> è¡¨ç¤ºæœªæ¥å¥–åŠ±ç°å€¼çš„æŠ˜æ‰£å› å­ã€‚ä¸ºäº†æè¿°ä¸å¯å®Œå…¨è§‚æµ‹çš„çŠ¶æ€ï¼ŒMDPé—®é¢˜å¯ä»¥æ‰©å±•ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰[[LINK: Table_2|<a href="#Table_2" class="tab-link" onclick="highlightAsset('Table_2'); return false;">Table2</a>]]ã€‚å¯¹äº POMDPsï¼Œå­˜åœ¨ä¸€ä¸ªè§‚å¯Ÿç©ºé—´ Oã€ä¸€ä¸ªè§‚å¯Ÿå‡½æ•° â„¦(at, st+1, ot+1) : S â†’ O å’Œåœ¨æ‰§è¡Œ at å¹¶åˆ°è¾¾ st+1 åè§‚æµ‹åˆ° ot+1 çš„æ¦‚ç‡ P(ot+1|at, st+1)ã€‚

ç­–ç•¥ Ï€ : (at|st) å°†è§‚æµ‹çŠ¶æ€ st æ˜ å°„åˆ°åŠ¨ä½œ at çš„æ¦‚ç‡ï¼Œè¿™ä»£è¡¨äº†ADé©¾é©¶ä»»åŠ¡ä¸­çš„é©¾é©¶è¡Œä¸ºã€‚æ‰€æœ‰å¯èƒ½çš„ç­–ç•¥é›†åˆè¡¨ç¤ºä¸º Î ã€‚ä½¿ç”¨ç­–ç•¥ Ï€ ç”Ÿæˆçš„åºåˆ— {s0, a0, s1, a1, Â· Â· Â· , st, at, Â· Â· Â· } è¢«ç§°ä¸ºè½¨è¿¹æˆ–å±•å¼€ã€‚<div id="badge-14" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: MDP (POMDP) [22]. å…¶ä¸­[22]æ˜¯å‚è€ƒæ–‡çŒ®åºå·ä¸åº”è¯¥æ”¹åŠ¨å…¶æ ¼å¼ï¼Œæ›´ä¸èƒ½è«åå‡ºç°è¡¨æ ¼å¼•ç”¨ </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 14)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">MDP (POMDP) [22]. å…¶ä¸­[22]æ˜¯å‚è€ƒæ–‡çŒ®åºå·ä¸åº”è¯¥æ”¹åŠ¨å…¶æ ¼å¼ï¼Œæ›´ä¸èƒ½è«åå‡ºç°è¡¨æ ¼å¼•ç”¨</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('14', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-15"><div class="row text-row"><div class="col-src">The solution objective of the MDP is to find the optimal policy Ï€âˆ—resulting in the highest expected discounted return over all possible trajectories, where h is the current timestep and H is the finite horizon (for an infinite horizon H is set to âˆ). Furthermore, the expectation of return following the policy Ï€ from a state s is defined as the value-function:

where Gt means the total return for the current state st. Similarly, the action-value function, i.e., â€œQ-value functionâ€ is defined as:

According to whether the state transition probability T is known, RL methods can be classified into model-based and model-free. Typical model-based RL methods can utilize dynamic programming (DP) [23] to find the optimal policy with known environment dynamics. However, since the state transition function in many engineering applications (e.g.,</div><div class="col-trans" id="trans-15">MDPçš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥Ï€ï¼Œä½¿å¾—åœ¨æ‰€æœ‰å¯èƒ½è½¨è¿¹ä¸Šçš„æœŸæœ›æŠ˜ç°å›æŠ¥æœ€é«˜ã€‚å…¶ä¸­hè¡¨ç¤ºå½“å‰æ—¶é—´æ­¥ï¼ŒHè¡¨ç¤ºæœ‰é™çš„å±•æœ›æœŸï¼ˆå¯¹äºæ— é™å±•æœ›æœŸï¼ŒHè®¾ä¸ºâˆï¼‰ã€‚æ­¤å¤–ï¼Œä»çŠ¶æ€så‡ºå‘éµå¾ªç­–ç•¥Ï€åçš„å›æŠ¥æœŸæœ›å®šä¹‰ä¸ºä»·å€¼å‡½æ•°ï¼š

\[ V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] \]

å…¶ä¸­Gtè¡¨ç¤ºå½“å‰çŠ¶æ€stçš„æ€»å›æŠ¥ã€‚åŒæ ·åœ°ï¼Œâ€œQå€¼å‡½æ•°â€å³åŠ¨ä½œä»·å€¼å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š

\[ Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right] \]

æ ¹æ®çŠ¶æ€è½¬ç§»æ¦‚ç‡Tæ˜¯å¦å·²çŸ¥ï¼ŒRLæ–¹æ³•å¯ä»¥åˆ†ä¸ºåŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œæ— æ¨¡å‹çš„æ–¹æ³•ã€‚å…¸å‹çš„åŸºäºæ¨¡å‹çš„RLæ–¹æ³•å¯ä»¥é€šè¿‡åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰<a href="#ref-23" class="ref-link">[23]</a>æ¥æ‰¾åˆ°å·²çŸ¥ç¯å¢ƒåŠ¨åŠ›å­¦æƒ…å†µä¸‹çš„æœ€ä¼˜ç­–ç•¥ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå·¥ç¨‹åº”ç”¨ä¸­ï¼ˆä¾‹å¦‚ï¼‰ï¼Œç”±äºçŠ¶æ€è½¬ç§»å‡½æ•°æœªçŸ¥ï¼Œé€šå¸¸é‡‡ç”¨æ— æ¨¡å‹çš„æ–¹æ³•ã€‚<div id="badge-15" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 15)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('15', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-16"><div class="row text-row"><div class="col-src">MoP for AD) is often unclear, it is a challenge to model the interaction between the agent and environment, limiting the application of model-based RL methods [24]. In contrast, model-free RL implicitly construct environment dynamics during learning, which are typically solved by Monte Carlo methods [25] and temporal difference (TD) methods [26]. When the states of the environment and the agent are highdimensional or even infinite, it is impractical to store all Qvalues.

One widely used method is to use deep neural networks (DNNs) as a nonlinear Q-value function approximator over high-dimensional state spaces. Subsequently, Ï€Î¸ is denoted as the policy parameterized by the network parameter Î¸, which aims to fit arbitrarily complex policy distribution functions.</div><div class="col-trans" id="trans-16">MoPå¯¹äºADæ¥è¯´å¾€å¾€ä¸å¤Ÿæ¸…æ™°ï¼Œè¿™ä½¿å¾—å»ºæ¨¡æ™ºèƒ½ä½“ä¸ç¯å¢ƒä¹‹é—´çš„äº¤äº’å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä»è€Œé™åˆ¶äº†åŸºäºæ¨¡å‹çš„RLæ–¹æ³•çš„åº”ç”¨<a href="#ref-24" class="ref-link">[24]</a>ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ¨¡å‹å¤–çš„RLæ–¹æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­éšå¼åœ°æ„å»ºç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œé€šå¸¸é€šè¿‡è’™ç‰¹å¡æ´›æ–¹æ³•<a href="#ref-25" class="ref-link">[25]</a>å’Œæ—¶å·®ï¼ˆTDï¼‰æ–¹æ³•<a href="#ref-26" class="ref-link">[26]</a>æ¥è§£å†³ã€‚å½“ç¯å¢ƒçŠ¶æ€å’Œæ™ºèƒ½ä½“çš„çŠ¶æ€æ˜¯é«˜ç»´ç”šè‡³æ— é™ç»´æ—¶ï¼Œåœ¨æ‰€æœ‰çŠ¶æ€ä¸‹å­˜å‚¨Qå€¼æ˜¯ä¸åˆ‡å®é™…çš„ã€‚

ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•æ˜¯åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä½œä¸ºéçº¿æ€§Qå€¼å‡½æ•°é€¼è¿‘å™¨ï¼Œç”¨äºé«˜ç»´çŠ¶æ€ç©ºé—´ã€‚éšåï¼ŒÏ€Î¸è¡¨ç¤ºç”±ç½‘ç»œå‚æ•°Î¸å‚æ•°åŒ–çš„ç­–ç•¥ï¼Œå…¶ç›®æ ‡æ˜¯æ‹Ÿåˆä»»æ„å¤æ‚çš„ç­–ç•¥åˆ†å¸ƒå‡½æ•°ã€‚<div id="badge-16" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 16)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('16', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-17"><div class="row text-row"><div class="col-src">In this article, RL algorithms are categorized by the difference in policy generation, agent configuration, and learning mode, as shown in Fig. 3, and we focus on model-free methods that are more applicable to MoP for AD. 1) Policy generation a) Value-based Methods: These methods explicitly identify an optimal value function and learn the optimal policy from the value function. Q-learning is one of the most classic RL models.

The optimal policy Ï€âˆ—of Q-learning aims to maximize the Q-value and can be defined as:</div><div class="col-trans" id="trans-17">åœ¨æœ¬æ–‡ä¸­ï¼ŒRLç®—æ³•æ ¹æ®ç­–ç•¥ç”Ÿæˆã€ä»£ç†é…ç½®å’Œå­¦ä¹ æ¨¡å¼çš„ä¸åŒè¿›è¡Œåˆ†ç±»ï¼Œå¦‚<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">å›¾3</a>æ‰€ç¤ºï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨æ›´é€‚ç”¨äºADçš„MoPçš„æ–¹æ³•â€”â€”å³æ¨¡å‹è‡ªç”±æ–¹æ³•ã€‚1) ç­–ç•¥ç”Ÿæˆ a) åŸºäºå€¼çš„æ–¹æ³•ï¼šè¿™äº›æ–¹æ³•æ˜ç¡®åœ°è¯†åˆ«å‡ºæœ€ä¼˜çš„ä»·å€¼å‡½æ•°ï¼Œå¹¶ä»ä»·å€¼å‡½æ•°ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚Q-learningæ˜¯æœ€ç»å…¸çš„RLæ¨¡å‹ä¹‹ä¸€ã€‚

Q-learningçš„ç›®æ ‡ç­–ç•¥Ï€âˆ—æ—¨åœ¨æœ€å¤§åŒ–Qå€¼ï¼Œå¯ä»¥å®šä¹‰ä¸ºï¼š<div id="badge-17" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 17)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('17', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-18"><div class="row text-row"><div class="col-src">[[HEADER: 1) Policy generation]]</div><div class="col-trans" id="trans-18"><b>1) æ”¿ç­–ç”Ÿæˆ</b><div id="badge-18" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 18)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('18', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Equation_3"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_3</div><img src="./assets/Equation_3.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_4"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_4</div><img src="./assets/Equation_4.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_5"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_5</div><img src="./assets/Equation_5.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_6"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_6</div><img src="./assets/Equation_6.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_7"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_7</div><img src="./assets/Equation_7.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_8"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_8</div><img src="./assets/Equation_8.png" class="asset-img-raw" loading="lazy"></div></div><div class="row-container" id="task-19"><div class="row text-row"><div class="col-src">The RL agent can update their policies by estimating Qvalue as follows: QÏ€(st, at) â†QÏ€(st, at)

where Î± is the learning rate. With the use of DNNs, Qlearning has evolved into far-reaching algorithms represented by Deep Q-Network (DQN) [27], Double DQN (DDQN) [28], dueling DQN [29], and Dueling Double DQN (D3QN) [30]. In practice, the outstanding aspects of DQN are the experience replay and the design of the target network. The former breaks the correlation between experience samples and improves data utilization efficiency.

The latter introduces a target network with parameters updated periodically during Q-network updates, thereby alleviating instability from rapid fluctuations in the Q-network. The loss function of the Q-network in the DQN can be expressed as:</div><div class="col-trans" id="trans-19">RLæ™ºèƒ½ä½“å¯ä»¥é€šè¿‡ä¼°è®¡Qå€¼æ¥æ›´æ–°å…¶ç­–ç•¥ï¼š\[ Q_{\pi}(s_t, a_t) \leftarrow Q_{\pi}(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_{a'} Q_{\pi}(s_{t+1}, a') - Q_{\pi}(s_t, a_t)] \]

å…¶ä¸­ï¼Œ\(\alpha\) æ˜¯å­¦ä¹ ç‡ã€‚å€ŸåŠ©æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼ŒQå­¦ä¹ å·²ç»å‘å±•æˆä¸ºè¯¸å¦‚æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰<a href="#ref-27" class="ref-link">[27]</a>ã€åŒé‡DQNï¼ˆDDQNï¼‰<a href="#ref-28" class="ref-link">[28]</a>ã€å¯¹åˆ†DQNï¼ˆdueling DQNï¼‰<a href="#ref-29" class="ref-link">[29]</a> å’Œ å¯¹åˆ†åŒé‡DQNï¼ˆD3QNï¼‰<a href="#ref-30" class="ref-link">[30]</a> ç­‰æ·±è¿œçš„ç®—æ³•ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒDQN çš„çªå‡ºä¹‹å¤„åœ¨äºç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œçš„è®¾è®¡ã€‚å‰è€…æ‰“ç ´äº†ç»éªŒæ ·æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæé«˜äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚

åè€…å¼•å…¥äº†ä¸€ä¸ªå‚æ•°å®šæœŸæ›´æ–°çš„ç›®æ ‡ç½‘ç»œï¼Œåœ¨Qç½‘ç»œæ›´æ–°æ—¶ç¼“è§£äº†ç”±äºQç½‘ç»œå¿«é€Ÿæ³¢åŠ¨å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚DQN ä¸­Qç½‘ç»œçš„æŸå¤±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š\[ L = \mathbb{E}_{(s_t, a_t, r_{t+1}, s_{t+1}) \sim D} \left[ (r_{t+1} + \gamma \max_{a'} Q_{\pi}(s_{t+1}, a') - Q_{\pi}(s_t, a_t))^2 \right] \]<div id="badge-19" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 19)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('19', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-20"><div class="row text-row"><div class="col-src">(5) where Qâ€²(Â·; Î¸â€²) is the target network. Furthermore, the DDQN implements action selection and value evaluation with different Q networks, which reduces overestimation bias. The dueling DQN models the value function separately from the advantage function to improve the stability of the strategy. D3QN combines the techniques underlying the three algorithms above to obtain a more advanced value-based approach.

b) Policy-based Methods: Unlike value-based methods that indirectly obtain the policy by the optimal value function, policy-based methods directly iterate the parameters of the differentiable policy function. Such policy-based methods are more suitable for continuous control problems with infinite action sets. Specifically, the objective function for directly optimizing a stochastic policy function Ï€Î¸ is:

Policy gradient methods [25] use gradient descent to estimate the policy parameters that maximize the expected reward:</div><div class="col-trans" id="trans-20">(5) å…¶ä¸­ \( Q'(\cdot; \theta') \) æ˜¯ç›®æ ‡ç½‘ç»œã€‚æ­¤å¤–ï¼ŒDDQN ä½¿ç”¨ä¸åŒçš„ Q ç½‘ç»œè¿›è¡ŒåŠ¨ä½œé€‰æ‹©å’Œä»·å€¼è¯„ä¼°ï¼Œä»è€Œå‡å°‘äº†è¿‡åº¦ä¼°è®¡åå·®ã€‚åˆ†è·¯ DQN å°†ä»·å€¼å‡½æ•°ä¸ä¼˜åŠ¿å‡½æ•°åˆ†å¼€å»ºæ¨¡ï¼Œä»¥æé«˜ç­–ç•¥çš„ç¨³å®šæ€§ã€‚D3QN ç»“åˆäº†ä¸Šè¿°ä¸‰ç§ç®—æ³•çš„æŠ€æœ¯ï¼Œè·å¾—æ›´å…ˆè¿›çš„åŸºäºå€¼çš„æ–¹æ³•ã€‚

b) åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼šä¸é€šè¿‡æœ€ä¼˜ä»·å€¼å‡½æ•°é—´æ¥è·å–ç­–ç•¥çš„ä»·å€¼åŸºæ–¹æ³•ä¸åŒï¼ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ç›´æ¥è¿­ä»£å¯å¾®ç­–ç•¥å‡½æ•°çš„å‚æ•°ã€‚è¿™ç±»åŸºäºç­–ç•¥çš„æ–¹æ³•æ›´é€‚åˆæ— é™åŠ¨ä½œé›†çš„è¿ç»­æ§åˆ¶é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œç›´æ¥ä¼˜åŒ–éšæœºç­–ç•¥å‡½æ•° \( \pi_\theta \) çš„ç›®æ ‡å‡½æ•°ä¸ºï¼š

ç­–ç•¥æ¢¯åº¦æ–¹æ³• <a href="#ref-25" class="ref-link">[25]</a> ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼°è®¡ä½¿æœŸæœ›å¥–åŠ±æœ€å¤§åŒ–çš„ç­–ç•¥å‚æ•°ï¼š<div id="badge-20" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 20)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('20', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-21"><div class="row text-row"><div class="col-src">The value function still needs to be computed via the policy-based approach to update the policy. The REINFORCE algorithm [31] uses the Monte Carlo method to estimate QÏ€(st, at), but the estimation results exhibit large variance. In addition, the advantage function AÏ€(st, at) = QÏ€(st, at)âˆ’ VÏ€(st, at) [32] can be utilized to replace QÏ€(st, at) to emphasize better actions.

Note that policy-based methods can also use a deterministic policy (determining an action based on the state s, i.e. a = ÂµÎ¸(s), which can be more efficient), rather than just a stochastic policy (selecting an action from a probability distribution, a âˆ¼Ï€Î¸(Â·|s)). In this case, the gradient of the objective function can be expressed as:</div><div class="col-trans" id="trans-21">ä»ç„¶éœ€è¦é€šè¿‡åŸºäºç­–ç•¥çš„æ–¹æ³•è®¡ç®—ä»·å€¼å‡½æ•°æ¥æ›´æ–°ç­–ç•¥ã€‚REINFORCEç®—æ³•[[LINK: <a href="#Equation_3" class="eq-link" onclick="highlightAsset('Equation_3'); return false;">Eq. 3</a>|<a href="#Equation_3" class="eq-link" onclick="highlightAsset('Equation_3'); return false;">Eq. 3</a>]]ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¼°è®¡QÏ€(st, at)ï¼Œä½†å…¶ä¼°è®¡ç»“æœè¡¨ç°å‡ºè¾ƒå¤§çš„æ–¹å·®ã€‚æ­¤å¤–ï¼Œå¯ä»¥åˆ©ç”¨ä¼˜åŠ¿å‡½æ•°AÏ€(st, at) = QÏ€(st, at)âˆ’ VÏ€(st, at)[[LINK: <a href="#Equation_4" class="eq-link" onclick="highlightAsset('Equation_4'); return false;">Eq. 4</a>|<a href="#Equation_4" class="eq-link" onclick="highlightAsset('Equation_4'); return false;">Eq. 4</a>]]æ¥æ›¿ä»£QÏ€(st, at)ï¼Œä»¥å¼ºè°ƒæ›´å¥½çš„è¡ŒåŠ¨ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ä¹Ÿå¯ä»¥ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆæ ¹æ®çŠ¶æ€sç¡®å®šä¸€ä¸ªåŠ¨ä½œï¼Œå³a = ÂµÎ¸(s)ï¼Œè¿™å¯èƒ½æ›´æœ‰æ•ˆï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯éšæœºç­–ç•¥ï¼ˆä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œa âˆ¼Ï€Î¸(Â·|s)ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡å‡½æ•°çš„æ¢¯åº¦å¯ä»¥è¡¨ç¤ºä¸ºï¼š<div id="badge-21" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 21)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('21', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-22"><div class="row text-row"><div class="col-src">The deterministic policy focuses only on exploitation during training and not on exploration. Therefore, the Deterministic Policy Gradient algorithm (DPG) [33] utilizes an off-policy approach to optimize the deterministic policy by sampling from the stochastic policy to ensure sufficient exploration.

c) Actor-Critic Methods: Actor-Critic methods are a special type of policy-based method that integrates techniques from value-based methods, where the actor is the policy function Ï€Î¸ generating actions to obtain the maximum return, and the critic is the value function VÏ€Î¸ that estimates the actions. This coupled structure integrates the flexibility of policy optimization and the stability of value estimation.</div><div class="col-trans" id="trans-22">ç¡®å®šæ€§ç­–ç•¥ä»…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…³æ³¨åˆ©ç”¨ï¼Œè€Œä¸è¿›è¡Œæ¢ç´¢ã€‚å› æ­¤ï¼Œç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼ˆDPGï¼‰<a href="#ref-33" class="ref-link">[33]</a> é‡‡ç”¨ç¦»ç­–ç­–ç•¥æ–¹æ³•é€šè¿‡ä»éšæœºç­–ç•¥ä¸­é‡‡æ ·æ¥ä¼˜åŒ–ç¡®å®šæ€§ç­–ç•¥ï¼Œä»è€Œç¡®ä¿è¶³å¤Ÿçš„æ¢ç´¢ã€‚

c) è¡Œä¸º-æ‰¹è¯„æ–¹æ³•ï¼šè¡Œä¸º-æ‰¹è¯„æ–¹æ³•æ˜¯ä¸€ç§ç‰¹æ®Šçš„åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºå€¼çš„æ–¹æ³•çš„æŠ€æœ¯ã€‚å…¶ä¸­ï¼Œè¡Œä¸ºæ˜¯ç”ŸæˆåŠ¨ä½œä»¥è·å¾—æœ€å¤§å›æŠ¥çš„ç­–ç•¥å‡½æ•°Ï€Î¸ï¼Œè€Œæ‰¹è¯„åˆ™æ˜¯ä¼°è®¡åŠ¨ä½œçš„ä»·å€¼å‡½æ•°VÏ€Î¸ã€‚è¿™ç§è€¦åˆç»“æ„å°†ç­–ç•¥ä¼˜åŒ–çš„çµæ´»æ€§ä¸ä»·å€¼ä¼°è®¡çš„ç¨³å®šæ€§ç›¸ç»“åˆã€‚<div id="badge-22" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 22)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('22', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-23"><div class="row text-row"><div class="col-src">The Deep Deterministic Policy Gradient (DDPG) [34], Proximal Policy Optimization (PPO) [35], and Soft Actor-Critic (SAC) [36] algorithms are typical algorithms that utilize the actor-critic framework. In particular, the SAC algorithm maximizes the entropy of the actions while maximizing the expected return, thus encouraging exploration to obtain better performance. This has made it a popular paradigm in recent years [37].

2) Agent Configuration In a single agent configuration, all interactions with the environment occur through a single agent. Specially, hierarchical RL (HRL) leverages hierarchical abstraction techniques [38] to decomposes an agent into multiple components, simplifying complex tasks by breaking them into subtasks learned by subagents. Not all subagents interact with the environment;</div><div class="col-trans" id="trans-23">Deep Deterministic Policy Gradientï¼ˆDDPGï¼‰<a href="#ref-34" class="ref-link">[34]</a>ã€Proximal Policy Optimizationï¼ˆPPOï¼‰<a href="#ref-35" class="ref-link">[35]</a> å’Œ Soft Actor-Criticï¼ˆSACï¼‰<a href="#ref-36" class="ref-link">[36]</a> ç®—æ³•æ˜¯åˆ©ç”¨æ¼”å‘˜-è¯„è®ºå®¶æ¡†æ¶çš„å…¸å‹ç®—æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSAC ç®—æ³•åœ¨æœ€å¤§åŒ–åŠ¨ä½œçš„ç†µçš„åŒæ—¶æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼Œä»è€Œé¼“åŠ±æ¢ç´¢ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ä½¿å…¶æˆä¸ºè¿‘å¹´æ¥æµè¡Œçš„èŒƒå¼ä¹‹ä¸€ <a href="#ref-37" class="ref-link">[37]</a>ã€‚

2) ä»£ç†é…ç½®
åœ¨ä¸€ä¸ªå•ä»£ç†é…ç½®ä¸­ï¼Œæ‰€æœ‰ä¸ç¯å¢ƒçš„äº¤äº’éƒ½æ˜¯é€šè¿‡å•ä¸€ä»£ç†è¿›è¡Œçš„ã€‚ç‰¹åˆ«åœ°ï¼Œå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰åˆ©ç”¨åˆ†å±‚æŠ½è±¡æŠ€æœ¯ <a href="#ref-38" class="ref-link">[38]</a> å°†ä¸€ä¸ªä»£ç†åˆ†è§£ä¸ºå¤šä¸ªç»„ä»¶ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£æˆç”±å­ä»£ç†å­¦ä¹ çš„å­ä»»åŠ¡æ¥ç®€åŒ–è¿™äº›ä»»åŠ¡ã€‚å¹¶éæ‰€æœ‰çš„å­ä»£ç†éƒ½ä¼šä¸ç¯å¢ƒäº’åŠ¨ï¼›<div id="badge-23" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 23)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('23', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-24"><div class="row text-row"><div class="col-src">typically, the actions from high-level subagents are concatenated into the state space of low-level subagents to provide context and guidance [8] while low-level subagents can control the entire agent. HRL is grounded in Semi MDP (SMDP), which includes the option selection policy Ï€O(ot|st) and the option internal policy Ï€o(at|st).

The high-level agent selects an option ot, and then the low-level agent executes the policy Ï€o(at|st) corresponding to ot, continuing until the option is interrupted [39]. Depending on whether the policies of the high-level and low-level agents are trained synchronously, HRL can be categorized into synchronous and asynchronous architectures.</div><div class="col-trans" id="trans-24">é€šå¸¸ï¼Œé«˜å±‚å­ä»£ç†çš„åŠ¨ä½œä¼šè¢«è¿æ¥åˆ°ä½å±‚å­ä»£ç†çš„çŠ¶æ€ç©ºé—´ä¸­ï¼Œä»¥æä¾›ä¸Šä¸‹æ–‡å’ŒæŒ‡å¯¼<a href="#ref-8" class="ref-link">[8]</a>ï¼Œè€Œä½å±‚å­ä»£ç†åˆ™å¯ä»¥æ§åˆ¶æ•´ä¸ªä»£ç†ã€‚å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰åŸºäºåŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆSMDPï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬é€‰é¡¹é€‰æ‹©ç­–ç•¥Ï€O(ot|st) å’Œ é€‰é¡¹å†…éƒ¨ç­–ç•¥Ï€o(at|st)ã€‚

é«˜å±‚ä»£ç†é€‰æ‹©ä¸€ä¸ªé€‰é¡¹otï¼Œç„¶åä½å±‚ä»£ç†æ‰§è¡Œä¸otå¯¹åº”çš„ç­–ç•¥Ï€o(at|st)ï¼Œç›´åˆ°è¯¥é€‰é¡¹è¢«ä¸­æ–­<a href="#ref-39" class="ref-link">[39]</a>ã€‚æ ¹æ®é«˜å±‚å’Œä½å±‚ä»£ç†çš„ç­–ç•¥æ˜¯å¦åŒæ­¥è®­ç»ƒï¼ŒHRLå¯ä»¥åˆ†ä¸ºåŒæ­¥æ¶æ„å’Œå¼‚æ­¥æ¶æ„ã€‚<div id="badge-24" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 24)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('24', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-25"><div class="row text-row"><div class="col-src">Methods with synchronous architectures are usually composed of a high-level policy providing coarsegrained subgoals, and a low-level policy to achieve finegrained control [40]. Asynchronous HRL pre-trains multiple low-level policies for different tasks and trains the high-level policy to invoke them appropriately [41]. Multi-Agent RL (MARL) enables multiple agents to independently interact with the shared environment.

Each agent has its own task, but its observations and rewards are influenced by the joint actions of all agents. Meanwhile, a single agentâ€™s long-term optimization objective also impacts the policy learning of other agents.</div><div class="col-trans" id="trans-25">å…·æœ‰åŒæ­¥æ¶æ„çš„æ–¹æ³•é€šå¸¸ç”±ä¸€ä¸ªé«˜å±‚ç­–ç•¥æä¾›ç²—ç²’åº¦å­ç›®æ ‡ï¼Œä»¥åŠä¸€ä¸ªä½å±‚ç­–ç•¥ä»¥å®ç°ç»†ç²’åº¦æ§åˆ¶<a href="#ref-40" class="ref-link">[40]</a>ã€‚å¼‚æ­¥å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰é¢„å…ˆè®­ç»ƒå¤šä¸ªé€‚ç”¨äºä¸åŒä»»åŠ¡çš„ä½å±‚ç­–ç•¥ï¼Œå¹¶è®­ç»ƒé«˜å±‚ç­–ç•¥é€‚å½“åœ°è°ƒç”¨å®ƒä»¬<a href="#ref-41" class="ref-link">[41]</a>ã€‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä½¿å¤šä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿç‹¬ç«‹åœ°ä¸å…±äº«ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚

æ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰è‡ªå·±çš„ä»»åŠ¡ï¼Œä½†å…¶è§‚æµ‹å’Œå¥–åŠ±ä¼šå—åˆ°æ‰€æœ‰æ™ºèƒ½ä½“è”åˆåŠ¨ä½œçš„å½±å“ã€‚åŒæ—¶ï¼Œå•ä¸ªæ™ºèƒ½ä½“çš„é•¿æœŸä¼˜åŒ–ç›®æ ‡ä¹Ÿä¼šå½±å“å…¶ä»–æ™ºèƒ½ä½“çš„ç­–ç•¥å­¦ä¹ ã€‚<div id="badge-25" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 25)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('25', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-26"><div class="row text-row"><div class="col-src">Given the differences in observations among agents, the interaction process between agents and the environment is typically described by Markov Game (MG) [42], which is defined by an extension tuple < S, N, A(i)i=1âˆ¼N, R(i)i=1âˆ¼N, T , Î³, â„¦, O(i)i=1âˆ¼N >, where A(i)i=1âˆ¼N is the action sets for N agents, R(i)i=1âˆ¼N is the reward set and T : SÃ—A(1)Ã—Â· Â· Â·Ã—A(i)Ã—Â· Â· Â·Ã—A(N) â†’[0, 1] is the transition function. Each agent receives a local observation O(i) by â„¦(S, i).

Relationships between agents can be categorized as coop-</div><div class="col-trans" id="trans-26">ç”±äºå„ä»£ç†ä¹‹é—´çš„è§‚æµ‹å·®å¼‚ä»¥åŠä»£ç†ä¸ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ï¼Œé€šå¸¸é€šè¿‡é©¬å°”å¯å¤«æ¸¸æˆï¼ˆMarkov Game, MGï¼‰<a href="#ref-42" class="ref-link">[42]</a>æ¥æè¿°ã€‚è¯¥è¿‡ç¨‹ç”±æ‰©å±•å…ƒç»„ < S, N, A(i)i=1âˆ¼N, R(i)i=1âˆ¼N, T , Î³, â„¦, O(i)i=1âˆ¼N > å®šä¹‰ï¼Œå…¶ä¸­ A(i)i=1âˆ¼N æ˜¯ N ä¸ªä»£ç†çš„åŠ¨ä½œé›†ï¼ŒR(i)i=1âˆ¼N æ˜¯å¥–åŠ±é›†ï¼ŒT : SÃ—A(1)Ã—Â· Â· Â·Ã—A(i)Ã—Â· Â· Â·Ã—A(N) â†’<a href="#ref-0" class="ref-link">[0, 1]</a> æ˜¯è½¬ç§»å‡½æ•°ã€‚æ¯ä¸ªä»£ç†é€šè¿‡ â„¦(S, i) æ¥æ”¶å±€éƒ¨è§‚æµ‹ O(i)ã€‚

ä»£ç†ä¹‹é—´çš„å…³ç³»å¯ä»¥å½’ç±»ä¸ºåˆä½œã€<div id="badge-26" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 26)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('26', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-27"><div class="row text-row"><div class="col-src">[[HEADER: p p 2) Agent Configurationi]]</div><div class="col-trans" id="trans-27"><b>p p 2) Agent Configuration</b><br><div id="badge-27" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 27)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('27', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-28"><div class="row text-row"><div class="col-src">erative, competitive, or mixed [43]. Additionally, the training process and action execution in MARL systems can generally be classified into two paradigms: centralized and decentralized. However, centralized execution requires real-time communication and a shared policy among all agents, which is difficult to implement in real-world systems [44].

Therefore, researchers often use two main architectures: i) Centralized Training Decentralized Execution (CTDE): During training, a central critic controls the global perspective and updates the policies of all agents based on their states and actions. ii) Decentralized Training Decentralized Execution (DTDE): Each agent is trained and operated independently, without the need to access global information.</div><div class="col-trans" id="trans-28">æ­¤å¤–ï¼Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç³»ç»Ÿä¸­çš„è®­ç»ƒè¿‡ç¨‹å’ŒåŠ¨ä½œæ‰§è¡Œé€šå¸¸å¯ä»¥å½’ç±»ä¸ºä¸¤ç§èŒƒå¼ï¼šé›†ä¸­å¼å’Œå»ä¸­å¿ƒåŒ–ã€‚ç„¶è€Œï¼Œé›†ä¸­å¼æ‰§è¡Œéœ€è¦å®æ—¶é€šä¿¡å¹¶åœ¨æ‰€æœ‰ä»£ç†ä¹‹é—´å…±äº«ç­–ç•¥ï¼Œåœ¨å®é™…ç³»ç»Ÿä¸­å¾ˆéš¾å®ç°<a href="#ref-44" class="ref-link">[44]</a>ã€‚

å› æ­¤ï¼Œç ”ç©¶äººå‘˜é€šå¸¸ä½¿ç”¨ä¸¤ç§ä¸»è¦æ¶æ„ï¼ši) é›†ä¸­å¼è®­ç»ƒå»ä¸­å¿ƒåŒ–æ‰§è¡Œï¼ˆCTDEï¼‰ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸­å¤®æ‰¹è¯„å®¶æ§åˆ¶å…¨å±€è§†è§’ï¼Œå¹¶åŸºäºå„æ™ºèƒ½ä½“çš„çŠ¶æ€å’ŒåŠ¨ä½œæ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚ii) å»ä¸­å¿ƒåŒ–è®­ç»ƒå»ä¸­å¿ƒåŒ–æ‰§è¡Œï¼ˆDTDEï¼‰ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è¿›è¡Œè®­ç»ƒå’Œæ“ä½œï¼Œæ— éœ€è®¿é—®å…¨å±€ä¿¡æ¯ã€‚<div id="badge-28" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 28)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('28', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-29"><div class="row text-row"><div class="col-src">However, as the number of agents increases, the state space grows exponentially, making it challenging and slow to train a MARL system [45]. 3) Learning Mode Online RL allows the agent to freely interact with the environment and thus collect experience. The RL agent is required to collect sample data (trial-and-error experience) by itself in the training environment and relies on these data to update the policy. This allows the RL agent to discover an unknown optimal policy.

However, online RL usually suffers from sample inefficiency in some tasks and places high demands on the fidelity of the training environment. Offline RL is a framework dedicated to policy optimization from static, previously collected datasets, and it capitalizes on historical interaction data to derive optimal policy. In contrast to online RL, Offline RL relies solely on a pre-established dataset D, thereby eliminating the need for ongoing exploration while mitigating associated risks.</div><div class="col-trans" id="trans-29">ç„¶è€Œï¼Œéšç€ä»£ç†æ•°é‡çš„å¢åŠ ï¼ŒçŠ¶æ€ç©ºé—´å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä½¿å¾—è®­ç»ƒMARLç³»ç»Ÿå˜å¾—æ—¢å…·æœ‰æŒ‘æˆ˜æ€§åˆè€—æ—¶<a href="#ref-45" class="ref-link">[45]</a>ã€‚3ï¼‰åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOnline RLï¼‰å…è®¸æ™ºèƒ½ä½“è‡ªç”±åœ°ä¸ç¯å¢ƒäº¤äº’ï¼Œä»è€Œæ”¶é›†ç»éªŒæ•°æ®ã€‚åœ¨è®­ç»ƒç¯å¢ƒä¸­ï¼ŒRLæ™ºèƒ½ä½“éœ€è¦é€šè¿‡è‡ªèº«æ”¶é›†æ ·æœ¬æ•°æ®ï¼ˆè¯•é”™ç»éªŒï¼‰ï¼Œå¹¶ä¾èµ–è¿™äº›æ•°æ®æ¥æ›´æ–°ç­–ç•¥ã€‚è¿™ä½¿å¾—RLæ™ºèƒ½ä½“èƒ½å¤Ÿå‘ç°æœªçŸ¥çš„æœ€ä¼˜ç­–ç•¥ã€‚

ç„¶è€Œï¼Œåœ¨çº¿RLé€šå¸¸åœ¨æŸäº›ä»»åŠ¡ä¸­ä¼šé­å—æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¹¶ä¸”å¯¹è®­ç»ƒç¯å¢ƒçš„çœŸå®æ€§æå‡ºäº†è¾ƒé«˜çš„è¦æ±‚ã€‚ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§ä¸“é—¨ä»é™æ€ã€é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†ä¸­è¿›è¡Œç­–ç•¥ä¼˜åŒ–çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å†å²äº¤äº’æ•°æ®æ¥æ¨å¯¼å‡ºæœ€ä¼˜ç­–ç•¥ã€‚ä¸åœ¨çº¿RLä¸åŒï¼Œç¦»çº¿RLä»…ä¾èµ–äºä¸€ä¸ªé¢„å…ˆå»ºç«‹çš„æ•°æ®åº“Dï¼Œä»è€Œæ¶ˆé™¤äº†æŒç»­æ¢ç´¢çš„éœ€æ±‚å¹¶å‡è½»äº†ç›¸å…³é£é™©ã€‚<div id="badge-29" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 29)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('29', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-30"><div class="row text-row"><div class="col-src">The core objective of offline RL is to minimize the Bellman error:</div><div class="col-trans" id="trans-30">ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç›®æ ‡æ˜¯æœ€å°åŒ–è´å°”æ›¼è¯¯å·®ï¼š<div id="badge-30" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 30)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('30', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-31"><div class="row text-row"><div class="col-src">[[HEADER: g g 3) Learning Mode]]</div><div class="col-trans" id="trans-31"><b>g g å­¦ä¹ æ¨¡å¼</b><div id="badge-31" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 31)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('31', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Equation_9"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_9</div><img src="./assets/Equation_9.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Figure_4"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_4</div><img src="./assets/Figure_4.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Fig. 4. RL algorithm applied to MoP for AD.</div><div class="asset-desc-zh">Fig. 4. åº”ç”¨äºADæ²»ç–—çš„RLç®—æ³•ã€‚</div></div></div></div><div class="row-container" id="task-32"><div class="row text-row"><div class="col-src">âˆ’QÏ€Î¸(st, at))2] (9) Achieving accurate error estimation requires alignment between the evaluation policy and the target policy. However, offline RL inherently aims to discover policies that outperform the original policy, which introduces an unavoidable distributional shift.

This shift occurs when the state-action distribution under the learned policy diverges from that under the original policy, leading to inaccuracies in value estimation due to cumulative biases from sampling and function approximation. To address this distributional shift, Offline RL methods are broadly divided into model-based and model-free methods. Model-based methods leverage learned dynamics models to estimate uncertainty and handle distributional discrepancies.</div><div class="col-trans" id="trans-32">\[-Q_{\pi_\theta}(s_t, a_t))^2] \tag{9}\) å‡†ç¡®çš„è¯¯å·®ä¼°è®¡éœ€è¦è¯„ä¼°ç­–ç•¥ä¸ç›®æ ‡ç­–ç•¥ä¹‹é—´å­˜åœ¨å¯¹é½ã€‚ç„¶è€Œï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æœ¬è´¨ä¸Šæ—¨åœ¨å‘ç°è¶…è¶ŠåŸå§‹ç­–ç•¥çš„æ–°ç­–ç•¥ï¼Œè¿™ä¸å¯é¿å…åœ°å¼•å…¥äº†åˆ†å¸ƒæ€§çš„å˜åŒ–ã€‚

è¿™ç§å˜åŒ–å‘ç”Ÿåœ¨æ‰€å­¦ç­–ç•¥ä¸‹çš„çŠ¶æ€-åŠ¨ä½œåˆ†å¸ƒä¸åŸå§‹ç­–ç•¥ä¸‹çš„çŠ¶æ€-åŠ¨ä½œåˆ†å¸ƒç›¸å¼‚æ—¶ï¼Œå¯¼è‡´ç”±äºé‡‡æ ·å’Œå‡½æ•°è¿‘ä¼¼çš„ç´¯ç§¯åå·®è€Œå¼•èµ·çš„ä»·å€¼ä¼°è®¡ä¸å‡†ç¡®ã€‚ä¸ºåº”å¯¹è¿™ç§åˆ†å¸ƒæ€§å˜åŒ–ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¤§è‡´å¯åˆ†ä¸ºåŸºäºæ¨¡å‹çš„æ–¹æ³•å’Œæ— æ¨¡å‹çš„æ–¹æ³•ã€‚åŸºäºæ¨¡å‹çš„æ–¹æ³•åˆ©ç”¨å­¦åˆ°çš„åŠ¨åŠ›å­¦æ¨¡å‹æ¥ä¼°ç®—ä¸ç¡®å®šæ€§å¹¶å¤„ç†åˆ†å¸ƒå·®å¼‚ã€‚<div id="badge-32" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 32)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('32', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-33"><div class="row text-row"><div class="col-src">Prominent examples include MORel [46], MOPO [47], COMBO [48], etc. Model-free methods are further split into explicit and implicit regularization techniques. Explicit regularization methods, such as (Batch-Constrained Q-learning) BCQ [49], (Bootstrapping Error Accumulation Reduction) BEAR [50], Conservative Q-Learning (CQL) [51], etc., impose direct constraints on policy improvement to limit distributional divergence and encourage conservative policy update.

Additionally, the inability to interact with the environment to find more rewarding regions further restricts the performance of offline RL.</div><div class="col-trans" id="trans-33">å…¸å‹çš„ä¾‹å­åŒ…æ‹¬MORel <a href="#ref-46" class="ref-link">[46]</a>ã€MOPO <a href="#ref-47" class="ref-link">[47]</a>å’ŒCOMBO <a href="#ref-48" class="ref-link">[48]</a>ç­‰ã€‚æ— æ¨¡å‹æ–¹æ³•è¿›ä¸€æ­¥åˆ†ä¸ºæ˜¾å¼æ­£åˆ™åŒ–æŠ€æœ¯å’Œéšå¼æ­£åˆ™åŒ–æŠ€æœ¯ã€‚æ˜¾å¼æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚ï¼ˆæ‰¹çº¦æŸQå­¦ä¹ ï¼‰BCQ <a href="#ref-49" class="ref-link">[49]</a>ã€ï¼ˆç´¯ç§¯è¯¯å·®å‡å°çš„è‡ªåŠ©æ³•ï¼‰BEAR <a href="#ref-50" class="ref-link">[50]</a>ã€ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰<a href="#ref-51" class="ref-link">[51]</a>ç­‰ï¼Œç›´æ¥å¯¹ç­–ç•¥æ”¹è¿›æ–½åŠ é™åˆ¶ä»¥é™åˆ¶åˆ†å¸ƒå·®å¼‚ï¼Œå¹¶é¼“åŠ±ä¿å®ˆçš„ç­–ç•¥æ›´æ–°ã€‚

æ­¤å¤–ï¼Œæ— æ³•ä¸ç¯å¢ƒäº’åŠ¨ä»¥æ‰¾åˆ°æ›´å…·å¥–åŠ±æ€§çš„åŒºåŸŸè¿›ä¸€æ­¥é™åˆ¶äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„è¡¨ç°ã€‚<div id="badge-33" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 33)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('33', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-34"><div class="row text-row"><div class="col-src">[[HEADER: B. RL-based Motion Planning for Autonomous Driving]]</div><div class="col-trans" id="trans-34">åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»é©¾é©¶è¿åŠ¨è§„åˆ’  
åœ¨è‡ªä¸»é©¾é©¶ä¸­ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¢«ç”¨äºå®ç°æ™ºèƒ½çš„è¿åŠ¨è§„åˆ’ã€‚è¿™äº›æ–¹æ³•é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜çš„åŠ¨ä½œç­–ç•¥ï¼Œä»è€Œä¼˜åŒ–è½¦è¾†çš„è¡Œé©¶è·¯å¾„å’Œè¡Œä¸ºã€‚æœ¬èŠ‚å°†ä»‹ç»å‡ ç§å…¸å‹çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿åŠ¨è§„åˆ’ç®—æ³•ï¼Œå¹¶è®¨è®ºå®ƒä»¬åœ¨è‡ªä¸»é©¾é©¶ä¸­çš„åº”ç”¨ã€‚<div id="badge-34" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 34)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('34', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_5"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_5</div><img src="./assets/Figure_5.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Fig. 5. Illustration of RL-based MoP for different driving tasks.</div><div class="asset-desc-zh">Fig. 5. åŸºäºRLçš„æ–¹æ³•è®ºæ¡†æ¶å›¾ï¼Œé€‚ç”¨äºä¸åŒé©¾é©¶ä»»åŠ¡ã€‚</div></div></div></div><div class="row-container" id="task-35"><div class="row text-row"><div class="col-src">MoP for AD generally refers to the planning process for generating feasible states and control sequences, and it is aimed at achieving safe and efficient movement. It generally requires a given route, or specified task to consider the evolution of the agent and environment dynamics [52]. A schematic of application of RL to MoP for AD is shown in Fig. 4, where the RL agent learns a driving policy from trialand-error data.

The ego vehicle (EV) states and environmental observations usually constitute the state space of the RL agent, and the action output by the RL agent is used for high-level behavioral-type decisions and for direct control of the vehicle maneuvering at a low-level.</div><div class="col-trans" id="trans-35">MoP å¯¹äºADé€šå¸¸æŒ‡çš„æ˜¯ç”Ÿæˆå¯è¡ŒçŠ¶æ€å’Œæ§åˆ¶åºåˆ—çš„è§„åˆ’è¿‡ç¨‹ï¼Œå…¶ç›®æ ‡æ˜¯å®ç°å®‰å…¨é«˜æ•ˆçš„ç§»åŠ¨ã€‚å®ƒä¸€èˆ¬éœ€è¦ç»™å®šä¸€æ¡è·¯çº¿æˆ–ç‰¹å®šä»»åŠ¡æ¥è€ƒè™‘ä»£ç†å’Œç¯å¢ƒåŠ¨åŠ›å­¦çš„å˜åŒ– <a href="#ref-52" class="ref-link">[52]</a>ã€‚<a href="#Figure_4" class="fig-link" onclick="highlightAsset('Figure_4'); return false;">å›¾4</a>å±•ç¤ºäº†å°†RLåº”ç”¨äºADçš„MoPçš„ä¸€ç§æ–¹æ¡ˆï¼Œåœ¨æ­¤RLä»£ç†é€šè¿‡è¯•é”™æ•°æ®å­¦ä¹ é©¾é©¶ç­–ç•¥ã€‚

egoè½¦è¾†ï¼ˆEVï¼‰çš„çŠ¶æ€å’Œç¯å¢ƒè§‚å¯Ÿé€šå¸¸æ„æˆäº†RLä»£ç†çš„çŠ¶æ€ç©ºé—´ï¼Œè€ŒRLä»£ç†è¾“å‡ºçš„åŠ¨ä½œç”¨äºé«˜å±‚è¡Œä¸ºç±»å‹çš„å†³ç­–ï¼Œå¹¶ç›´æ¥æ§åˆ¶è½¦è¾†åœ¨ä½å±‚çš„æ“ä½œã€‚<div id="badge-35" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 35)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('35', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-36"><div class="row text-row"><div class="col-src">For instance, value-based methods are widely used for behavioral planning in MoP [12], [53], [54].The discrete action output of the value-Based RL fits well to supervisory control solutions where the higher level commands by the RL planner are implemented by the legacy motion control systems [55]. Meanwhile, policy-based methods can output the continuous control commands such as the steering angle and acceleration [56], [57], [58].

In recent years, the superior performance of the actor-critic methods has led to the direct learning of vehicle control commands becoming the mainstream direction in the current research [13], [59], [60]. Furthermore, it transpired that HRL motion planning has a similar algorithm architecture to the rule-based modular approach. Different sub-agent can be created to learn the policy for decision-making, trajectory planning, motion control tasks separately.</div><div class="col-trans" id="trans-36">ä¾‹å¦‚ï¼ŒåŸºäºä»·å€¼çš„æ–¹æ³•å¹¿æ³›ç”¨äºMoPä¸­çš„è¡Œä¸ºè§„åˆ’<a href="#ref-12" class="ref-link">[12]</a>ã€<a href="#ref-53" class="ref-link">[53]</a>ã€<a href="#ref-54" class="ref-link">[54]</a>ã€‚åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¦»æ•£åŠ¨ä½œè¾“å‡ºéå¸¸é€‚åˆç›‘ç£æ§åˆ¶è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¿™ç§æ–¹æ¡ˆä¸­ï¼Œç”±RLè®¡åˆ’å™¨å‘å‡ºçš„é«˜å±‚å‘½ä»¤ç”±é—ç•™çš„åŠ¨åŠ›å­¦æ§åˆ¶ç³»ç»Ÿå®ç°<a href="#ref-55" class="ref-link">[55]</a>ã€‚åŒæ—¶ï¼ŒåŸºäºç­–ç•¥çš„æ–¹æ³•å¯ä»¥è¾“å‡ºè¿ç»­æ§åˆ¶æŒ‡ä»¤ï¼Œå¦‚è½¬å‘è§’å’ŒåŠ é€Ÿåº¦<a href="#ref-56" class="ref-link">[56]</a>ã€<a href="#ref-57" class="ref-link">[57]</a>ã€<a href="#ref-58" class="ref-link">[58]</a>ã€‚

è¿‘å¹´æ¥ï¼Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ä½¿å¾—ç›´æ¥å­¦ä¹ è½¦è¾†æ§åˆ¶æŒ‡ä»¤æˆä¸ºå½“å‰ç ”ç©¶çš„ä¸»è¦æ–¹å‘<a href="#ref-13" class="ref-link">[13]</a>ã€<a href="#ref-59" class="ref-link">[59]</a>ã€<a href="#ref-60" class="ref-link">[60]</a>ã€‚æ­¤å¤–ï¼Œå‘ç°å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰è¿åŠ¨è§„åˆ’ä¸åŸºäºè§„åˆ™çš„æ¨¡å—åŒ–æ–¹æ³•å…·æœ‰ç±»ä¼¼çš„ç®—æ³•æ¶æ„ã€‚å¯ä»¥åˆ›å»ºä¸åŒçš„å­ä»£ç†åˆ†åˆ«å­¦ä¹ å†³ç­–ç­–ç•¥ã€è½¨è¿¹è§„åˆ’å’ŒåŠ¨åŠ›å­¦æ§åˆ¶ä»»åŠ¡çš„æ”¿ç­–ã€‚<div id="badge-36" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 36)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('36', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-37"><div class="row text-row"><div class="col-src">Some works [61], [62], [63], [64] train the high-level policy to select discrete semantic decision actions, and then utilize a separate low-level policy to directly control the steering angle and acceleration, achieving more precise and flexible motion control while ensuring clear driving objectives. Several studies have used MARL to better model the interaction between vehicles and provide a global perspective on multi-vehicle control.

CTDE methods are commonly used to generate multi-vehicle policies when collaborative tasks are involved, such as maintaining formation, and cooperative lane changing or merging [65], [66]. The core advantage of RL is its theoretical framework, which is focuses on optimizing decisions for long-term returns rather than merely imitating observed behavior.

This capability enables RL to potentially outperform human drivers by uncovering innovative driving policies that extend beyond traditional rule-based models.</div><div class="col-trans" id="trans-37">ä¸€äº›ç ”ç©¶<a href="#ref-61" class="ref-link">[61]</a>, <a href="#ref-62" class="ref-link">[62]</a>, <a href="#ref-63" class="ref-link">[63]</a>, <a href="#ref-64" class="ref-link">[64]</a>è®­ç»ƒé«˜å±‚ç­–ç•¥é€‰æ‹©ç¦»æ•£çš„è¯­ä¹‰å†³ç­–åŠ¨ä½œï¼Œç„¶ååˆ©ç”¨å•ç‹¬çš„ä½å±‚ç­–ç•¥ç›´æ¥æ§åˆ¶è½¬å‘è§’å’ŒåŠ é€Ÿåº¦ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®å’Œçµæ´»çš„è¿åŠ¨æ§åˆ¶ï¼ŒåŒæ—¶ç¡®ä¿æ˜ç¡®çš„é©¾é©¶ç›®æ ‡ã€‚å¤šé¡¹ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ï¼Œå¯ä»¥æ›´å¥½åœ°å»ºæ¨¡è½¦è¾†ä¹‹é—´çš„äº¤äº’ï¼Œå¹¶ä»å…¨å±€è§†è§’æä¾›å¤šè½¦è¾†æ§åˆ¶ã€‚

CTDEæ–¹æ³•å¸¸ç”¨äºç”Ÿæˆåä½œä»»åŠ¡ä¸‹çš„å¤šè½¦è¾†ç­–ç•¥ï¼Œä¾‹å¦‚ä¿æŒé˜Ÿå½¢ã€ååŒå˜é“æˆ–åˆå¹¶<a href="#ref-65" class="ref-link">[65]</a>, <a href="#ref-66" class="ref-link">[66]</a>ã€‚RLçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶ç†è®ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“æ³¨äºä¼˜åŒ–é•¿æœŸå›æŠ¥çš„å†³ç­–ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¨¡ä»¿è§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚

è¿™ç§èƒ½åŠ›ä½¿RLæœ‰å¯èƒ½è¶…è¶Šäººç±»é©¾é©¶å‘˜ï¼Œé€šè¿‡å‘ç°è¶…å‡ºä¼ ç»ŸåŸºäºè§„åˆ™æ¨¡å‹çš„æ–°é¢–é©¾é©¶ç­–ç•¥æ¥è¡¨ç°å‡ºè‰²ã€‚<div id="badge-37" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 37)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('37', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-38"><div class="row text-row"><div class="col-src">[[HEADER: III. A REVIEW FROM THE DRIVING TASK PERSPECTIVE]]</div><div class="col-trans" id="trans-38">III. ä»é©¾é©¶ä»»åŠ¡çš„è§’åº¦è¿›è¡Œå›é¡¾<div id="badge-38" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 38)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('38', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-39"><div class="row text-row"><div class="col-src">Most RL MoP studies in the AD field have focused on specific driving tasks, ranging from single tasks such as lane keeping or car following, to multi-task integrated urban navigation, etc. Different driving tasks and their application scenarios usually have their own unique characteristics, which have an enormous impact on the design of RL models. From the driving tasks perspective, this section describes the scenario characteristics and task requirements of different driving tasks.

On this basis, we review the RL-based MoP literature under these tasks (as illustrated in Fig. 5), especially how they design an RL model for AD.</div><div class="col-trans" id="trans-39">å¤§å¤šæ•°ADé¢†åŸŸçš„RL MoPç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šçš„é©¾é©¶ä»»åŠ¡ä¸Šï¼Œè¿™äº›ä»»åŠ¡èŒƒå›´ä»å•ä¸€çš„ä»»åŠ¡ï¼ˆå¦‚è½¦é“ä¿æŒæˆ–è·Ÿéšå‰è½¦ï¼‰åˆ°å¤šä»»åŠ¡é›†æˆçš„åŸå¸‚å¯¼èˆªç­‰ã€‚ä¸åŒçš„é©¾é©¶ä»»åŠ¡åŠå…¶åº”ç”¨åœºæ™¯é€šå¸¸å…·æœ‰å„è‡ªç‹¬ç‰¹çš„ç‰¹å¾ï¼Œè¿™å¯¹RLæ¨¡å‹çš„è®¾è®¡äº§ç”Ÿäº†å·¨å¤§å½±å“ã€‚ä»é©¾é©¶ä»»åŠ¡çš„è§’åº¦æ¥çœ‹ï¼Œæœ¬èŠ‚æè¿°äº†ä¸åŒé©¾é©¶ä»»åŠ¡çš„åœºæ™¯ç‰¹æ€§å’Œä»»åŠ¡è¦æ±‚ã€‚

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å›é¡¾äº†è¿™äº›ä»»åŠ¡ä¸‹çš„åŸºäºRLçš„MoPæ–‡çŒ®ï¼ˆå¦‚<a href="#Figure_5" class="fig-link" onclick="highlightAsset('Figure_5'); return false;">å›¾5</a>æ‰€ç¤ºï¼‰ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¦‚ä½•è®¾è®¡ADä¸­çš„RLæ¨¡å‹ã€‚<div id="badge-39" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 39)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('39', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-40"><div class="row text-row"><div class="col-src">[[HEADER: A. Car Following/Lane Keeping]]</div><div class="col-trans" id="trans-40">A. è½¦è¾†è·Ÿéš/è½¦é“ä¿æŒ<div id="badge-40" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 40)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('40', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-41"><div class="row text-row"><div class="col-src">Car following (CF) and lane keeping (LK) are the two simple autonomous driving tasks for which early applications of RL approach have been explored. The former task aims to adjust the longitudinal speed to maintain a suitable speed between the EV and the front vehicle (FV), whereas the latter focuses on lateral distance control.

Zhu et al. [67] use the speed of EV, the speed difference from the FV, and the headway distance as the observed states, and then directly control acceleration using DDPG algorithm. Meanwhile, Time to Collision (TTC), Time Headway, and jerk are prioritized in the design of the reward function. Furthermore, Shi et al. [65] add the rear vehicle (RV) information into the state space, and correspondingly considers the safety reward and efficiency reward related to the RV.</div><div class="col-trans" id="trans-41">è·Ÿéšè½¦è¾†ï¼ˆCFï¼‰å’Œè½¦é“ä¿æŒï¼ˆLKï¼‰æ˜¯æ—©æœŸç ”ç©¶ä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•çš„ä¸¤ä¸ªç®€å•è‡ªä¸»é©¾é©¶ä»»åŠ¡ã€‚å‰è€…æ—¨åœ¨è°ƒæ•´çºµå‘é€Ÿåº¦ä»¥åœ¨ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰ä¸å‰æ–¹è½¦è¾†ï¼ˆFVï¼‰ä¹‹é—´ç»´æŒåˆé€‚çš„è¡Œé©¶é€Ÿåº¦ï¼Œè€Œåè€…åˆ™ä¸“æ³¨äºæ¨ªå‘è·ç¦»æ§åˆ¶ã€‚

æœ±ç­‰<a href="#ref-67" class="ref-link">[67]</a>å°†ç”µåŠ¨æ±½è½¦çš„é€Ÿåº¦ã€ä¸å‰æ–¹è½¦è¾†çš„é€Ÿåº¦å·®ä»¥åŠè½¦å¤´é—´è·ä½œä¸ºè§‚æµ‹çŠ¶æ€ï¼Œå¹¶ç›´æ¥ä½¿ç”¨DDPGç®—æ³•æ§åˆ¶åŠ é€Ÿåº¦ã€‚åŒæ—¶ï¼Œåœ¨å¥–åŠ±å‡½æ•°çš„è®¾è®¡ä¸­ä¼˜å…ˆè€ƒè™‘ç¢°æ’æ—¶é—´ï¼ˆTTCï¼‰ã€æ—¶è·ï¼ˆTime Headwayï¼‰å’ŒåŠ é€Ÿåº¦å˜åŒ–ç‡ï¼ˆjerkï¼‰ã€‚æ­¤å¤–ï¼ŒçŸ³ç­‰<a href="#ref-65" class="ref-link">[65]</a>å°†åæ–¹è½¦è¾†çš„ä¿¡æ¯çº³å…¥çŠ¶æ€ç©ºé—´ï¼Œå¹¶ç›¸åº”åœ°è€ƒè™‘ä¸åæ–¹è½¦è¾†ç›¸å…³çš„å®‰å…¨å¥–åŠ±å’Œæ•ˆç‡å¥–åŠ±ã€‚<div id="badge-41" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 41)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('41', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-42"><div class="row text-row"><div class="col-src">Chen et al. [54] further consider the cut-in maneuvers of vehicles from adjacent lanes. Specifically, a target acceleration it selected via DDQN by discretizing a continuous acceleration interval. For the LK task, Kendall et al. [68] control the steering angle and target speed through the DDPG algorithm, with the state space containing the vehicleâ€™s speed and steering angle, as well as monocular camera images from the environment.

Notably, they conducted real-world experiments on a 250 meter section of road, using a modified Renault Twizy vehicle to learn the driving policy online. Moreover, Peng et al. [53] exploit D3QN to control quantized steering angle and acceleration values, promoting the EV follows the road centerline.

Given the target path points, Tian et al. [69] add the lateral distance from the target path to the preview points in the observation space, and use two actor networks to control steering angle and vehicle speed, improving motion accuracy.</div><div class="col-trans" id="trans-42">Chenç­‰<a href="#ref-54" class="ref-link">[54]</a>è¿›ä¸€æ­¥è€ƒè™‘äº†ç›¸é‚»è½¦é“è½¦è¾†åˆ‡å…¥çš„æœºåŠ¨è¡Œä¸ºã€‚å…·ä½“è€Œè¨€ï¼Œé€šè¿‡DDQNç®—æ³•å°†è¿ç»­åŠ é€Ÿåº¦åŒºé—´ç¦»æ•£åŒ–åé€‰æ‹©ç›®æ ‡åŠ é€Ÿåº¦ã€‚å¯¹äºLKä»»åŠ¡ï¼ŒKendallç­‰<a href="#ref-68" class="ref-link">[68]</a>åˆ©ç”¨DDPGç®—æ³•æ§åˆ¶è½¬å‘è§’å’Œç›®æ ‡é€Ÿåº¦ï¼ŒçŠ¶æ€ç©ºé—´åŒ…å«è½¦è¾†çš„é€Ÿåº¦ã€è½¬å‘è§’ä»¥åŠç¯å¢ƒä¸­çš„å•ç›®æ‘„åƒå¤´å›¾åƒã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»–ä»¬åœ¨250ç±³é•¿çš„è·¯æ®µä¸Šè¿›è¡Œäº†å®åœ°å®éªŒï¼Œå¹¶ä½¿ç”¨æ”¹è£…åçš„é›·è¯ºTwizyè½¦è¾†åœ¨çº¿å­¦ä¹ é©¾é©¶ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒPengç­‰<a href="#ref-53" class="ref-link">[53]</a>åˆ©ç”¨D3QNæ§åˆ¶é‡åŒ–åçš„è½¬å‘è§’å’ŒåŠ é€Ÿåº¦å€¼ï¼Œä½¿ç”µåŠ¨æ±½è½¦æ²¿é“è·¯ä¸­å¿ƒçº¿è¡Œé©¶ã€‚

ç»™å®šç›®æ ‡è·¯å¾„ç‚¹ï¼ŒTianç­‰<a href="#ref-69" class="ref-link">[69]</a>åœ¨è§‚æµ‹ç©ºé—´ä¸­åŠ å…¥äº†ç›®æ ‡è·¯å¾„çš„æ¨ªå‘è·ç¦»ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ªactorç½‘ç»œåˆ†åˆ«æ§åˆ¶è½¬å‘è§’å’Œè½¦è¾†é€Ÿåº¦ï¼Œä»è€Œæé«˜è¿åŠ¨ç²¾åº¦ã€‚<div id="badge-42" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 42)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('42', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-43"><div class="row text-row"><div class="col-src">[[HEADER: B. Lane Change/Overtaking]]</div><div class="col-trans" id="trans-43"><b>B. å˜é“/è¶…è½¦</b><div id="badge-43" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 43)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('43', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-44"><div class="row text-row"><div class="col-src">ane change (LC) is a common driving maneuver, and it causes large collision accidents [70]. The purpose of LC is to avoid collision and improve driving efficiency. On structured roads, lane changes are often accompanied by overtaking behavior, i.e. a continuous LC to obtain a faster driving speed. Some researchers divide overtaking maneuvers into three phases: moving to the target passing lane, overtaking another vehicle and then moving back to the original lane [71].

Many studies address such tasks through high-level behavior planning with physical feature inputs, such as continuous feature states of both the EV and surrounding vehicles (SVs) (e.g. surrounding six [72], or eight [12] vehicles, and vehicles within a certain range). Reference [13] use a discrete state grid of the surrounding environment as the input.</div><div class="col-trans" id="trans-44">ä¸€æ¬¡å˜é“ï¼ˆLCï¼‰æ˜¯ä¸€ç§å¸¸è§çš„é©¾é©¶æ“ä½œï¼Œä¼šå¯¼è‡´ä¸¥é‡çš„ç¢°æ’äº‹æ•…[[LINK: 70|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]ã€‚LCçš„ç›®çš„åœ¨äºé¿å…ç¢°æ’å¹¶æé«˜é©¾é©¶æ•ˆç‡ã€‚åœ¨ç»“æ„åŒ–é“è·¯ä¸Šï¼Œå˜é“é€šå¸¸ä¼´éšç€è¶…è½¦è¡Œä¸ºï¼Œå³è¿ç»­è¿›è¡Œå˜é“ä»¥è·å¾—æ›´é«˜çš„è¡Œé©¶é€Ÿåº¦ã€‚ä¸€äº›ç ”ç©¶è€…å°†è¶…è½¦åŠ¨ä½œåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šç§»åŠ¨åˆ°ç›®æ ‡è¶…è½¦é“ã€è¶…è¶Šå…¶ä»–è½¦è¾†ä»¥åŠç„¶åè¿”å›åŸè½¦é“[[LINK: 71|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]ã€‚

è®¸å¤šç ”ç©¶é€šè¿‡ä½¿ç”¨ç‰©ç†ç‰¹å¾è¾“å…¥çš„é«˜çº§è¡Œä¸ºè§„åˆ’æ¥è§£å†³æ­¤ç±»ä»»åŠ¡ï¼Œä¾‹å¦‚ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰å’Œå‘¨å›´è½¦è¾†ï¼ˆSVsï¼‰çš„è¿ç»­ç‰¹å¾çŠ¶æ€ï¼ˆå¦‚å‘¨å›´çš„å…­è¾†[[LINK: 72|<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">Fig.3</a>]]æˆ–å…«è¾†[[LINK: 12|<a href="#Figure_4" class="fig-link" onclick="highlightAsset('Figure_4'); return false;">Fig.4</a>]]è½¦è¾†ï¼Œä»¥åŠä¸€å®šèŒƒå›´å†…çš„è½¦è¾†ï¼‰ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-13" class="ref-link">[13]</a>ä½¿ç”¨å‘¨å›´ç¯å¢ƒçš„ç¦»æ•£çŠ¶æ€ç½‘æ ¼ä½œä¸ºè¾“å…¥ã€‚<div id="badge-44" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 44)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('44', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-45"><div class="row text-row"><div class="col-src">Specifically, references [55], [62], [73], [74] use DQN and its improved algorithms to output three semantic actionsâ€”lane change to the left (LCL), lane change to the right (LCR), and lane keeping (LK)â€”focusing exclusively on lateral behaviors. Among them, reference [74] realizes the first application of RL lanechanging policy in the real world. Based on this, reference [72] further incorporates ac/deceleration in the action space, but do not fundamentally change the output form.

These actions with low control granularity still limit the impact of the RL agent on vehicleâ€™s maneuverability. More recently, an increasing number of studies have used the DDPG, PPO, SAC techniques, etc., to directly control the steering angle and acceleration [13], [60], [75]. Meanwhile, sensor data from LiDAR [76], camera [77], etc., are utilized as observation inputs to achieve direct mapping between perception and control commands.</div><div class="col-trans" id="trans-45">å…·ä½“è€Œè¨€ï¼Œå‚è€ƒæ–‡çŒ®<a href="#ref-55" class="ref-link">[55]</a>ã€<a href="#ref-62" class="ref-link">[62]</a>ã€<a href="#ref-73" class="ref-link">[73]</a>å’Œ<a href="#ref-74" class="ref-link">[74]</a>ä½¿ç”¨äº†DQNåŠå…¶æ”¹è¿›ç®—æ³•æ¥è¾“å‡ºä¸‰ç§è¯­ä¹‰åŠ¨ä½œâ€”â€”å·¦å˜é“ï¼ˆLCLï¼‰ã€å³å˜é“ï¼ˆLCRï¼‰å’Œä¿æŒè½¦é“è¡Œé©¶ï¼ˆLKï¼‰ï¼Œä¸“æ³¨äºæ¨ªå‘è¡Œä¸ºã€‚å…¶ä¸­ï¼Œå‚è€ƒæ–‡çŒ®<a href="#ref-74" class="ref-link">[74]</a>å®ç°äº†é¦–ä¸ªå°†RLå˜é“ç­–ç•¥åº”ç”¨äºå®é™…ä¸–ç•Œçš„å®ä¾‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå‚è€ƒæ–‡çŒ®<a href="#ref-72" class="ref-link">[72]</a>è¿›ä¸€æ­¥åœ¨åŠ¨ä½œç©ºé—´ä¸­åŠ å…¥äº†åŠ /å‡é€Ÿæ“ä½œï¼Œä½†å¹¶æœªä»æ ¹æœ¬ä¸Šæ”¹å˜è¾“å‡ºå½¢å¼ã€‚

è¿™äº›æ§åˆ¶ç²’åº¦è¾ƒä½çš„åŠ¨ä½œä»ç„¶é™åˆ¶äº†RLä»£ç†å¯¹è½¦è¾†æœºåŠ¨æ€§çš„å½±å“ã€‚è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å¼€å§‹ç›´æ¥ä½¿ç”¨DDPGã€PPOã€SACç­‰æŠ€æœ¯æ¥æ§åˆ¶è½¬å‘è§’å’ŒåŠ é€Ÿåº¦<a href="#ref-13" class="ref-link">[13]</a>ã€<a href="#ref-60" class="ref-link">[60]</a>ã€<a href="#ref-75" class="ref-link">[75]</a>ã€‚åŒæ—¶ï¼ŒLiDAR <a href="#ref-76" class="ref-link">[76]</a>ã€æ‘„åƒå¤´ <a href="#ref-77" class="ref-link">[77]</a> ç­‰ä¼ æ„Ÿå™¨çš„æ•°æ®è¢«ç”¨ä½œè§‚æµ‹è¾“å…¥ï¼Œä»¥å®ç°æ„ŸçŸ¥ä¸æ§åˆ¶å‘½ä»¤ä¹‹é—´çš„ç›´æ¥æ˜ å°„ã€‚<div id="badge-45" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 45)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('45', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-46"><div class="row text-row"><div class="col-src">However, even small differences in adjacent inputs may cause significant fluctuations in the control commands output from the policy network. Several studies proposed approaches to generating trajectory targets to indirectly control vehicles, aiming to balance flexibility and stability of lane change/overtaking behavior. For example, Yu et al. [71] select a trajectory from a given discrete trajectory set, which is then sent to a tracker module.

Lu et al. [78] allow the agent to output a target point location as well as a desired vehicle speed, and then optimize the motion sequence for lane change or overtaking. Despite differences in state and action space, most studies are consistent in reward design because of driving task characteristics. The safety reward is essential and is crucial and is typically associated with collision [64], relative distance [55], TTC [67], etc.</div><div class="col-trans" id="trans-46">ç„¶è€Œï¼Œå³ä½¿ç›¸é‚»è¾“å…¥ä¹‹é—´å­˜åœ¨å¾®å°å·®å¼‚ä¹Ÿå¯èƒ½å¯¼è‡´ç­–ç•¥ç½‘ç»œè¾“å‡ºçš„æ§åˆ¶å‘½ä»¤å‘ç”Ÿæ˜¾è‘—æ³¢åŠ¨ã€‚å¤šé¡¹ç ”ç©¶æå‡ºäº†ç”Ÿæˆè½¨è¿¹ç›®æ ‡çš„æ–¹æ³•ï¼Œä»¥é—´æ¥æ§åˆ¶è½¦è¾†ï¼Œæ—¨åœ¨å¹³è¡¡å˜é“/è¶…è½¦è¡Œä¸ºçš„çµæ´»æ€§å’Œç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼ŒYuç­‰äºº[[LINK: 71|ç­‰]]ä»ç»™å®šçš„ç¦»æ•£è½¨è¿¹é›†ä¸­é€‰æ‹©ä¸€æ¡è½¨è¿¹ï¼Œç„¶åå°†å…¶å‘é€åˆ°è·Ÿè¸ªæ¨¡å—ã€‚

Luç­‰äºº[[LINK: 78|ç­‰]]å…è®¸ä»£ç†è¾“å‡ºç›®æ ‡ç‚¹ä½ç½®ä»¥åŠæœŸæœ›çš„è½¦è¾†é€Ÿåº¦ï¼Œå¹¶éšåä¼˜åŒ–å˜é“æˆ–è¶…è½¦çš„è¿åŠ¨åºåˆ—ã€‚å°½ç®¡çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´å­˜åœ¨å·®å¼‚ï¼Œä½†ç”±äºé©¾é©¶ä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œå¤§å¤šæ•°ç ”ç©¶åœ¨å¥–åŠ±è®¾è®¡ä¸Šä¿æŒä¸€è‡´ã€‚å®‰å…¨å¥–åŠ±è‡³å…³é‡è¦ï¼Œé€šå¸¸ä¸ç¢°æ’[[LINK: 64|ç­‰]]ã€ç›¸å¯¹è·ç¦»[[LINK: 55|ç­‰]]ã€TTCï¼ˆæ—¶é—´ç›´åˆ°ç¢°æ’ï¼‰[[LINK: 67|ç­‰]]ç›¸å…³è”ã€‚<div id="badge-46" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 46)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('46', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-47"><div class="row text-row"><div class="col-src">Efficiency is also important and the efficiency reward is often dependent on the vehicle speed [73], the degree of task completion [79], etc. Other components of the reward function can represent comfort related to acceleration and jerk [80], as well as adherence to traffic rules [68], e.g., overtaking on the left side. Furthermore, some studies design segmented rewards according to the overtaking phase to represent the goals of different phases [71].</div><div class="col-trans" id="trans-47">æ•ˆç‡åŒæ ·é‡è¦ï¼Œå¥–åŠ±å¾€å¾€å–å†³äºè½¦è¾†é€Ÿåº¦[[LINK: 73|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]ã€ä»»åŠ¡å®Œæˆåº¦[[LINK: 79|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]ç­‰ã€‚å¥–åŠ±å‡½æ•°çš„å…¶ä»–ç»„æˆéƒ¨åˆ†å¯ä»¥è¡¨ç¤ºä¸åŠ é€Ÿåº¦å’Œå†²å‡»ç›¸å…³çš„èˆ’é€‚æ€§[[LINK: 80|<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">Fig.3</a>]]ï¼Œä»¥åŠéµå®ˆäº¤é€šè§„åˆ™çš„ç¨‹åº¦[[LINK: 68|<a href="#Figure_4" class="fig-link" onclick="highlightAsset('Figure_4'); return false;">Fig.4</a>]]ï¼Œä¾‹å¦‚åœ¨å·¦ä¾§è¶…è½¦ã€‚æ­¤å¤–ï¼Œä¸€äº›ç ”ç©¶æ ¹æ®è¶…è½¦é˜¶æ®µè®¾è®¡äº†åˆ†æ®µå¥–åŠ±ï¼Œä»¥ä»£è¡¨ä¸åŒé˜¶æ®µçš„ç›®æ ‡[[LINK: 71|<a href="#Figure_5" class="fig-link" onclick="highlightAsset('Figure_5'); return false;">Fig.5</a>]]ã€‚<div id="badge-47" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 47)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('47', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-48"><div class="row text-row"><div class="col-src">[[HEADER: C. Ramp Merge/Intersection/Roundabout]]</div><div class="col-trans" id="trans-48"><b>C. æ»‘è¡Œé“åˆæµ/äº¤å‰å£/ç¯å²›</b><div id="badge-48" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 48)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('48', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-49"><div class="row text-row"><div class="col-src">) Ramp Merge: the ramp merge task is typically prompted by the driving lanes of the EV and SVs will overlap in the future, resulting in a forced interaction between EV and SVs. The EV needs to adjust its speed before arriving at the lane merge point to find an acceptable gap between the SVs in the target lane. Therefore, the merge task requires the road geometry be used as an extra observation input compared to the lane changing/overtaking tasks [81], [82].

The simplest way is to learn longitudinal control, which ensures the EV drives to the right place at the appropriate time. Notable approaches include learning the ac/deceleration behavior [83] or the speed control command [84]. Some merge tasks allow the EV to complete the merge operation anywhere within a lane between a start and an end merge point [85].</div><div class="col-trans" id="trans-49">) æŒ¡é“å˜é“ï¼šæŒ¡é“å˜é“ä»»åŠ¡é€šå¸¸ç”±ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰å’Œè·Ÿéšè½¦è¾†ï¼ˆSVsï¼‰çš„è¡Œé©¶è½¦é“å†³å®šï¼Œæœªæ¥ä¸¤è€…çš„è½¦é“å°†ä¼šé‡å ï¼Œä»è€Œå¯¼è‡´EVä¸SVsä¹‹é—´äº§ç”Ÿå¼ºåˆ¶æ€§äº¤äº’ã€‚ä¸ºäº†åœ¨åˆ°è¾¾åˆæµç‚¹æ—¶æ‰¾åˆ°ç›®æ ‡è½¦é“ä¸­SVsä¹‹é—´çš„åˆé€‚ç©ºéš™ï¼ŒEVéœ€è¦è°ƒæ•´å…¶é€Ÿåº¦ã€‚å› æ­¤ï¼Œç›¸è¾ƒäºå˜é“æˆ–è¶…è½¦ä»»åŠ¡ï¼Œåˆæµä»»åŠ¡è¦æ±‚ä½¿ç”¨é“è·¯å‡ ä½•ä½œä¸ºé¢å¤–çš„è§‚æµ‹è¾“å…¥ <a href="#ref-81" class="ref-link">[81, 82]</a>ã€‚

æœ€ç®€å•çš„æ–¹æ³•æ˜¯å­¦ä¹ çºµå‘æ§åˆ¶ï¼Œä»¥ç¡®ä¿ç”µåŠ¨æ±½è½¦åœ¨é€‚å½“çš„æ—¶é—´åˆ°è¾¾æ­£ç¡®çš„ä½ç½®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›æ–¹æ³•åŒ…æ‹¬å­¦ä¹ åŠ å‡é€Ÿè¡Œä¸º <a href="#ref-83" class="ref-link">[83]</a> æˆ–é€Ÿåº¦æ§åˆ¶å‘½ä»¤ <a href="#ref-84" class="ref-link">[84]</a>ã€‚æŸäº›åˆæµä»»åŠ¡å…è®¸ç”µåŠ¨æ±½è½¦åœ¨å…¶èµ·å§‹å’Œç»“æŸåˆæµç‚¹ä¹‹é—´çš„ä»»æ„è½¦é“ä¸Šå®Œæˆåˆæµæ“ä½œ <a href="#ref-85" class="ref-link">[85]</a>ã€‚<div id="badge-49" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 49)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('49', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-50"><div class="row text-row"><div class="col-src">Like lane changing/overtaking, RL solutions for merging can generate steering angle and acceleration commands to directly control the vehicle for more flexible merging maneuvers [59]. The merge task typically introduces an additional reward for reaching the target lane [86], while some studies also consider the driving distance or time to complete the merge [83], [87].

Due to the stronger interactions in merging scenarios, it is also crucial to devote attention to the collaborative behavior of SVs. Several studies employed game theory to model these interaction [88]. Recent research has advanced with MARL framework [89], [90] that provides each agent with strategies for the merging process. This approach learns the interaction characteristics among vehicles and helps them perform actions with a consistent optimal goal.</div><div class="col-trans" id="trans-50">ä¸å˜é“/è¶…è½¦ç±»ä¼¼ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³æ–¹æ¡ˆå¯ä»¥ä¸ºå¹¶çº¿ç”Ÿæˆè½¬å‘è§’å’ŒåŠ é€Ÿåº¦æŒ‡ä»¤ï¼Œä»è€Œæ›´çµæ´»åœ°æ§åˆ¶è½¦è¾† <a href="#ref-59" class="ref-link">[59]</a>ã€‚åˆå¹¶ä»»åŠ¡é€šå¸¸ä¼šå¼•å…¥ä¸€ä¸ªé¢å¤–çš„å¥–åŠ±ä»¥è¾¾åˆ°ç›®æ ‡è½¦é“ <a href="#ref-86" class="ref-link">[86]</a>ï¼Œè€Œä¸€äº›ç ”ç©¶è¿˜è€ƒè™‘äº†å®Œæˆåˆå¹¶çš„è·ç¦»æˆ–æ—¶é—´ <a href="#ref-83" class="ref-link">[83]</a>, <a href="#ref-87" class="ref-link">[87]</a>ã€‚

ç”±äºå¹¶çº¿åœºæ™¯ä¸­çš„äº¤äº’ä½œç”¨æ›´å¼ºï¼Œå› æ­¤è¿˜éœ€è¦å…³æ³¨SVï¼ˆè‡ªä¸»è½¦è¾†ï¼‰ä¹‹é—´çš„ååŒè¡Œä¸ºã€‚å¤šé¡¹ç ”ç©¶åˆ©ç”¨åšå¼ˆè®ºæ¥å»ºæ¨¡è¿™äº›äº¤äº’ <a href="#ref-88" class="ref-link">[88]</a>ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ™é‡‡ç”¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶ <a href="#ref-89" class="ref-link">[89]</a>, <a href="#ref-90" class="ref-link">[90]</a>ï¼Œä¸ºæ¯ä¸ªä»£ç†æä¾›äº†åˆå¹¶è¿‡ç¨‹ä¸­çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å­¦ä¹ è½¦è¾†é—´çš„äº¤äº’ç‰¹æ€§ï¼Œå¹¶å¸®åŠ©å®ƒä»¬ä»¥ä¸€è‡´çš„æœ€ä¼˜ç›®æ ‡æ‰§è¡ŒåŠ¨ä½œã€‚<div id="badge-50" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 50)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('50', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-51"><div class="row text-row"><div class="col-src">2) Intersection: Intersection scenario is similar to a ramp merge but with more lane conflicts (turn vs. straight, unsignalized intersections, etc.), complex road structures and road elements, and diverse driving behaviors. This makes intersection one of the most challenging tasks for AD on structured roads [17].

Early approaches focused on controlling vehicle longitudinal behavior through physical feature inputs, by adjusting vehicle acceleration/deceleration [91], or deciding whether to yield or assert the right of way (e.g., wait, pass, yield, take up, give up, etc.) [63], [92], [93], so as to pass through the intersection successfully.</div><div class="col-trans" id="trans-51">2) äº¤å‰å£ï¼šäº¤å‰å£åœºæ™¯ç±»ä¼¼äºåŒé“æ±‡æµï¼Œä½†æœ‰æ›´å¤šçš„è½¦é“å†²çªï¼ˆå¦‚è½¬å¼¯ä¸ç›´è¡Œå†²çªã€æ— ä¿¡å·æ§åˆ¶çš„äº¤å‰å£ç­‰ï¼‰ï¼Œå¤æ‚çš„é“è·¯ç»“æ„å’Œé“è·¯å…ƒç´ ï¼Œä»¥åŠå¤šæ ·çš„é©¾é©¶è¡Œä¸ºã€‚è¿™ä½¿å¾—äº¤å‰å£æˆä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨ç»“æ„åŒ–é“è·¯ä¸Šé¢ä¸´çš„æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ <a href="#ref-17" class="ref-link">[17]</a>ã€‚

æ—©æœŸçš„æ–¹æ³•ä¸»è¦é€šè¿‡ç‰©ç†ç‰¹å¾è¾“å…¥æ¥æ§åˆ¶è½¦è¾†çºµå‘è¡Œä¸ºï¼Œä¾‹å¦‚è°ƒæ•´åŠ å‡é€Ÿ <a href="#ref-91" class="ref-link">[91]</a>ï¼Œæˆ–è€…å†³å®šæ˜¯å¦è®©è¡Œæˆ–ä¸»å¼ ä¼˜å…ˆæƒï¼ˆå¦‚ç­‰å¾…ã€è¶…è¶Šã€è®©è¡Œã€å æ®ã€æ”¾å¼ƒç­‰ï¼‰<a href="#ref-63" class="ref-link">[63]</a>, <a href="#ref-92" class="ref-link">[92]</a>, <a href="#ref-93" class="ref-link">[93]</a>ï¼Œä»¥ç¡®ä¿èƒ½å¤ŸæˆåŠŸé€šè¿‡äº¤å‰å£ã€‚<div id="badge-51" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 51)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('51', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-52"><div class="row text-row"><div class="col-src">Subsequent research has sought to increase agent flexibility by directly controlling vehicle motion and exploiting the reward function reflecting tracking error, collisions, success rate and passage time [94], [95]. Some studies also incorporate additional rewards for violating traffic rules, such as crossing solid lines or running red lights [96], [97]. Other studies break down various scenarios into sub-tasks for training [98], or employ state machines [99] to manage these multiple tasks.

However, handing semantic constraints for agents in these approaches remains challenging. Due to the increasing number of scenario features and state observation inputs, recent research has attempted extracting feature information directly from raw sensor data. Ren et al. [95] utilize LiDAR point cloud data to extract features</div><div class="col-trans" id="trans-52">åç»­ç ”ç©¶è‡´åŠ›äºé€šè¿‡ç›´æ¥æ§åˆ¶è½¦è¾†è¿åŠ¨å’Œåˆ©ç”¨åæ˜ è·Ÿè¸ªè¯¯å·®ã€ç¢°æ’ã€æˆåŠŸç‡å’Œé€šè¡Œæ—¶é—´çš„å¥–åŠ±å‡½æ•°æ¥å¢åŠ ä»£ç†çš„çµæ´»æ€§<a href="#ref-94" class="ref-link">[94]</a>, <a href="#ref-95" class="ref-link">[95]</a>ã€‚ä¸€äº›ç ”ç©¶è¿˜å¼•å…¥äº†è¿åäº¤é€šè§„åˆ™çš„é¢å¤–å¥–åŠ±ï¼Œä¾‹å¦‚è·¨è¶Šå®çº¿æˆ–é—¯çº¢ç¯<a href="#ref-96" class="ref-link">[96]</a>, <a href="#ref-97" class="ref-link">[97]</a>ã€‚å…¶ä»–ç ”ç©¶å°†å„ç§åœºæ™¯åˆ†è§£ä¸ºå­ä»»åŠ¡è¿›è¡Œè®­ç»ƒ<a href="#ref-98" class="ref-link">[98]</a>ï¼Œæˆ–è€…é‡‡ç”¨çŠ¶æ€æœº<a href="#ref-99" class="ref-link">[99]</a>æ¥ç®¡ç†è¿™äº›å¤šä¸ªä»»åŠ¡ã€‚

ç„¶è€Œï¼Œåœ¨è¿™äº›æ–¹æ³•ä¸­å¤„ç†ä»£ç†çš„è¯­ä¹‰çº¦æŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç”±äºåœºæ™¯ç‰¹å¾å’ŒçŠ¶æ€è§‚æµ‹è¾“å…¥çš„æ•°é‡ä¸æ–­å¢åŠ ï¼Œæœ€è¿‘çš„ç ”ç©¶å°è¯•ç›´æ¥ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æå–ç‰¹å¾ä¿¡æ¯ã€‚Renç­‰äºº<a href="#ref-95" class="ref-link">[95]</a>åˆ©ç”¨æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®æ¥æå–ç‰¹å¾ã€‚<div id="badge-52" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 52)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('52', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-53"><div class="row text-row"><div class="col-src">[[HEADER: E. Urban Navigation]]</div><div class="col-trans" id="trans-53"><b>åŸå¸‚å¯¼èˆª</b><div id="badge-53" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 53)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('53', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-54"><div class="row text-row"><div class="col-src">of traffic participants, including vehicles, bicycles, and pedestrians. In [60], multi-view camera images are projected into the birdâ€™s-eye view (BEV) format to capture global scene features. References [97], [100] address sensor occlusion at intersections with roadside sensing information supplement. Similarly, several studies have explored the use of MARL to address driving through intersections.

Antonio et al. [101] iteratively process the observations of the relative positions, speeds, and driving intentions and then individually control the desired speed of each vehicle. Zhao et al. [102] integrate the positional and speed information of each vehicle into a global state feature, and output a joint action based on each vehicleâ€™s desired speed.</div><div class="col-trans" id="trans-54">äº¤é€šå‚ä¸è€…ï¼ŒåŒ…æ‹¬è½¦è¾†ã€è‡ªè¡Œè½¦å’Œè¡Œäººã€‚åœ¨<a href="#ref-60" class="ref-link">[60]</a>ä¸­ï¼Œå¤šè§†è§’ç›¸æœºå›¾åƒè¢«æŠ•å½±åˆ°é¸Ÿç°å›¾ï¼ˆBEVï¼‰æ ¼å¼ä»¥æ•æ‰å…¨å±€åœºæ™¯ç‰¹å¾ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-97" class="ref-link">[97]</a>ã€<a href="#ref-100" class="ref-link">[100]</a>å¤„ç†äº¤å‰å£å¤„çš„ä¼ æ„Ÿå™¨é®æŒ¡é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è·¯è¾¹æ„ŸçŸ¥ä¿¡æ¯è¿›è¡Œè¡¥å……ã€‚ç±»ä¼¼åœ°ï¼Œå¤šé¡¹ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨MARLæ¥è§£å†³é€šè¿‡äº¤å‰å£çš„é—®é¢˜ã€‚

Antonioç­‰äºº<a href="#ref-101" class="ref-link">[101]</a>è¿­ä»£å¤„ç†ç›¸å¯¹ä½ç½®ã€é€Ÿåº¦å’Œé©¾é©¶æ„å›¾çš„è§‚æµ‹å€¼ï¼Œç„¶ååˆ†åˆ«æ§åˆ¶æ¯è¾†è½¦çš„ç›®æ ‡é€Ÿåº¦ã€‚Zhaoç­‰äºº<a href="#ref-102" class="ref-link">[102]</a>å°†æ¯è¾†è½¦çš„ä½ç½®å’Œé€Ÿåº¦ä¿¡æ¯æ•´åˆåˆ°ä¸€ä¸ªå…¨å±€çŠ¶æ€ç‰¹å¾ä¸­ï¼Œå¹¶åŸºäºæ¯è¾†è½¦çš„ç›®æ ‡é€Ÿåº¦è¾“å‡ºè”åˆåŠ¨ä½œã€‚<div id="badge-54" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 54)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('54', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-55"><div class="row text-row"><div class="col-src">3) Roundabout: the roundabout scenario can be viewed as a combination of two T-junctions and a circular multi-lane road. It involves both merge/intersection and lane change/overtaking tasks. At the entrance, the EV is required to perform a merging task similar to that at an intersection, while driving in the roundabout may involve lane changes and moving to the outside lane before exiting.

This combination of multi-scenario features and multi-tasking creates a significant challenge for the MoP system, especially for environment encoding. Zhang et al. [13] divide the state input into an environmental representation (ER) and a task representation (TR). The ER focuses the physical features of eight SVs, and the TR includes relative lane and exit distances.</div><div class="col-trans" id="trans-55">3ï¼‰ç¯å²›ï¼šç¯å²›åœºæ™¯å¯ä»¥è§†ä¸ºä¸¤ä¸ªTå­—è·¯å£å’Œä¸€ä¸ªåœ†å½¢å¤šè½¦é“é“è·¯çš„ç»„åˆã€‚å®ƒæ¶‰åŠåˆå¹¶/äº¤å‰å£ä»»åŠ¡ä»¥åŠå˜é“/è¶…è½¦ä»»åŠ¡ã€‚åœ¨å…¥å£å¤„ï¼ŒEVéœ€è¦æ‰§è¡Œç±»ä¼¼äºäº¤å‰å£çš„åˆå¹¶ä»»åŠ¡ï¼›è€Œåœ¨ç¯å²›ä¸Šè¡Œé©¶æ—¶ï¼Œåˆ™å¯èƒ½æ¶‰åŠåˆ°å˜é“å¹¶åœ¨å‡ºå£å‰é©¶å…¥å¤–ä¾§è½¦é“ã€‚

è¿™ç§å¤šç§åœºæ™¯ç‰¹æ€§å’Œå¤šé‡ä»»åŠ¡çš„ç»“åˆä¸ºMoPç³»ç»Ÿå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¯å¢ƒç¼–ç æ–¹é¢ã€‚å¼ ç­‰äººçš„ç ”ç©¶<a href="#ref-13" class="ref-link">[13]</a>å°†çŠ¶æ€è¾“å…¥åˆ†ä¸ºç¯å¢ƒè¡¨ç¤ºï¼ˆERï¼‰å’Œä»»åŠ¡è¡¨ç¤ºï¼ˆTRï¼‰ã€‚ç¯å¢ƒè¡¨ç¤ºï¼ˆERï¼‰å…³æ³¨å…«è¾†å‘¨å›´è½¦è¾†çš„ç‰©ç†ç‰¹å¾ï¼Œè€Œä»»åŠ¡è¡¨ç¤ºï¼ˆTRï¼‰åˆ™åŒ…æ‹¬ç›¸å¯¹è½¦é“è·ç¦»å’Œå‡ºå£è·ç¦»ã€‚<div id="badge-55" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 55)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('55', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-56"><div class="row text-row"><div class="col-src">The action space represents macro-scale behavior (change lane or not) and mesoscale behaviors of desired acceleration and action time. The authors of [13] use MPC to generate pre-trained trajectories, which are embedded in the actor-critic network to improve the learning efficiency. Additionally, TR vectors are replicated in the environment encoding process to emphasize task success in the later training stages.</div><div class="col-trans" id="trans-56">åŠ¨ä½œç©ºé—´è¡¨ç¤ºå®è§‚è¡Œä¸ºï¼ˆæ¢é“ä¸å¦ï¼‰å’ŒæœŸæœ›åŠ é€Ÿåº¦åŠè¡ŒåŠ¨æ—¶é—´çš„ä¸­è§‚è¡Œä¸ºã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-13" class="ref-link">[13]</a>çš„ä½œè€…ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶(MPC)ç”Ÿæˆé¢„è®­ç»ƒè½¨è¿¹ï¼Œå¹¶å°†è¿™äº›è½¨è¿¹åµŒå…¥åˆ°actor-criticç½‘ç»œä¸­ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œåœ¨ç¯å¢ƒç¼–ç è¿‡ç¨‹ä¸­å¤åˆ¶TRå‘é‡ï¼Œä»¥åœ¨åç»­è®­ç»ƒé˜¶æ®µå¼ºè°ƒä»»åŠ¡çš„æˆåŠŸå®Œæˆã€‚<div id="badge-56" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 56)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('56', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-57"><div class="row text-row"><div class="col-src">[[HEADER: D. Parking]]</div><div class="col-trans" id="trans-57">D. åœè½¦åœº<div id="badge-57" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 57)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('57', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-58"><div class="row text-row"><div class="col-src">A parking scenario involves an unstructured environment in an urban area with partially regular roads and perpendicular, parallel or diagonal parking slots. Parking tasks have been widely studied, and automated parking technologies have been deployed in many produced vehicles. Current research tends to improve parking flexibility, i.e., reduce â€œD-Râ€ gear shifting in unconventional or narrow parking spaces [103]. RL can be used for finding the optimal parking path.

Most studies directly control the vehicleâ€™s motion during the parking process, and use the position, velocity, and heading angle of the EV as necessary observation inputs, see, e.g., [58], [104]. For the reward design, safety and parking targets are necessary to encourage the EV to reach the target position and heading angle without collision.</div><div class="col-trans" id="trans-58">ä¸€ä¸ªåœè½¦åœºæ™¯æ¶‰åŠåŸå¸‚åŒºåŸŸä¸­éƒ¨åˆ†è§„åˆ™çš„é“è·¯å’Œå‚ç›´ã€å¹³è¡Œæˆ–æ–œå‘çš„åœè½¦ä½ã€‚åœè½¦ä»»åŠ¡å·²ç»è¢«å¹¿æ³›ç ”ç©¶ï¼Œå¹¶ä¸”è‡ªåŠ¨æ³Šè½¦æŠ€æœ¯å·²ç»åº”ç”¨äºè®¸å¤šç”Ÿäº§è½¦è¾†ã€‚å½“å‰çš„ç ”ç©¶å€¾å‘äºæé«˜æ³Šè½¦çµæ´»æ€§ï¼Œå³åœ¨éå¸¸è§„æˆ–ç‹­çª„çš„åœè½¦ä½å‡å°‘â€œD-Râ€æ¢æŒ¡æ¬¡æ•°<a href="#ref-103" class="ref-link">[103]</a>ã€‚å¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºå¯»æ‰¾æœ€ä¼˜çš„æ³Šè½¦è·¯å¾„ã€‚

å¤§å¤šæ•°ç ”ç©¶ç›´æ¥æ§åˆ¶è½¦è¾†åœ¨æ³Šè½¦è¿‡ç¨‹ä¸­çš„è¿åŠ¨ï¼Œå¹¶ä½¿ç”¨ç”µåŠ¨æ±½è½¦çš„ä½ç½®ã€é€Ÿåº¦å’Œèˆªå‘è§’ä½œä¸ºå¿…è¦çš„è§‚æµ‹è¾“å…¥ï¼Œä¾‹å¦‚å‚è§æ–‡çŒ®<a href="#ref-58" class="ref-link">[58]</a>ã€<a href="#ref-104" class="ref-link">[104]</a>ã€‚å¯¹äºå¥–åŠ±è®¾è®¡è€Œè¨€ï¼Œå®‰å…¨æ€§å’Œæ³Šè½¦ä½ç›®æ ‡æ˜¯å¿…è¦çš„ï¼Œä»¥é¼“åŠ±ç”µåŠ¨æ±½è½¦è¾¾åˆ°ç›®æ ‡ä½ç½®å’Œèˆªå‘è§’è€Œä¸å‘ç”Ÿç¢°æ’ã€‚<div id="badge-58" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 58)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('58', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-59"><div class="row text-row"><div class="col-src">Additionally, parking is encouraged to be completed as quickly as possible to enhance efficiency [58], and smooth ness of control commands is promoted to improve comfort [104]. [57] proposes a unified approach capable of coping with perpendicular, parallel or diagonal parking slots, with proximity sensor data as one of the observation inputs.</div><div class="col-trans" id="trans-59">æ­¤å¤–ï¼Œå»ºè®®å°½å¿«å®Œæˆåœè½¦ä»¥æé«˜æ•ˆç‡<a href="#ref-58" class="ref-link">[58]</a>ï¼Œå¹¶ä¿ƒè¿›æ§åˆ¶å‘½ä»¤çš„å¹³æ»‘æ€§ä»¥æ”¹å–„èˆ’é€‚åº¦<a href="#ref-104" class="ref-link">[104]</a>ã€‚æ–‡çŒ®<a href="#ref-57" class="ref-link">[57]</a>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåº”å¯¹å‚ç›´ã€å¹³è¡Œæˆ–å¯¹è§’çº¿åœè½¦ä½çš„æƒ…å†µï¼Œå…¶ä¸­æ¥è¿‘ä¼ æ„Ÿå™¨æ•°æ®æ˜¯è§‚å¯Ÿè¾“å…¥ä¹‹ä¸€ã€‚<div id="badge-59" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 59)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('59', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-60"><div class="row text-row"><div class="col-src">[[HEADER: F. Racing]]</div><div class="col-trans" id="trans-60"><b>F. Racing</b><div id="badge-60" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 60)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('60', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-61"><div class="row text-row"><div class="col-src">rban scenario encompasses almost all above driving tasks. Unlike driving tasks in a single scenario, navigation in urban areas requires agents to be able to simultaneously understand the characteristics of different scenarios, including various combinations of merges, intersections, roundabouts, etc. Urban scenarios involve vehicles and pedestrians with different characteristics, and much more semantic traffic elements, which can lead to more complex interactions.

Most related studies use the E2E framework to address this task. For example, Reference [105] deals with a BEV rendering of the scene (map, routing, surrounding objects and previous ego states) that is compressed to a low dim latent space using a Variational Auto-encoders (VAE). The latent state is then fed to an RL controller.</div><div class="col-trans" id="trans-61">rbanåœºæ™¯å‡ ä¹æ¶µç›–äº†æ‰€æœ‰ä¸Šè¿°é©¾é©¶ä»»åŠ¡ã€‚ä¸å•ä¸€åœºæ™¯ä¸­çš„é©¾é©¶ä»»åŠ¡ä¸åŒï¼ŒåŸå¸‚åŒºåŸŸçš„å¯¼èˆªè¦æ±‚ä»£ç†èƒ½å¤ŸåŒæ—¶ç†è§£ä¸åŒç±»å‹åœºæ™¯çš„ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬å„ç§åˆå¹¶ã€äº¤å‰å£ã€ç¯å²›ç­‰çš„ä¸åŒç»„åˆã€‚åŸå¸‚åœºæ™¯æ¶‰åŠå…·æœ‰ä¸åŒç‰¹æ€§çš„è½¦è¾†å’Œè¡Œäººï¼Œä»¥åŠæ›´å¤šçš„è¯­ä¹‰äº¤é€šå…ƒç´ ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´å¤æ‚çš„äº¤äº’ã€‚

å¤§å¤šæ•°ç›¸å…³ç ”ç©¶ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸€ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå‚è€ƒæ–‡çŒ®<a href="#ref-105" class="ref-link">[105]</a>å¤„ç†äº†åŸºäºé¸Ÿç°å›¾ï¼ˆBEVï¼‰çš„åœºæ™¯æ¸²æŸ“ï¼ˆåŒ…æ‹¬åœ°å›¾ã€è·¯å¾„è§„åˆ’ã€å‘¨å›´ç‰©ä½“å’Œä¹‹å‰çš„è‡ªæˆ‘çŠ¶æ€ï¼‰ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªä½ç»´æ½œåœ¨ç©ºé—´ä¸­ï¼Œä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚ç„¶åå°†æ½œåœ¨çŠ¶æ€è¾“å…¥åˆ°å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ä¸­ã€‚<div id="badge-61" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 61)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('61', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-62"><div class="row text-row"><div class="col-src">Scenario generalization capabilities need to be specifically considered in navigation tasks, as long-tailed/out-ofdistribution scenarios that are difficult to simulate with training data may appear in a city at any time. Anzalone et al. [106] present an E2E RL framework in the CARLA environment, utilizing the entire town map. The training process ranges from simple routing under speed constraints to a more complex phase involving randomized starting points and dynamic pedestrian scenarios.

Zhan et al. [107] utilize transformers to aggregate a collection of variable-sized and unordered traffic participants into a state vector. They implement offline training and decouple it from downstream RL control to prevent overfitting and improve generalizablity. In another study, Jin et al. [108] combine VAE with Generative Adversarial Networks (GAN) to encode input RGB images, thereby reducing the state dimension while also lowering the collision rate in adverse weather conditions.</div><div class="col-trans" id="trans-62">åœ¨å¯¼èˆªä»»åŠ¡ä¸­ï¼Œéœ€è¦ç‰¹åˆ«è€ƒè™‘åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºéš¾ä»¥é€šè¿‡è®­ç»ƒæ•°æ®æ¨¡æ‹Ÿçš„é•¿å°¾/åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰åœºæ™¯å¯èƒ½ä¼šéšæ—¶å‡ºç°åœ¨åŸå¸‚ä¸­ã€‚Anzaloneç­‰äºº<a href="#ref-106" class="ref-link">[106]</a>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåœ¨CARLAç¯å¢ƒä¸­åˆ©ç”¨æ•´ä¸ªåŸé•‡åœ°å›¾è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ä»åœ¨é€Ÿåº¦çº¦æŸä¸‹çš„ç®€å•è·¯å¾„è§„åˆ’å¼€å§‹ï¼Œé€æ­¥è¿‡æ¸¡åˆ°æ¶‰åŠéšæœºèµ·ç‚¹å’ŒåŠ¨æ€è¡Œäººæƒ…æ™¯çš„æ›´å¤æ‚é˜¶æ®µã€‚

Zhanç­‰äºº<a href="#ref-107" class="ref-link">[107]</a>ä½¿ç”¨å˜å‹å™¨å°†ä¸€ç»„å¤§å°å¯å˜ä¸”æ— åºçš„äº¤é€šå‚ä¸è€…èšåˆä¸ºä¸€ä¸ªçŠ¶æ€å‘é‡ã€‚ä»–ä»¬å®æ–½ç¦»çº¿è®­ç»ƒï¼Œå¹¶å°†å…¶ä¸ä¸‹æ¸¸çš„RLæ§åˆ¶åˆ†ç¦»ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¦ä¸€é¡¹ç ”ç©¶ä¸­ï¼ŒJinç­‰äºº<a href="#ref-108" class="ref-link">[108]</a>ç»“åˆäº†VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå¯¹è¾“å…¥çš„RGBå›¾åƒè¿›è¡Œç¼–ç ï¼Œä»è€Œé™ä½çŠ¶æ€ç»´åº¦å¹¶åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹å‡å°‘ç¢°æ’ç‡ã€‚<div id="badge-62" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 62)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('62', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-63"><div class="row text-row"><div class="col-src">Hu et al. [109] propose a querybased design and connect perception, prediction and planning nodes in an integrated large parameterized E2E framework that incorporates full-stack driving tasks. These approaches focus more on the generalization capability and comprehensive driving performance of RL-based MoP under different tasks, which usually requires a large network model and large amounts of training data.

acing is a specialized, niche automotive activity, that typically takes place on a fixed, enclosed circuit, requiring intense competition with other vehicles. The object of the racing task is to drive as fast as possible. Similar to lane changing/overtaking, racing requires continually surpassing SVs to improve the position within the vehicle pack, but the lane changing process is not bound by clear lanes and only needs to be within the racetrack.</div><div class="col-trans" id="trans-63">Huç­‰<a href="#ref-109" class="ref-link">[109]</a>æå‡ºäº†ä¸€ç§åŸºäºæŸ¥è¯¢çš„è®¾è®¡æ–¹æ³•ï¼Œå¹¶åœ¨åŒ…å«å…¨æ ˆé©¾é©¶ä»»åŠ¡çš„å¤§å‹å‚æ•°åŒ–ç«¯åˆ°ç«¯æ¡†æ¶ä¸­ï¼Œå°†æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’èŠ‚ç‚¹è¿›è¡Œæ•´åˆã€‚è¿™äº›æ–¹æ³•æ›´ä¾§é‡äºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œç»¼åˆé©¾é©¶æ€§èƒ½ï¼Œé€šå¸¸éœ€è¦è¾ƒå¤§çš„ç½‘ç»œæ¨¡å‹å’Œå¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚

èµ›è½¦æ˜¯ä¸€é¡¹ä¸“é—¨åŒ–çš„æ±½è½¦æ´»åŠ¨ï¼Œé€šå¸¸åœ¨å›ºå®šçš„å°é—­èµ›é“ä¸Šè¿›è¡Œï¼Œå¹¶ä¸”é€šå¸¸ä¸å…¶ä»–è½¦è¾†è¿›è¡Œæ¿€çƒˆçš„ç«äº‰ã€‚æ¯”èµ›çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å¿«åœ°è¡Œé©¶ã€‚ä¸å˜é“/è¶…è½¦ç±»ä¼¼ï¼Œèµ›è½¦ä¹Ÿéœ€è¦ä¸æ–­è¶…è¶Šå…¶ä»–è½¦è¾†ï¼ˆSVsï¼‰ä»¥æ”¹å–„è‡ªèº«çš„ä½ç½®ï¼Œä½†å˜é“è¿‡ç¨‹ä¸å—æ˜ç¡®è½¦é“çš„é™åˆ¶ï¼Œåªéœ€åœ¨èµ›é“èŒƒå›´å†…å³å¯ã€‚<div id="badge-63" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 63)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('63', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-64"><div class="row text-row"><div class="col-src">Additionally, racing places greater emphasis on controlling the vehicle to follow the best route, especially around the corners. The current way of controlling racing cars is dominated by the direct output of their acceleration and steering angle, but some studies also introduce high-level semantic behaviors for collaborative or adversarial purposes [41].

Typically, the observation inputs contain the position, speed, and heading angle of the EV, radar [110] or BEV images [111], and track progression information [6]. Moreover, some studies have further considered information such as tire temperature and engine speed [41] to achieve better control at high speeds.</div><div class="col-trans" id="trans-64">æ­¤å¤–ï¼Œèµ›è½¦æ›´åŠ æ³¨é‡æ§åˆ¶è½¦è¾†ä»¥éµå¾ªæœ€ä½³è·¯çº¿ï¼Œå°¤å…¶æ˜¯åœ¨å¼¯é“å¤„ã€‚å½“å‰æ§åˆ¶èµ›è½¦çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå…¶åŠ é€Ÿåº¦å’Œè½¬å‘è§’çš„ç›´æ¥è¾“å‡ºï¼Œä½†ä¹Ÿæœ‰ç ”ç©¶å¼•å…¥äº†é«˜çº§è¯­ä¹‰è¡Œä¸ºç”¨äºåä½œæˆ–å¯¹æŠ—ç›®çš„[[LINK: 41|<a href="#Figure_4" class="fig-link" onclick="highlightAsset('Figure_4'); return false;">Fig.4</a>]]ã€‚

é€šå¸¸ï¼Œè§‚æµ‹è¾“å…¥åŒ…æ‹¬ç”µåŠ¨æ±½è½¦çš„ä½ç½®ã€é€Ÿåº¦å’Œèˆªå‘è§’ï¼Œé›·è¾¾[[LINK: 110|<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">Fig.3</a>]] æˆ–é¸Ÿç°å›¾ï¼ˆBEVï¼‰å›¾åƒ[[LINK: 111|<a href="#Figure_3" class="fig-link" onclick="highlightAsset('Figure_3'); return false;">Fig.3</a>]]ï¼Œä»¥åŠèµ›é“è¿›åº¦ä¿¡æ¯[[LINK: 6|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]ã€‚æ­¤å¤–ï¼Œä¸€äº›ç ”ç©¶è¿˜è¿›ä¸€æ­¥è€ƒè™‘äº†è½®èƒæ¸©åº¦å’Œå‘åŠ¨æœºè½¬é€Ÿç­‰ä¿¡æ¯[[LINK: 41|<a href="#Figure_4" class="fig-link" onclick="highlightAsset('Figure_4'); return false;">Fig.4</a>]]ï¼Œä»¥å®ç°æ›´å¥½çš„é«˜é€Ÿæ§åˆ¶ã€‚<div id="badge-64" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 64)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('64', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-65"><div class="row text-row"><div class="col-src">In particular, the reward function should consider not only safety and efficiency, but also overtaking rewards [110] and even sportsmanship rules [6] due to the competitive nature of racing. Specifically, some studies use human presentation data to assist with policy updating or pre-training [41]. To obtain more robust overtaking strategies, some studies use curriculum learning for staged training [111].

In addition, [76] pursues to MARL to enhance agent consideration for high interaction and competitiveness. Notably, RL is now able to compete directly with and overtake top human racing players in a racing simulator [6].</div><div class="col-trans" id="trans-65">ç‰¹åˆ«æ˜¯åœ¨èµ›è½¦æ¯”èµ›ä¸­ï¼Œå¥–åŠ±å‡½æ•°ä¸ä»…åº”è€ƒè™‘å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œè¿˜åº”è€ƒè™‘åˆ°è¶…è¶Šå¥–åŠ±<a href="#ref-110" class="ref-link">[110]</a>ä»¥åŠä½“è‚²ç²¾ç¥è§„åˆ™<a href="#ref-6" class="ref-link">[6]</a>ç­‰ç«äº‰æ€§å› ç´ ã€‚å…·ä½“è€Œè¨€ï¼Œä¸€äº›ç ”ç©¶åˆ©ç”¨äººç±»æ¼”ç¤ºæ•°æ®æ¥è¾…åŠ©ç­–ç•¥æ›´æ–°æˆ–é¢„è®­ç»ƒ<a href="#ref-41" class="ref-link">[41]</a>ã€‚ä¸ºäº†è·å¾—æ›´ç¨³å¥çš„è¶…è¶Šç­–ç•¥ï¼Œä¸€äº›ç ”ç©¶ä½¿ç”¨åˆ†é˜¶æ®µè®­ç»ƒçš„æ–¹æ³•è¿›è¡Œè¯¾ç¨‹å­¦ä¹ <a href="#ref-111" class="ref-link">[111]</a>ã€‚

æ­¤å¤–ï¼Œ<a href="#ref-76" class="ref-link">[76]</a>è‡´åŠ›äºé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æé«˜ä»£ç†å¯¹é«˜äº¤äº’æ€§å’Œç«äº‰åŠ›çš„å…³æ³¨åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç°åœ¨çš„RLå·²ç»èƒ½å¤Ÿåœ¨èµ›è½¦æ¨¡æ‹Ÿå™¨ä¸­ç›´æ¥ä¸é¡¶çº§äººç±»èµ›è½¦æ‰‹ç«äº‰å¹¶è¶…è¶Šä»–ä»¬<a href="#ref-6" class="ref-link">[6]</a>ã€‚<div id="badge-65" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 65)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('65', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-66"><div class="row text-row"><div class="col-src">[[HEADER: G. Off-road Driving]]</div><div class="col-trans" id="trans-66"><b>G. è¶Šé‡é©¾é©¶</b><div id="badge-66" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 66)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('66', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-67"><div class="row text-row"><div class="col-src">In the context of off-road driving task, distinct road boundaries and traffic signs commonly found in urban scenarios may be lacking. In such situations, it is more important to consider the terrain, irregular obstacles, and cartographic data. Huang et al. [112] propose an RL-based 2.5D multiobjective path planning method.

On the processed small-size 2.5D maps, a reward function that combines terrain, distance, and boundary information is designed to achieve multiobjective path planning that balances energy consumption and distance through DQN method. A multi-objective RL is proposed in [113] as a solution to the path planning problem of an unmanned mining truck in an irregular environment. The feasible path is obtained by extrapolating the steering angle output by RL within a kinematic model in the simulation.</div><div class="col-trans" id="trans-67">åœ¨éå…¬è·¯é©¾é©¶ä»»åŠ¡çš„èƒŒæ™¯ä¸‹ï¼Œä¸åŸå¸‚åœºæ™¯ä¸­å¸¸è§çš„æ˜ç¡®é“è·¯è¾¹ç•Œå’Œäº¤é€šæ ‡å¿—ç›¸æ¯”ï¼Œå¯èƒ½ç¼ºä¹è¿™äº›å…ƒç´ ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ›´é‡è¦çš„æ˜¯è€ƒè™‘åœ°å½¢ã€ä¸è§„åˆ™éšœç¢ç‰©ä»¥åŠåœ°å›¾æ•°æ®ã€‚é»„ç­‰äººçš„ç ”ç©¶ <a href="#ref-112" class="ref-link">[112]</a> æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„äºŒç»´åŠï¼ˆ2.5Dï¼‰å¤šç›®æ ‡è·¯å¾„è§„åˆ’æ–¹æ³•ã€‚

åœ¨å¤„ç†è¿‡çš„è¾ƒå°å°ºå¯¸çš„ 2.5D åœ°å›¾ä¸Šï¼Œè®¾è®¡äº†ä¸€ä¸ªç»“åˆåœ°å½¢ã€è·ç¦»å’Œè¾¹ç•Œä¿¡æ¯çš„å¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDQN æ–¹æ³•ï¼‰å®ç°å¹³è¡¡èƒ½è€—ä¸è·ç¦»çš„å¤šç›®æ ‡è·¯å¾„è§„åˆ’ã€‚æ–‡çŒ® <a href="#ref-113" class="ref-link">[113]</a> æå‡ºäº†ä¸€ç§å¤šç›®æ ‡ RL æ–¹æ³•ï¼Œç”¨ä»¥è§£å†³åœ¨ä¸è§„åˆ™ç¯å¢ƒä¸­æ— äººé©¾é©¶çŸ¿è½¦çš„è·¯å¾„è§„åˆ’é—®é¢˜ã€‚é€šè¿‡åœ¨ä¸€ä¸ªåŠ¨åŠ›å­¦æ¨¡å‹ä¸­å¤–æ¨ç”± RL è¾“å‡ºçš„æ–¹å‘è§’æ¥è·å¾—å¯è¡Œè·¯å¾„ã€‚<div id="badge-67" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 67)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('67', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-68"><div class="row text-row"><div class="col-src">This technique is able to plan a path from the starting point to the target in less time than the hybrid A* algorithm. Zhang el al. [114] combine RL with Dynamic Window Approach (DWA). The state space includes local elevation map, vehicle attitude, obstacle object features, and the target information, while the action space consists of the weight parameters of the evaluation function in the DWA and the time period.

In addition, RL in the off-road scenario has to accommodate more complex with vehicle dynamics. Wang et al. [115] propose a model-based RL algorithm that trains a probabilistic dynamic model to consider model-uncertainty, thereby improving the accuracy of system predictions. More specifically, they train a System Identification Transformer and an Adaptive Dynamics Model under a variety of simulated dynamics, improving robustness and adaptability.</div><div class="col-trans" id="trans-68">è¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨æ¯”æ··åˆA*ç®—æ³•æ›´çŸ­çš„æ—¶é—´å†…ä»èµ·ç‚¹è§„åˆ’è·¯å¾„è‡³ç›®æ ‡ç‚¹ã€‚å¼ ç­‰äºº<a href="#ref-114" class="ref-link">[114]</a>å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸åŠ¨æ€çª—å£æ–¹æ³•ï¼ˆDWAï¼‰ç›¸ç»“åˆã€‚çŠ¶æ€ç©ºé—´åŒ…æ‹¬å±€éƒ¨åœ°å½¢å›¾ã€è½¦è¾†å§¿æ€ã€éšœç¢ç‰©ç‰¹å¾ä»¥åŠç›®æ ‡ä¿¡æ¯ï¼Œè€ŒåŠ¨ä½œç©ºé—´åˆ™ç”±DWAä¸­çš„è¯„ä¼°å‡½æ•°æƒé‡å‚æ•°å’Œæ—¶é—´å‘¨æœŸç»„æˆã€‚

æ­¤å¤–ï¼Œåœ¨éé“è·¯åœºæ™¯ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¿…é¡»è€ƒè™‘æ›´å¤æ‚çš„è½¦è¾†åŠ¨åŠ›å­¦é—®é¢˜ã€‚ç‹ç­‰äºº<a href="#ref-115" class="ref-link">[115]</a>æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„RLç®—æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªæ¦‚ç‡åŠ¨æ€æ¨¡å‹æ¥è€ƒè™‘æ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬é’ˆå¯¹å¤šç§æ¨¡æ‹ŸåŠ¨åŠ›å­¦æ¡ä»¶è®­ç»ƒäº†ä¸€ä¸ªç³»ç»Ÿè¯†åˆ«å˜æ¢å™¨å’Œè‡ªé€‚åº”åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œä»¥å¢å¼ºé²æ£’æ€§å’Œé€‚åº”æ€§ã€‚<div id="badge-68" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 68)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('68', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-69"><div class="row text-row"><div class="col-src">[[HEADER: IV. LESSONS FOR RL-BASED MOP DESIGN]]</div><div class="col-trans" id="trans-69">IV. åŸºäºRLçš„å¤šç›®æ ‡ä¼˜åŒ–è®¾è®¡çš„ç»éªŒæ•™è®­<div id="badge-69" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 69)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('69', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-70"><div class="row text-row"><div class="col-src">Most studies on RL-based MoP focus on specific driving tasks. Each driving task typically involves distinct scenario characteristics and task requirements, which significantly affect the RL agent design. The effective application of RL to a particular driving task requires careful consideration of several critical design elements, including the design of the observation input, the action output, the reward function, and the training environment. According to the review in Sec.

III, these design components vary substantially across different driving tasks and algorithms. In addition, these manual designs strongly influence subsequent self-learning and policy iterations. This section summarizes and analyzes the patterns of RL model design, aiming to extract lessons learned from various driving tasks and to provide clear guidelines for the application of RL-basedr MoP techniques for AD.</div><div class="col-trans" id="trans-70">å¤§å¤šæ•°åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç§»åŠ¨ç‰©ä½“å¤„ç†ï¼ˆMoPï¼‰ç ”ç©¶é›†ä¸­åœ¨ç‰¹å®šé©¾é©¶ä»»åŠ¡ä¸Šã€‚æ¯ä¸ªé©¾é©¶ä»»åŠ¡é€šå¸¸æ¶‰åŠä¸åŒçš„åœºæ™¯ç‰¹æ€§å’Œä»»åŠ¡è¦æ±‚ï¼Œè¿™æ˜¾è‘—å½±å“äº†RLä»£ç†çš„è®¾è®¡ã€‚å°†RLæœ‰æ•ˆåº”ç”¨äºç‰¹å®šé©¾é©¶ä»»åŠ¡éœ€è¦ä»”ç»†è€ƒè™‘å¤šä¸ªå…³é”®è®¾è®¡è¦ç´ ï¼ŒåŒ…æ‹¬è§‚å¯Ÿè¾“å…¥ã€åŠ¨ä½œè¾“å‡ºã€å¥–åŠ±å‡½æ•°ä»¥åŠè®­ç»ƒç¯å¢ƒçš„è®¾è®¡ã€‚æ ¹æ®ç¬¬ä¸‰ç« çš„å›é¡¾ï¼Œè¿™äº›è®¾è®¡ç»„ä»¶åœ¨ä¸åŒé©¾é©¶ä»»åŠ¡å’Œç®—æ³•ä¹‹é—´å·®å¼‚å¾ˆå¤§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ‰‹åŠ¨è®¾è®¡å¼ºçƒˆåœ°å½±å“åç»­çš„è‡ªæˆ‘å­¦ä¹ å’Œç­–ç•¥è¿­ä»£ã€‚æœ¬èŠ‚æ€»ç»“å¹¶åˆ†æäº†RLæ¨¡å‹è®¾è®¡æ¨¡å¼ï¼Œæ—¨åœ¨ä»å„ç§é©¾é©¶ä»»åŠ¡ä¸­æå–ç»éªŒæ•™è®­ï¼Œå¹¶ä¸ºåŸºäºRLçš„MoPæŠ€æœ¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰æä¾›æ˜ç¡®æŒ‡å—ã€‚<div id="badge-70" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 70)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('70', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-71"><div class="row text-row"><div class="col-src">[[HEADER: A. Observation Input]]</div><div class="col-trans" id="trans-71">A. è§‚æµ‹è¾“å…¥<div id="badge-71" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 71)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('71', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-72"><div class="row text-row"><div class="col-src">[[HEADER: 1) State Space Design]]</div><div class="col-trans" id="trans-72"><b>1) çŠ¶æ€ç©ºé—´è®¾è®¡</b><div id="badge-72" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 72)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('72', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-73"><div class="row text-row"><div class="col-src">nlike imitation learning, which constructs loss functions directly from expert data to establish input-output mappings, the RL agent collects feedback indirectly through interactions with the environment. This enables the RL agent to discover optimal solutions beyond expert data, but also makes establishing input-output mappings more difficult. Therefore, using low-dimensional feature data as model inputs simplifies the problem and accelerates the convergence process.

a) Physical Features: Physical features, which are highly abstract features processed by perception modules, are the most commonly used input information. They usually include the motion state of the EV (e.g., position, heading, speed, chassis states) and surrounding traffic participants (e.g., relative position, speed, distance).</div><div class="col-trans" id="trans-73">ä¸æ¨¡ä»¿å­¦ä¹ ç›´æ¥ä»ä¸“å®¶æ•°æ®æ„å»ºæŸå¤±å‡½æ•°ä»¥å»ºç«‹è¾“å…¥-è¾“å‡ºæ˜ å°„ä¸åŒï¼ŒRLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ä»£ç†é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’é—´æ¥æ”¶é›†åé¦ˆã€‚è¿™ä½¿å¾—RLä»£ç†èƒ½å¤Ÿå‘ç°è¶…è¶Šä¸“å®¶æ•°æ®çš„æœ€ä¼˜è§£ï¼Œä½†ä¹Ÿä½¿å»ºç«‹è¾“å…¥-è¾“å‡ºæ˜ å°„æ›´åŠ å›°éš¾ã€‚å› æ­¤ï¼Œä½¿ç”¨ä½ç»´ç‰¹å¾æ•°æ®ä½œä¸ºæ¨¡å‹è¾“å…¥ç®€åŒ–äº†é—®é¢˜å¹¶åŠ é€Ÿäº†æ”¶æ•›è¿‡ç¨‹ã€‚

a) ç‰©ç†ç‰¹å¾ï¼šç‰©ç†ç‰¹å¾æ˜¯ç»è¿‡æ„ŸçŸ¥æ¨¡å—å¤„ç†çš„é«˜åº¦æŠ½è±¡ç‰¹å¾ï¼Œé€šå¸¸è¢«ç”¨ä½œæœ€å¸¸ç”¨çš„è¾“å…¥ä¿¡æ¯ã€‚å®ƒä»¬é€šå¸¸åŒ…æ‹¬ç”µåŠ¨æ±½è½¦çš„è¿åŠ¨çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œä½ç½®ã€èˆªå‘ã€é€Ÿåº¦ã€åº•ç›˜çŠ¶æ€ï¼‰ä»¥åŠå‘¨å›´äº¤é€šå‚ä¸è€…ï¼ˆä¾‹å¦‚ï¼Œç›¸å¯¹ä½ç½®ã€é€Ÿåº¦ã€è·ç¦»ï¼‰ã€‚<div id="badge-73" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 73)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('73', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-74"><div class="row text-row"><div class="col-src">b) Sensor Input: To reduce the loss of input features, sensor information (e.g., camera images, birdâ€™s-eye-view (BEV), LiDAR point cloud, etc.) is fed directly to the RL agent, aiming to achieve higher performance. In most cases, even with raw sensor inputs, abstract physical features remain essential. The reason is that it is difficult for RL agent to learn effective policy from high-dimensional and multi-source sensor information entirely on its own.

In recent popular intersection/urban navigation tasks, the observation inputs are usually multimodal, including both physical features and multi-source sensor information. In some collaborative tasks, such as formation driving and cooperative merging, extra V2X communication information is considered. c) Auxiliary Representation: Additionally, some auxiliary representational inputs may be useful to help the RL agent better understand the surrounding environment and the task requirements.</div><div class="col-trans" id="trans-74">b) ä¼ æ„Ÿå™¨è¾“å…¥ï¼šä¸ºäº†å‡å°‘è¾“å…¥ç‰¹å¾çš„æŸå¤±ï¼Œç›´æ¥å°†ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œç›¸æœºå›¾åƒã€é¸Ÿç°å›¾(BEV)ã€æ¿€å…‰é›·è¾¾ç‚¹äº‘ç­‰ï¼‰é¦ˆé€ç»™RLä»£ç†ï¼Œæ—¨åœ¨å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå³ä½¿ä½¿ç”¨åŸå§‹ä¼ æ„Ÿå™¨è¾“å…¥ï¼ŒæŠ½è±¡çš„ç‰©ç†ç‰¹å¾ä»ç„¶è‡³å…³é‡è¦ã€‚åŸå› æ˜¯ï¼Œå¯¹äºé«˜ç»´åº¦å’Œå¤šæºä¼ æ„Ÿå™¨ä¿¡æ¯ï¼ŒRLä»£ç†å¾ˆéš¾å®Œå…¨è‡ªä¸»åœ°å­¦ä¹ æœ‰æ•ˆçš„ç­–ç•¥ã€‚

è¿‘å¹´æ¥ï¼Œåœ¨äº¤å‰å£/åŸå¸‚å¯¼èˆªä»»åŠ¡ä¸­ï¼Œè§‚å¯Ÿè¾“å…¥é€šå¸¸ä¸ºå¤šæ¨¡æ€çš„ï¼ŒåŒ…æ‹¬ç‰©ç†ç‰¹å¾å’Œå¤šæºä¼ æ„Ÿå™¨ä¿¡æ¯ã€‚åœ¨æŸäº›åä½œä»»åŠ¡ï¼ˆå¦‚ç¼–é˜Ÿé©¾é©¶å’ŒååŒæ±‡å…¥ï¼‰ä¸­ï¼Œè¿˜ä¼šè€ƒè™‘é¢å¤–çš„V2Xé€šä¿¡ä¿¡æ¯ã€‚c) è¾…åŠ©è¡¨ç¤ºï¼šæ­¤å¤–ï¼Œä¸€äº›è¾…åŠ©è¡¨ç¤ºè¾“å…¥ä¹Ÿå¯èƒ½æœ‰åŠ©äºå¸®åŠ©RLä»£ç†æ›´å¥½åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œä»»åŠ¡è¦æ±‚ã€‚<div id="badge-74" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 74)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('74', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-75"><div class="row text-row"><div class="col-src">For instance, a grid map [116] or risk potential field informing driving costs in [117]. For navigation in urban scenarios, the prior knowledge including road map, global route, and traffic rules is commonly used. The detailed categories and descriptions of the observation inputs are provided in TABLE I.

2) Multi-Model Observation Input As driving tasks of interest to researchers become increasingly complex and RL algorithms continue to advance, research on RL-based MoP has evolved from using primarily physical features as observation inputs to integrating a broader range of information. Notably, the urban navigation tasks predominantly employs E2E architecture, which encompasses most independent driving tasks and has become the most prevalent paradigm for future research.</div><div class="col-trans" id="trans-75">ä¾‹å¦‚ï¼Œä¸€ä¸ªç½‘æ ¼åœ°å›¾<a href="#ref-116" class="ref-link">[116]</a>æˆ–é£é™©æ½œåŠ›åœºï¼Œåœ¨å…¶ä¸­æä¾›äº†é©¾é©¶æˆæœ¬ä¿¡æ¯[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]ã€‚åœ¨åŸå¸‚åœºæ™¯ä¸­çš„å¯¼èˆªä¸­ï¼Œé€šå¸¸ä¼šä½¿ç”¨å…ˆéªŒçŸ¥è¯†åŒ…æ‹¬é“è·¯å›¾ã€å…¨å±€è·¯å¾„å’Œäº¤é€šè§„åˆ™ç­‰ã€‚è§‚æµ‹è¾“å…¥çš„å…·ä½“ç±»åˆ«å’Œæè¿°è§è¡¨Iã€‚

2) å¤šæ¨¡å‹è§‚æµ‹è¾“å…¥ éšç€ç ”ç©¶äººå‘˜æ„Ÿå…´è¶£çš„é©¾é©¶ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ä»¥åŠRLç®—æ³•çš„ä¸æ–­è¿›æ­¥ï¼ŒåŸºäºRLçš„æ–¹æ³•è®ºï¼ˆMoPï¼‰çš„ç ”ç©¶å·²ç»ä»ä¸»è¦ä¾èµ–ç‰©ç†ç‰¹å¾ä½œä¸ºè§‚æµ‹è¾“å…¥å‘å±•åˆ°æ•´åˆæ›´å¹¿æ³›çš„ä¿¡æ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸå¸‚å¯¼èˆªä»»åŠ¡ä¸»è¦é‡‡ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„æ¶µç›–äº†å¤§å¤šæ•°ç‹¬ç«‹é©¾é©¶ä»»åŠ¡ï¼Œå¹¶å·²æˆä¸ºæœªæ¥ç ”ç©¶ä¸­æœ€æ™®éçš„èŒƒå¼ã€‚<div id="badge-75" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 75)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('75', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-76"><div class="row text-row"><div class="col-src">However, E2E RL also results in a very redundant observation space and usually requires the integration of a large number of sensor inputs in addition to physical features. Furthermore, different driving tasks need to be supplemented with specific auxiliary representations, such as map information for urban navigation or terrain information for off-road driving. While these multi-source</div><div class="col-trans" id="trans-76">ç„¶è€Œï¼Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ä¹Ÿä¼šå¯¼è‡´ä¸€ä¸ªéå¸¸å†—ä½™çš„è§‚æµ‹ç©ºé—´ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦é›†æˆå¤§é‡çš„ä¼ æ„Ÿå™¨è¾“å…¥ä»¥åŠç‰©ç†ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸åŒçš„é©¾é©¶ä»»åŠ¡è¿˜éœ€è¦è¡¥å……ç‰¹å®šçš„è¾…åŠ©è¡¨ç¤ºï¼Œä¾‹å¦‚åŸå¸‚å¯¼èˆªæ‰€éœ€çš„åœ°å›¾ä¿¡æ¯æˆ–è¶Šé‡é©¾é©¶æ‰€éœ€çš„åœ°å½¢ä¿¡æ¯ã€‚è™½ç„¶è¿™äº›å¤šæº<div id="badge-76" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 76)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('76', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-77"><div class="row text-row"><div class="col-src">[[HEADER: p p 2) Multi-Model Observation Input]]</div><div class="col-trans" id="trans-77">p p 2) å¤šæ¨¡å‹è§‚æµ‹è¾“å…¥<br><div id="badge-77" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 77)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('77', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_1</div><img src="./assets/Table_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">TABLE I OMMON OBSERVATION INPUT CATEGORIES AND DESCRIPTIONS</div><div class="asset-desc-zh">TABLE I å¸¸è§è§‚å¯Ÿè¾“å…¥ç±»åˆ«åŠå…¶æè¿°</div></div></div></div><div class="row-container" id="task-78"><div class="row text-row"><div class="col-src">observation inputs contain nearly all environmental features, directly feeding them into policy networks for learning may increase computational complexity and impede the efficient extraction of latent features. In addition, when these multisource observation inputs are incomplete or perturbed, policy execution may be unstable.</div><div class="col-trans" id="trans-78">è§‚æµ‹è¾“å…¥å‡ ä¹åŒ…å«äº†æ‰€æœ‰ç¯å¢ƒç‰¹å¾ï¼Œç›´æ¥å°†è¿™äº›ä¿¡æ¯å–‚å…¥ç­–ç•¥ç½‘ç»œè¿›è¡Œå­¦ä¹ å¯èƒ½ä¼šå¢åŠ è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶å¦¨ç¢æ½œåœ¨ç‰¹å¾çš„æœ‰æ•ˆæå–ã€‚æ­¤å¤–ï¼Œå½“è¿™äº›å¤šæºè§‚æµ‹è¾“å…¥ä¸å®Œæ•´æˆ–å—åˆ°å¹²æ‰°æ—¶ï¼Œç­–ç•¥æ‰§è¡Œå¯èƒ½ä¼šå˜å¾—ä¸ç¨³å®šã€‚<div id="badge-78" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 78)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('78', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-79"><div class="row text-row"><div class="col-src">[[HEADER: B. Action Output]]</div><div class="col-trans" id="trans-79"><b>B. Action Output</b><div id="badge-79" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 79)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('79', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-80"><div class="row text-row"><div class="col-src">he agent interacts with the environment by executing actions, updating its state accordingly. In the current RL theoretical framework, the form of action output directly determines the model type, and is generally classified into discrete and continuous action. In addition, there are some approaches that use indirect actions and hierarchical actions.

2) Action Space Design a) Discrete Commands: Discrete commands can represent high level decisions such as LCL, LCR and LK for the lateral vehicle behavior or a discrete set of acceleration/deceleration values in term of longitudinal dynamics. They can be viewed as prescribing vehicle semantic behavior. In some early studies, a set of discrete instructions was also used as an action space, thus the agent could to select one of discrete values, e.g., for the steering angle.</div><div class="col-trans" id="trans-80">æ™ºèƒ½ä½“é€šè¿‡æ‰§è¡ŒåŠ¨ä½œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°å…¶çŠ¶æ€ã€‚åœ¨å½“å‰çš„å¼ºåŒ–å­¦ä¹ ç†è®ºæ¡†æ¶ä¸­ï¼ŒåŠ¨ä½œè¾“å‡ºçš„å½¢å¼ç›´æ¥å†³å®šäº†æ¨¡å‹ç±»å‹ï¼Œé€šå¸¸è¢«åˆ†ä¸ºç¦»æ•£åŠ¨ä½œå’Œè¿ç»­åŠ¨ä½œä¸¤ç±»ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›æ–¹æ³•ä½¿ç”¨é—´æ¥åŠ¨ä½œå’Œå±‚æ¬¡åŒ–åŠ¨ä½œã€‚

2) åŠ¨ä½œç©ºé—´è®¾è®¡  
a) ç¦»æ•£å‘½ä»¤ï¼šç¦»æ•£å‘½ä»¤å¯ä»¥è¡¨ç¤ºæ¨ªå‘è½¦è¾†è¡Œä¸ºï¼ˆå¦‚LCLã€LCRå’ŒLKï¼‰æˆ–çºµå‘åŠ¨åŠ›å­¦ä¸­çš„åŠ å‡é€Ÿå€¼ç­‰é«˜å±‚æ¬¡å†³ç­–ã€‚å®ƒä»¬å¯ä»¥è¢«è§†ä¸ºè§„å®šäº†è½¦è¾†çš„è¯­ä¹‰è¡Œä¸ºã€‚åœ¨ä¸€äº›æ—©æœŸçš„ç ”ç©¶ä¸­ï¼Œä¸€ç»„ç¦»æ•£æŒ‡ä»¤ä¹Ÿè¢«ç”¨ä½œåŠ¨ä½œç©ºé—´ï¼Œå› æ­¤æ™ºèƒ½ä½“å¯ä»¥é€‰æ‹©è¿™äº›ç¦»æ•£å€¼ä¹‹ä¸€ï¼Œä¾‹å¦‚è½¬å‘è§’ã€‚<div id="badge-80" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 80)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('80', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-81"><div class="row text-row"><div class="col-src">Moreover, Yu et al. [71] allow the RL agent to select candidate trajectories from a real-time generated trajectory set. b) Continuous Commands: However, by restricting to a discrete set of commands, one may not be able to obtain optimal solutions in various dynamic scenarios, especially at the vehicle control level. In addition, sudden changes in discrete commands can cause jerky oscillations in driving maneuvers.

If an RL agent aims to directly control vehicle motion, it typically outputs continuous commands such as the steering angle and acceleration. Therefore, Policy-based and Actor-Critic methods are widely used for direct vehicle control; they can be applied to nearly all AD MoP tasks.</div><div class="col-trans" id="trans-81">æ­¤å¤–ï¼ŒYuç­‰äºº<a href="#ref-71" class="ref-link">[71]</a>å…è®¸RLä»£ç†ä»å®æ—¶ç”Ÿæˆçš„è½¨è¿¹é›†ä¸­é€‰æ‹©å€™é€‰è½¨è¿¹ã€‚b) è¿ç»­å‘½ä»¤ï¼šç„¶è€Œï¼Œåœ¨åŠ¨æ€åœºæ™¯ä¸­é™åˆ¶åœ¨ä¸€ç»„ç¦»æ•£å‘½ä»¤å¯èƒ½æ— æ³•è·å¾—æœ€ä¼˜è§£ï¼Œå°¤å…¶æ˜¯åœ¨è½¦è¾†æ§åˆ¶å±‚é¢ã€‚å¦å¤–ï¼Œç¦»æ•£å‘½ä»¤çš„çªç„¶å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´é©¾é©¶åŠ¨ä½œå‡ºç°ç”Ÿç¡¬çš„æŒ¯è¡ã€‚

å¦‚æœRLä»£ç†æ—¨åœ¨ç›´æ¥æ§åˆ¶è½¦è¾†è¿åŠ¨ï¼Œåˆ™é€šå¸¸ä¼šè¾“å‡ºè¿ç»­å‘½ä»¤ï¼Œå¦‚è½¬å‘è§’å’ŒåŠ é€Ÿåº¦ã€‚å› æ­¤ï¼ŒåŸºäºç­–ç•¥çš„æ–¹æ³•å’Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•å¹¿æ³›ç”¨äºç›´æ¥è½¦è¾†æ§åˆ¶ï¼›å®ƒä»¬å¯ä»¥åº”ç”¨äºå‡ ä¹æ‰€æœ‰AD MoPä»»åŠ¡ã€‚<div id="badge-81" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 81)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('81', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-82"><div class="row text-row"><div class="col-src">[[HEADER: pp 2) Action Space Design]]</div><div class="col-trans" id="trans-82">pp. 2) åŠ¨ä½œç©ºé—´è®¾è®¡<br><div id="badge-82" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 82)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('82', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_2</div><img src="./assets/Table_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">TABLE II COMMONLY USED ACTION OUTPUT FOR RL-BASED MOP</div><div class="asset-desc-zh">TABLE II åŸºäºRLçš„MoPä¸­å¸¸ç”¨çš„è¡ŒåŠ¨è¾“å‡º</div></div></div></div><div class="row-container" id="task-83"><div class="row text-row"><div class="col-src">c) Indirect Commands: In addition, an indirect command can be an output to affect the vehicleâ€™s motion. For instance, some studies indirectly plan continuous feasible trajectories by learning trajectory parameters (such as polynomial coefficients, objective function weights, etc.). Through parameterized action based RL framework, heterogeneous trajectory parameters can be generated synchronously, as in recent PDQN [124], RL-TPA [122], and other approaches.

Other works generate interactive actions to determine reference states for MoP, and then solve for optimal trajectories by, for example, constructing an optimal control problem, e.g., RL+MPC [125], etc. d) Hierarchical Actions: HRL uses different networks to determine heterogeneous components of whole actions.</div><div class="col-trans" id="trans-83">c) é—´æ¥å‘½ä»¤ï¼šæ­¤å¤–ï¼Œé—´æ¥å‘½ä»¤å¯ä»¥æ˜¯è¾“å‡ºä»¥å½±å“è½¦è¾†çš„è¿åŠ¨ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ç ”ç©¶é€šè¿‡å­¦ä¹ è½¨è¿¹å‚æ•°ï¼ˆå¦‚å¤šé¡¹å¼ç³»æ•°ã€ç›®æ ‡å‡½æ•°æƒé‡ç­‰ï¼‰é—´æ¥è§„åˆ’è¿ç»­å¯è¡Œè½¨è¿¹ã€‚å€ŸåŠ©åŸºäºå‚æ•°åŒ–åŠ¨ä½œçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸åŒç±»å‹çš„è½¨è¿¹å‚æ•°å¯ä»¥åŒæ­¥ç”Ÿæˆï¼Œç±»ä¼¼äºæœ€è¿‘çš„PDQN <a href="#ref-124" class="ref-link">[124]</a>ã€RL-TPA <a href="#ref-122" class="ref-link">[122]</a> ç­‰æ–¹æ³•ã€‚

å…¶ä»–ç ”ç©¶ç”Ÿæˆäº¤äº’åŠ¨ä½œä»¥ç¡®å®šMoPï¼ˆMotion Planningï¼‰çš„ç›®æ ‡çŠ¶æ€ï¼Œå¹¶é€šè¿‡æ„å»ºæœ€ä¼˜æ§åˆ¶é—®é¢˜ç­‰æ–¹æ³•æ±‚è§£æœ€ä¼˜è½¨è¿¹ï¼Œä¾‹å¦‚RL+MPC <a href="#ref-125" class="ref-link">[125]</a> ç­‰ã€‚d) åˆ†å±‚åŠ¨ä½œï¼šå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHRLï¼‰ä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ¥å†³å®šæ•´ä¸ªåŠ¨ä½œçš„ä¸åŒç»„æˆéƒ¨åˆ†ã€‚<div id="badge-83" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 83)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('83', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-84"><div class="row text-row"><div class="col-src">The hierarchy can be serial (upper layer outputs discrete behaviors, lower layer outputs control commands), parallel (heterogeneous control commands are output simultaneously), or even hybrid [61], [120], [123]. Complex driving tasks can be simplified and split in this way but may suffer from sparse rewards.

In particular, actions in different layers may be executed at different timescales, e.g., the upper layerâ€™s lane change decision may be updated over a long timestep, whereas the lower layerâ€™s steering commands may be outputted over a short timestep. In strictly HRL theory, a synchronously learned factor Î² is introduced to determine the update timing for upper level action [38], but this remains underexplored in MoP research.</div><div class="col-trans" id="trans-84">å±‚æ¬¡ç»“æ„å¯ä»¥æ˜¯ä¸²è¡Œçš„ï¼ˆé«˜å±‚è¾“å‡ºç¦»æ•£è¡Œä¸ºï¼Œä½å±‚è¾“å‡ºæ§åˆ¶å‘½ä»¤ï¼‰ã€å¹¶è¡Œçš„ï¼ˆåŒæ—¶è¾“å‡ºå¼‚æ„æ§åˆ¶å‘½ä»¤ï¼‰ï¼Œæˆ–è€…ç”šè‡³æ˜¯æ··åˆå‹çš„[<a href="#ref-61" class="ref-link">[61]</a>,<a href="#ref-120" class="ref-link">[120]</a>,<a href="#ref-123" class="ref-link">[123]</a>]ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¤æ‚çš„é©¾é©¶ä»»åŠ¡å¯ä»¥è¢«ç®€åŒ–å’Œæ‹†åˆ†ï¼Œä½†å¯èƒ½ä¼šé­å—ç¨€ç–å¥–åŠ±çš„é—®é¢˜ã€‚

ç‰¹åˆ«æ˜¯ï¼Œåœ¨ä¸åŒçš„å±‚æ¬¡ä¸­æ‰§è¡Œçš„åŠ¨ä½œå¯èƒ½åœ¨ä¸åŒçš„æ—¶é—´å°ºåº¦ä¸Šè¿›è¡Œï¼Œä¾‹å¦‚ï¼Œé«˜å±‚çš„å˜é“å†³ç­–å¯èƒ½åœ¨ä¸€ä¸ªé•¿çš„æ—¶é—´æ­¥å†…æ›´æ–°ï¼Œè€Œä½å±‚çš„è½¬å‘å‘½ä»¤åˆ™å¯èƒ½åœ¨ä¸€ä¸ªçŸ­çš„æ—¶é—´æ­¥å†…è¾“å‡ºã€‚åœ¨ä¸¥æ ¼çš„æ„ä¹‰ä¸Šçš„äººå·¥æ™ºèƒ½å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆHierarchical Reinforcement Learning, HRLï¼‰ç†è®ºä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŒæ­¥å­¦ä¹ å› å­Î²æ¥ç¡®å®šé«˜å±‚åŠ¨ä½œçš„æ›´æ–°æ—¶æœº[<a href="#ref-38" class="ref-link">[38]</a>]ï¼Œä½†åœ¨MoPç ”ç©¶ä¸­è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚<div id="badge-84" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 84)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('84', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-85"><div class="row text-row"><div class="col-src">In addition, the optimal consistency of the hierarchically generated actions may be affected since the upper-layer network cannot fully access the policy information of the lower-layer network when generating actions. Note also that the parameterized action based RL can be further extended to hierarchical architectures [126]. 2) Control Granularity Early research on RL for AD focused on decision-making for discrete behaviors.

In recent years, direct control of the steering angle and acceleration has become a simple and popular choice; notably it facilitates determining continuous driving</div><div class="col-trans" id="trans-85">æ­¤å¤–ï¼Œç”±äºé«˜å±‚ç½‘ç»œåœ¨ç”ŸæˆåŠ¨ä½œæ—¶æ— æ³•å®Œå…¨è®¿é—®ä½å±‚ç½‘ç»œçš„ç­–ç•¥ä¿¡æ¯ï¼Œå› æ­¤å±‚æ¬¡ç”Ÿæˆçš„åŠ¨ä½œçš„ä¸€è‡´æ€§å¯èƒ½å—åˆ°å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºå‚æ•°åŒ–åŠ¨ä½œçš„å¼ºåŒ–å­¦ä¹ å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ°å±‚æ¬¡ç»“æ„ä¸­[[LINK: 126|<a href="#ref-126" class="ref-link">[126]</a>]]ã€‚2) æ§åˆ¶ç²’åº¦

æ—©æœŸå¯¹RLåœ¨ADä¸­çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç¦»æ•£è¡Œä¸ºçš„å†³ç­–åˆ¶å®šä¸Šã€‚

è¿‘å¹´æ¥ï¼Œç›´æ¥æ§åˆ¶è½¬å‘è§’å’ŒåŠ é€Ÿåº¦å·²æˆä¸ºä¸€ç§ç®€å•ä¸”æµè¡Œçš„é€‰æ‹©ï¼›æ˜¾è‘—åœ°ï¼Œè¿™ä¾¿äºç¡®å®šè¿ç»­é©¾é©¶è¡Œä¸ºã€‚<div id="badge-85" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 85)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('85', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-86"><div class="row text-row"><div class="col-src">[[HEADER: 2) Control Granularity]]</div><div class="col-trans" id="trans-86"><b>2) æ§åˆ¶ç²’åº¦</b><div id="badge-86" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 86)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('86', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-87"><div class="row text-row"><div class="col-src">actions through a much larger policy network. Thinking about control granularity cannot be ignored. At the same time, applying RL to trajectory-level actions can enrich the control granularity of RL-based MoP and improve the agentâ€™s ability to focus on driving behaviors, control them more accurately and cope with complex, dynamic road environments.

However, the loss function in RL is typically generated indirectly based on accumulated reward signals, rather than being directly derived from expert trajectory data, as in IL. Consequently, achieving convergence for high-dimensional waypoint actions is often difficult. Related research based on a parameterized action space [126] has emerged as a promising direction to facilitate action granularity design.</div><div class="col-trans" id="trans-87">é€šè¿‡ä¸€ä¸ªæ›´å¤§çš„ç­–ç•¥ç½‘ç»œæ¥å®ç°è¿™äº›åŠ¨ä½œã€‚è€ƒè™‘æ§åˆ¶ç²’åº¦æ˜¯ä¸å¯å¿½è§†çš„ã€‚åŒæ—¶ï¼Œå°†RLåº”ç”¨äºè½¨è¿¹çº§åŠ¨ä½œå¯ä»¥ä¸°å¯ŒåŸºäºRLçš„æ–¹æ³•ï¼ˆMoPï¼‰çš„æ§åˆ¶ç²’åº¦ï¼Œå¹¶æé«˜ä»£ç†å¯¹é©¾é©¶è¡Œä¸ºçš„å…³æ³¨èƒ½åŠ›ã€æ›´å‡†ç¡®åœ°æ§åˆ¶è¿™äº›è¡Œä¸ºä»¥åŠåº”å¯¹å¤æ‚å¤šå˜çš„é“è·¯ç¯å¢ƒã€‚

ç„¶è€Œï¼ŒRLä¸­çš„æŸå¤±å‡½æ•°é€šå¸¸æ˜¯é—´æ¥æ ¹æ®ç´¯ç§¯å¥–åŠ±ä¿¡å·ç”Ÿæˆçš„ï¼Œè€Œä¸æ˜¯åƒåœ¨ILä¸­é‚£æ ·ç›´æ¥ä»ä¸“å®¶è½¨è¿¹æ•°æ®ä¸­æ¨å¯¼å‡ºæ¥çš„ã€‚å› æ­¤ï¼Œå¯¹äºé«˜ç»´èˆªç‚¹åŠ¨ä½œå®ç°æ”¶æ•›å¾€å¾€æ˜¯å›°éš¾çš„ã€‚åŸºäºå‚æ•°åŒ–åŠ¨ä½œç©ºé—´çš„ç›¸å…³ç ”ç©¶<a href="#ref-126" class="ref-link">[126]</a>å·²ç» emerged ä½œä¸ºä¿ƒè¿›åŠ¨ä½œç²’åº¦è®¾è®¡çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚<div id="badge-87" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 87)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('87', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-88"><div class="row text-row"><div class="col-src">Enabling the RL agent to output trajectory waypoints can further exploit the capabilities of RL. For instance, developers could evaluate the RL interaction with the environment objectively, which makes it easier to design a safety guarantee. Furthermore, this could greatly enhance the interpretability of RL MoP. The commonly used action outputs for RL-based MoP are summarized in TABLE II.</div><div class="col-trans" id="trans-88">ä½¿RLä»£ç†èƒ½å¤Ÿè¾“å‡ºè½¨è¿¹èˆªç‚¹å¯ä»¥è¿›ä¸€æ­¥å‘æŒ¥RLçš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¼€å‘è€…å¯ä»¥å®¢è§‚åœ°è¯„ä¼°RLä¸ç¯å¢ƒçš„äº¤äº’æƒ…å†µï¼Œè¿™ä½¿å¾—è®¾è®¡å®‰å…¨ä¿è¯å˜å¾—æ›´åŠ å®¹æ˜“ã€‚æ­¤å¤–ï¼Œè¿™è¿˜èƒ½å¤§å¤§å¢å¼ºåŸºäºRLçš„æ–¹æ³•çš„æ“ä½œæ¨¡å¼ï¼ˆMoPï¼‰çš„å¯è§£é‡Šæ€§ã€‚å¸¸ç”¨çš„åŸºäºRLçš„æ–¹æ³•çš„åŠ¨ä½œè¾“å‡ºæ€»ç»“åœ¨TABLE IIä¸­ã€‚<div id="badge-88" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 88)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('88', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-89"><div class="row text-row"><div class="col-src">[[HEADER: C. Reward Function]]</div><div class="col-trans" id="trans-89">C. å¥–åŠ±å‡½æ•°<div id="badge-89" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 89)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('89', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-90"><div class="row text-row"><div class="col-src">Reward design can significantly influence the performance of the RL agent as it directly informs the loss function required for network updating. Driving is a multi-attribute problem, and these attributes may include the time to reach destination, travel distance, collision, legal compliance, energy consumption, passenger experience, impacts on the traffic environment, etc. [127].

Defining a driving performance metric for an autonomous vehicle involves identifying various attributes and quantitatively describing them, and then combining them into a utility function. Current RL models for AD typically formulate the reward function as a weighted linear combination [16]. 1) Reward Attributes Safety, efficiency, comfort, and traffic compliance are typical attributes considered when designing reward functions for AD.</div><div class="col-trans" id="trans-90">å¥–åŠ±è®¾è®¡å¯ä»¥æ˜¾è‘—å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒç›´æ¥å†³å®šäº†ç”¨äºç½‘ç»œæ›´æ–°æ‰€éœ€çš„æŸå¤±å‡½æ•°ã€‚é©¾é©¶æ˜¯ä¸€ä¸ªå¤šå±æ€§é—®é¢˜ï¼Œè¿™äº›å±æ€§å¯èƒ½åŒ…æ‹¬åˆ°è¾¾ç›®çš„åœ°çš„æ—¶é—´ã€è¡Œé©¶è·ç¦»ã€ç¢°æ’ã€æ³•å¾‹åˆè§„æ€§ã€èƒ½è€—ã€ä¹˜å®¢ä½“éªŒä»¥åŠå¯¹äº¤é€šç¯å¢ƒçš„å½±å“ç­‰[<a href="#ref-127" class="ref-link">[127]</a>]ã€‚

ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†å®šä¹‰é©¾é©¶æ€§èƒ½æŒ‡æ ‡æ¶‰åŠè¯†åˆ«å„ç§å±æ€§å¹¶å®šé‡æè¿°å®ƒä»¬ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªæ•ˆç”¨å‡½æ•°ã€‚å½“å‰ç”¨äºADçš„RLæ¨¡å‹é€šå¸¸å°†å¥–åŠ±å‡½æ•°å½¢å¼åŒ–ä¸ºåŠ æƒçº¿æ€§ç»„åˆ<a href="#ref-16" class="ref-link">[16]</a>ã€‚1) å¥–åŠ±å±æ€§åœ¨è®¾è®¡ADçš„å¥–åŠ±å‡½æ•°æ—¶ï¼Œå®‰å…¨æ€§ã€æ•ˆç‡ã€èˆ’é€‚æ€§å’Œäº¤é€šåˆè§„æ€§æ˜¯å…¸å‹çš„è€ƒè™‘å› ç´ ã€‚<div id="badge-90" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 90)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('90', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-91"><div class="row text-row"><div class="col-src">On this basis, different driving tasks can incorporate unique reward functions to better achieve task-specific goals. a) Safety: Whether a collision occurs, including collision with vehicles or pedestrians, or out of road, is a direct measure of the safety. Some metrics that reflect the degree of potential danger are used to jointly describe the safety attribute, e.g., time to collision (TTC), and distance to SVs (DTV).

b) Efficiency: For efficiency, speed is a commonly used metric, e.g., driving at the desired speed or as fast as possible. Additionally, success, i.e. reaching the goal or completing the task, and the corresponding costs can be used as other important metrics for specific tasks. Examples include the success rate and time spent passing through an intersection, merging onto a main road, and completing a parking task.</div><div class="col-trans" id="trans-91">åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä¸åŒçš„é©¾é©¶ä»»åŠ¡å¯ä»¥é‡‡ç”¨ç‹¬ç‰¹çš„å¥–åŠ±å‡½æ•°ä»¥æ›´å¥½åœ°å®ç°ç‰¹å®šç›®æ ‡ã€‚a) å®‰å…¨æ€§ï¼šæ˜¯å¦å‘ç”Ÿç¢°æ’ï¼ˆåŒ…æ‹¬ä¸è½¦è¾†æˆ–è¡Œäººçš„ç¢°æ’ï¼Œæˆ–è€…åç¦»é“è·¯ï¼‰ï¼Œæ˜¯ç›´æ¥è¡¡é‡å®‰å…¨æ€§çš„æŒ‡æ ‡ã€‚ä¸€äº›åæ˜ æ½œåœ¨å±é™©ç¨‹åº¦çš„åº¦é‡æ ‡å‡†è¢«ç”¨æ¥å…±åŒæè¿°å®‰å…¨æ€§ï¼Œä¾‹å¦‚ç¢°æ’å‰æ—¶é—´ (TTC) å’Œä¸æœ€è¿‘éšœç¢ç‰©çš„è·ç¦» (DTV)ã€‚

b) æ•ˆç‡ï¼šå¯¹äºæ•ˆç‡è€Œè¨€ï¼Œé€Ÿåº¦æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„åº¦é‡æ ‡å‡†ï¼Œæ¯”å¦‚ä»¥æœŸæœ›çš„é€Ÿåº¦è¡Œé©¶æˆ–å°½å¯èƒ½å¿«åœ°è¡Œé©¶ã€‚æ­¤å¤–ï¼ŒæˆåŠŸï¼ˆå³è¾¾åˆ°ç›®æ ‡æˆ–å®Œæˆä»»åŠ¡ï¼‰åŠå…¶ç›¸åº”çš„æˆæœ¬å¯ä»¥ä½œä¸ºå…¶ä»–é‡è¦æŒ‡æ ‡ç”¨äºç‰¹å®šä»»åŠ¡ã€‚ä¾‹å¦‚æˆåŠŸç‡ã€é€šè¿‡äº¤å‰å£ã€å¹¶å…¥ä¸»è·¯ä»¥åŠå®Œæˆæ³Šè½¦ä»»åŠ¡æ‰€èŠ±è´¹çš„æ—¶é—´ç­‰ã€‚<div id="badge-91" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 91)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('91', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-92"><div class="row text-row"><div class="col-src">c) Comfort: The reward for comfort is relatively straightforward to define and is usually correlated with the smoothness of the vehicleâ€™s motion, including jerk, and lateral acceleration. Some studies use the variance of acceleration and the steering angle to further measure motion smoothness/comfort. In particular, racing and off-road tasks prioritize maneuverability over comfort.</div><div class="col-trans" id="trans-92">c) èˆ’é€‚åº¦ï¼šèˆ’é€‚åº¦çš„å¥–åŠ±ç›¸å¯¹å®¹æ˜“å®šä¹‰ï¼Œé€šå¸¸ä¸è½¦è¾†è¿åŠ¨çš„å¹³æ»‘æ€§ç›¸å…³ï¼ŒåŒ…æ‹¬åŠ é€Ÿåº¦å’Œè½¬å‘è§’çš„å˜åŒ–ç‡ï¼ˆjerkï¼‰ä»¥åŠä¾§å‘åŠ é€Ÿåº¦ã€‚ä¸€äº›ç ”ç©¶è¿˜ä½¿ç”¨åŠ é€Ÿåº¦å’Œè½¬å‘è§’åº¦çš„æ–¹å·®æ¥è¿›ä¸€æ­¥è¡¡é‡è¿åŠ¨çš„å¹³æ»‘æ€§å’Œèˆ’é€‚åº¦ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨èµ›è½¦å’Œè¶Šé‡ä»»åŠ¡ä¸­ï¼ŒæœºåŠ¨æ€§ä¼˜å…ˆäºèˆ’é€‚åº¦ã€‚<div id="badge-92" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 92)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('92', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-93"><div class="row text-row"><div class="col-src">[[HEADER: he reward function as a 1) Reward Attributes]]</div><div class="col-trans" id="trans-93"><b>he reward function as a 1) Reward Attributes</b><div id="badge-93" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 93)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('93', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_3</div><img src="./assets/Table_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">TABLE III REWARD DESIGN FOR RL MOP APPROACHES</div><div class="asset-desc-zh">TABLE III RL MoPæ–¹æ³•ä¸­çš„å¥–åŠ±è®¾è®¡</div></div></div></div><div class="row-container" id="task-94"><div class="row text-row"><div class="col-src">d) Traffic Compliance: Traffic rules represent a highly complex set of guidelines, encompassing multiple semantic levels of understanding and evaluation [128]. Common traffic rule conformance includes adhering to traffic signals, staying in the correct lane, not speeding, etc. There are also specific rules for certain scenarios, such as alternate right-of-way and road diversions.

A suitable generalization paradigm has yet to be established in the literature, since this attribute primarily appears in the urban navigation task and most studies approach it indirectly through multi-modal inputs and expert data labeling. The common reward functions mentioned above from both actual attributes and shaping rewards are listed in Table III. 2) Reward Utilization Most RL related studies use weighted summation to combine different rewards.</div><div class="col-trans" id="trans-94">d) äº¤é€šåˆè§„æ€§ï¼šäº¤é€šè§„åˆ™ä»£è¡¨äº†ä¸€å¥—é«˜åº¦å¤æ‚çš„æŒ‡å¯¼æ–¹é’ˆï¼Œæ¶µç›–äº†å¤šä¸ªè¯­ä¹‰å±‚æ¬¡çš„ç†è§£å’Œè¯„ä¼°<a href="#ref-128" class="ref-link">[128]</a>ã€‚å¸¸è§çš„äº¤é€šè§„åˆ™éµå®ˆè¡Œä¸ºåŒ…æ‹¬éµå¾ªäº¤é€šä¿¡å·ã€ä¿æŒæ­£ç¡®çš„è½¦é“ã€ä¸è¶…é€Ÿç­‰ã€‚è¿˜æœ‰ä¸€äº›ç‰¹å®šåœºæ™¯ä¸‹çš„ç‰¹æ®Šè§„åˆ™ï¼Œä¾‹å¦‚ä¼˜å…ˆé€šè¡Œæƒå’Œé“è·¯æ”¹é“ã€‚

ç”±äºè¿™ä¸€å±æ€§ä¸»è¦å‡ºç°åœ¨åŸå¸‚å¯¼èˆªä»»åŠ¡ä¸­ï¼Œæ–‡çŒ®ä¸­å°šæœªå»ºç«‹åˆé€‚çš„æ³›åŒ–èŒƒå¼ï¼Œå¤§å¤šæ•°ç ”ç©¶é€šè¿‡å¤šæ¨¡æ€è¾“å…¥å’Œä¸“å®¶æ•°æ®æ ‡æ³¨é—´æ¥å¤„ç†å®ƒã€‚ä¸Šè¿°ä»å®é™…å±æ€§å’Œå¡‘é€ å¥–åŠ±ä¸­æåˆ°çš„å¸¸è§å¥–åŠ±å‡½æ•°åˆ—äº<a href="#Table_3" class="tab-link" onclick="highlightAsset('Table_3'); return false;">è¡¨3</a>ä¸­ã€‚  
2) å¥–åŠ±åˆ©ç”¨ï¼šå¤§å¤šæ•°ä¸å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„ç ”ç©¶ä½¿ç”¨åŠ æƒæ±‚å’Œçš„æ–¹æ³•æ¥ç»“åˆä¸åŒçš„å¥–åŠ±ã€‚<div id="badge-94" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 94)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('94', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-95"><div class="row text-row"><div class="col-src">Knox et al. [127] discusses the calculation of weight factor limits, using crash, idle and success attributes as examples. However, human-manual weighting does not effectively harmonize the trade-offs and conflicts between multiple objectives. Parameter tuning methods such as GLIS [129] could be used to optimize the weight coefficient, which is an optional means.

In addition, Inverse Reinforcement Learning methods are applied to learn the weight of each attribute [130] or the reward value [131] from expert experience. Nevertheless, weighted tuning has a limited impact on improving achievable performance, and agents can still be skewed toward larger single-attribute rewards. The recent Multi-Critic approach excels in accommodating multiple objectives simultaneously [132].

Yuan et al. [133] decompose the value estimation based on a single reward function into decentralized estimation based on multiple reward functions through multiple Q-networks, which allows agents to better</div><div class="col-trans" id="trans-95">Knoxç­‰äºº<a href="#ref-127" class="ref-link">[127]</a>è®¨è®ºäº†æƒé‡å› å­é™åˆ¶çš„è®¡ç®—æ–¹æ³•ï¼Œä»¥ç¢°æ’ã€ç©ºé—²å’ŒæˆåŠŸå±æ€§ä¸ºä¾‹ã€‚ç„¶è€Œï¼Œäººå·¥æ‰‹åŠ¨åŠ æƒå¹¶ä¸èƒ½æœ‰æ•ˆåœ°åè°ƒå¤šä¸ªç›®æ ‡ä¹‹é—´çš„æƒè¡¡ä¸å†²çªã€‚å¯ä»¥ä½¿ç”¨GLISç­‰å‚æ•°è°ƒæ•´æ–¹æ³•<a href="#ref-129" class="ref-link">[129]</a>æ¥ä¼˜åŒ–æƒé‡ç³»æ•°ï¼Œè¿™æ˜¯ä¸€ç§å¯é€‰çš„æ–¹æ³•ã€‚

æ­¤å¤–ï¼Œé€†å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¢«åº”ç”¨äºä»ä¸“å®¶ç»éªŒä¸­å­¦ä¹ æ¯ä¸ªå±æ€§çš„æƒé‡<a href="#ref-130" class="ref-link">[130]</a>æˆ–å¥–åŠ±å€¼<a href="#ref-131" class="ref-link">[131]</a>ã€‚ä¸è¿‡ï¼ŒåŠ æƒè°ƒæ•´å¯¹æé«˜å¯å®ç°æ€§èƒ½çš„å½±å“æœ‰é™ï¼Œä»£ç†ä»ç„¶å¯èƒ½åå‘äºå•ä¸€å±æ€§è¾ƒå¤§çš„å¥–åŠ±ã€‚æœ€è¿‘çš„å¤šæ‰¹è¯„å®¶æ–¹æ³•åœ¨åŒæ—¶å¤„ç†å¤šä¸ªç›®æ ‡æ–¹é¢è¡¨ç°å‡ºè‰²<a href="#ref-132" class="ref-link">[132]</a>ã€‚

Yuanç­‰äºº<a href="#ref-133" class="ref-link">[133]</a>é€šè¿‡ä½¿ç”¨å¤šä¸ªQç½‘ç»œå°†åŸºäºå•ä¸ªå¥–åŠ±å‡½æ•°çš„ä»·å€¼ä¼°è®¡åˆ†è§£ä¸ºåŸºäºå¤šä¸ªå¥–åŠ±å‡½æ•°çš„åˆ†æ•£ä¼°è®¡ï¼Œä»è€Œä½¿ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°<div id="badge-95" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 95)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('95', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-96"><div class="row text-row"><div class="col-src">[[HEADER: 2) Reward Utilization]]</div><div class="col-trans" id="trans-96"><b>2ï¼‰å¥–åŠ±åˆ©ç”¨</b><div id="badge-96" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 96)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('96', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-97"><div class="row text-row"><div class="col-src">balance multiple learning objectives. Moreover, [134] incorporates the context as an input to construct a reward machine to transform the reward functions for different tasks/scenarios, enhancing the adaptability to environmental changes. 3) Reward Shaping When reward signals from objective attributes are sparse, it is a natural idea to encourage and indicate seemingly desirable maneuvers in reward functions, which is formalized as reward shaping [127].

For example, adding a reward for staying near the lane centerline can help a vehicle to quickly learn how to keep on track. However, combining this partially shaped rewards with existing safety rewards may lead the RL agent unexpectedly fall into a local optimum, such as persistently following SVs at a low speed, which is not actually the desired driving behavior.</div><div class="col-trans" id="trans-97">å¹³è¡¡å¤šä¸ªå­¦ä¹ ç›®æ ‡ã€‚æ­¤å¤–ï¼Œ<a href="#ref-134" class="ref-link">[134]</a> å°†ä¸Šä¸‹æ–‡ä½œä¸ºè¾“å…¥ä»¥æ„å»ºå¥–åŠ±æœºå™¨ï¼Œç”¨äºè½¬æ¢ä¸åŒä»»åŠ¡/åœºæ™¯ä¸‹çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œå¢å¼ºå¯¹ç¯å¢ƒå˜åŒ–çš„é€‚åº”æ€§ã€‚3) å¥–åŠ±å¡‘å½¢ å½“æ¥è‡ªç›®æ ‡å±æ€§çš„å¥–åŠ±ä¿¡å·ç¨€ç–æ—¶ï¼Œåœ¨å¥–åŠ±å‡½æ•°ä¸­é¼“åŠ±å’ŒæŒ‡ç¤ºçœ‹ä¼¼æœ‰åˆ©çš„è¡Œä¸ºæ˜¯ä¸€ç§è‡ªç„¶çš„æƒ³æ³•ï¼Œè¿™è¢«ç§°ä¸ºå¥–åŠ±å¡‘å½¢ <a href="#ref-127" class="ref-link">[127]</a>ã€‚

ä¾‹å¦‚ï¼Œå¢åŠ é è¿‘è½¦é“ä¸­å¿ƒçº¿çš„å¥–åŠ±å¯ä»¥å¸®åŠ©è½¦è¾†å¿«é€Ÿå­¦ä¼šä¿æŒåœ¨æ­£ç¡®çš„è¡Œé©¶è½¨è¿¹ä¸Šã€‚ç„¶è€Œï¼Œå°†è¿™ç§éƒ¨åˆ†å¡‘å½¢çš„å¥–åŠ±ä¸ç°æœ‰çš„å®‰å…¨å¥–åŠ±ç»“åˆä½¿ç”¨å¯èƒ½ä¼šå¯¼è‡´ RL ä»£ç†æ„å¤–åœ°é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œæ¯”å¦‚ä»¥è¾ƒä½çš„é€Ÿåº¦æŒç»­è·Ÿéšå…¶ä»–è½¦è¾†ï¼Œè€Œè¿™å¹¶ä¸æ˜¯æœŸæœ›çš„é©¾é©¶è¡Œä¸ºã€‚<div id="badge-97" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 97)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('97', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-98"><div class="row text-row"><div class="col-src">Common shaped rewards via one or more attributes include suggesting zero steering angle [105], increasing the separation distance with SVs [12], overtaking other vehicles [76], etc. While reward shaping improves the learning efficiency, it may reduce the achievable performance by subjectively changing the preference order of the reward function.

As Russell and Norvig assert [135], â€œIt is better to design performance metrics according to what one actually wants to be achieved in the environment, rather than according to how one thinks the agent should behaveâ€. The survey [127] boils it down to a pithy description: â€œspecify how to measure outcomes, not how to achieve them.â€ Despite its theoretical drawbacks, reward shaping remains effective in RL-based MoP methods as of this time.</div><div class="col-trans" id="trans-98">é€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå±æ€§æä¾›çš„å¸¸è§å½¢çŠ¶å¥–åŠ±åŒ…æ‹¬å»ºè®®é›¶è½¬å‘è§’[[LINK: <a href="#ref-105" class="ref-link">[105]</a>|<a href="#ref-105" class="ref-link">[105]</a>]]ã€å¢åŠ ä¸SVsçš„åˆ†ç¦»è·ç¦»[[LINK: <a href="#ref-12" class="ref-link">[12]</a>|<a href="#ref-12" class="ref-link">[12]</a>]]ã€è¶…è¶Šå…¶ä»–è½¦è¾†[[LINK: <a href="#ref-76" class="ref-link">[76]</a>|<a href="#ref-76" class="ref-link">[76]</a>]]ç­‰ã€‚è™½ç„¶å¥–åŠ±å¡‘é€ å¯ä»¥æé«˜å­¦ä¹ æ•ˆç‡ï¼Œä½†å®ƒå¯èƒ½ä¼šé€šè¿‡ä¸»è§‚æ”¹å˜å¥–åŠ±å‡½æ•°çš„åå¥½é¡ºåºè€Œé™ä½å¯å®ç°æ€§èƒ½ã€‚

æ­£å¦‚Russellå’ŒNorvigæ‰€ä¸»å¼ çš„[[LINK: <a href="#ref-135" class="ref-link">[135]</a>|<a href="#ref-135" class="ref-link">[135]</a>]]ï¼šâ€œåº”æ ¹æ®å®é™…å¸Œæœ›åœ¨ç¯å¢ƒä¸­å®ç°çš„ç›®æ ‡æ¥è®¾è®¡æ€§èƒ½æŒ‡æ ‡ï¼Œè€Œä¸æ˜¯æ ¹æ®è®¤ä¸ºä»£ç†åº”è¯¥å¦‚ä½•è¡Œä¸ºæ¥è¿›è¡Œè®¾è®¡â€ã€‚ç»¼è¿°[[LINK: <a href="#ref-127" class="ref-link">[127]</a>|<a href="#ref-127" class="ref-link">[127]</a>]]å°†å…¶ç²¾ç‚¼åœ°æè¿°ä¸ºï¼šâ€œè§„å®šå¦‚ä½•è¡¡é‡ç»“æœï¼Œè€Œä¸æ˜¯å¦‚ä½•å®ç°å®ƒä»¬ã€‚â€å°½ç®¡å­˜åœ¨ç†è®ºä¸Šçš„ç¼ºé™·ï¼Œä½†å¥–åŠ±å¡‘é€ ä»ç„¶åœ¨åŸºäºRLçš„æ–¹æ³•ä¸­æœ‰æ•ˆã€‚<div id="badge-98" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 98)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('98', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-99"><div class="row text-row"><div class="col-src">Until a better learning way emerge, reward shaping techniques, such as risk-aware shaping for safety [136] or directly traffic rule guidance, can enhancing driving performance to a certain extent. However, the potential negative consequences of each shaping operation need to be carefully considered. Designing effective reward functions remains an open problem, limiting the RL performance in MoP for AD as well as in other control tasks.</div><div class="col-trans" id="trans-99">ç›´åˆ°å‡ºç°æ›´å¥½çš„å­¦ä¹ æ–¹æ³•ä¹‹å‰ï¼Œè¯¸å¦‚é£é™©æ„è¯†å‹å¡‘é€ ä»¥ç¡®ä¿å®‰å…¨[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]] æˆ–ç›´æ¥äº¤é€šè§„åˆ™æŒ‡å¯¼ç­‰å¥–åŠ±å¡‘å½¢æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæå‡é©¾é©¶æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ¯ä¸ªå¡‘å½¢æ“ä½œæ½œåœ¨çš„è´Ÿé¢åæœéœ€è¦ä»”ç»†è€ƒè™‘ã€‚å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼Œè¿™é™åˆ¶äº†åœ¨ADåŠå…¶ä»–æ§åˆ¶ä»»åŠ¡ä¸­åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¡¨ç°ã€‚<div id="badge-99" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 99)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('99', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-100"><div class="row text-row"><div class="col-src">[[HEADER: g p 3) Reward Shaping]]</div><div class="col-trans" id="trans-100"><b>g p 3) å¥–åŠ±å¡‘å½¢</b><div id="badge-100" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 100)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('100', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-101"><div class="row text-row"><div class="col-src">[[HEADER: V. EXPLORATORY EFFORTS TO ADDRESS CONTEMPORARY CHALLENGES]]</div><div class="col-trans" id="trans-101"><b>V. æ¢ç´¢æ€§åŠªåŠ›ä»¥åº”å¯¹å½“ä»£æŒ‘æˆ˜</b><div id="badge-101" class="hint-badge has-hint">ğŸ’¡ ä¸Šæ¬¡æç¤º: é‡æ–°ç¿»è¯‘ </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 101)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º...">é‡æ–°ç¿»è¯‘</textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('101', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-102"><div class="row text-row"><div class="col-src">Although there have been many significant achievements in RL-based MoP, there are still many challenges in applying it to real-world AD systems. Owing to page limitations, we focus on three attributes that have the greatest impact on RL-based MoP for AD, i.e., safety performance, sample efficiency, and generalization capability. Other attributes, such as interpretability and ethics, are not discussed in this survey, and interested readers are referred to [15], [17], which address such topics.

This section reviews recent exploratory efforts for these three frontier issues and proposes directions for future research. Since promoting sample efficiency and generalization capability share some common technical aspects, we distinguish them according to the primary motivation for using these techniques to enhance the performance of RL-based MoP.</div><div class="col-trans" id="trans-102">å°½ç®¡åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMoPï¼‰æ–¹é¢å–å¾—äº†è®¸å¤šé‡è¦æˆæœï¼Œä½†åœ¨å°†å…¶åº”ç”¨äºå®é™…çš„è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç³»ç»Ÿæ—¶ä»ç„¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæœ¬æ–‡ä¸»è¦å…³æ³¨å¯¹åŸºäºRLçš„MoPå½±å“æœ€å¤§çš„ä¸‰ä¸ªå±æ€§ï¼Œå³å®‰å…¨æ€§ã€æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

æœ¬èŠ‚å›é¡¾äº†é’ˆå¯¹è¿™ä¸‰ä¸ªå‰æ²¿é—®é¢˜çš„è¿‘æœŸæ¢ç´¢æ€§åŠªåŠ›ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚ç”±äºæé«˜æ ·æœ¬æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›åœ¨æŸäº›æŠ€æœ¯æ–¹é¢å­˜åœ¨å…±é€šä¹‹å¤„ï¼Œæˆ‘ä»¬æ ¹æ®è¿™äº›æŠ€æœ¯çš„ä¸»è¦åŠ¨æœºæ¥åŒºåˆ†å®ƒä»¬ï¼Œä»¥å¢å¼ºåŸºäºRLçš„MoPæ€§èƒ½ã€‚

å…¶ä»–å±æ€§å¦‚å¯è§£é‡Šæ€§å’Œä¼¦ç†é—®é¢˜æœªåœ¨æ­¤è°ƒæŸ¥ä¸­è®¨è®ºï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒæ–‡çŒ®<a href="#ref-15" class="ref-link">[15]</a>ã€<a href="#ref-17" class="ref-link">[17]</a>ï¼Œå…¶ä¸­è¯¦ç»†æ¢è®¨äº†è¿™äº›é—®é¢˜ã€‚<div id="badge-102" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 102)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('102', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-103"><div class="row text-row"><div class="col-src">[[HEADER: A. Safety Performance]]</div><div class="col-trans" id="trans-103">A. å®‰å…¨æ€§èƒ½<div id="badge-103" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 103)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('103', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-104"><div class="row text-row"><div class="col-src">Safety is a fundamental requirement for AD. However, the RL agent may sometimes prioritize maximizing the overall reward over ensuring safety, especially under conditions where multiple objectives are considered. This can lead to unsafe or even disastrous behaviors, which is the most important hindrance to the application of RL to real-world AD [65].

Consequently, an increasing number of researchers have focused on the safety of RL-based MoP methods and have begun to explore the application of Safe RL. Safe RL is often modeled as the Constraint MDP (CMDP) [137], which additionally minimizes safety-related cost CÏ€(s) = E[PH+h t=h Î³tâˆ’hct+1|sh = s] while maximizing cumulative reward expectations, where ct is the safety-related cost value at timestep t.</div><div class="col-trans" id="trans-104">å®‰å…¨æ€§æ˜¯ADçš„åŸºæœ¬è¦æ±‚ã€‚ç„¶è€Œï¼ŒRLä»£ç†æœ‰æ—¶å¯èƒ½ä¼šä¼˜å…ˆè€ƒè™‘æœ€å¤§åŒ–æ€»ä½“å¥–åŠ±è€Œä¸æ˜¯ç¡®ä¿å®‰å…¨ï¼Œå°¤å…¶æ˜¯åœ¨åŒæ—¶è€ƒè™‘å¤šä¸ªç›®æ ‡çš„æƒ…å†µä¸‹ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸å®‰å…¨ç”šè‡³ç¾éš¾æ€§çš„è¡Œä¸ºï¼Œè¿™æ˜¯å°†RLåº”ç”¨äºå®é™…ADåº”ç”¨çš„æœ€å¤§éšœç¢ä¹‹ä¸€<a href="#ref-65" class="ref-link">[65]</a>ã€‚

å› æ­¤ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹å…³æ³¨åŸºäºRLçš„æ–¹æ³•çš„å®‰å…¨æ€§ï¼Œå¹¶å¼€å§‹æ¢ç´¢Safe RLçš„åº”ç”¨ã€‚Safe RLé€šå¸¸è¢«å»ºæ¨¡ä¸ºçº¦æŸMDPï¼ˆCMDPï¼‰<a href="#ref-137" class="ref-link">[137]</a>ï¼Œå®ƒåœ¨æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±æœŸæœ›çš„åŒæ—¶æœ€å°åŒ–ä¸å®‰å…¨ç›¸å…³çš„æˆæœ¬CÏ€(s) = E[PH+h t=h Î³tâˆ’hct+1|sh = s]ï¼Œå…¶ä¸­ctæ˜¯åœ¨æ—¶é—´æ­¥tæ—¶çš„å®‰å…¨ç›¸å…³æˆæœ¬å€¼ã€‚<div id="badge-104" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 104)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('104', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-105"><div class="row text-row"><div class="col-src">The objective of CMDP is to find a policy Ï€Î¸ âˆˆÎ C to maximize the expected reward, where Î C = {Ï€Î¸|CÏ€(s) â‰¤Cthres} represents the safe policy set with a cost threshold Cthres. Safe RL applied in the MoP can usually be categorized as: i) Policy objective optimization: This method uses the cumulative cost values on the trajectories to search for safe policies, gradually converging to safe set.

ii) Hard safety constraint: Stricter requirements on the safety of each step are imposed during training or testing through predefined constraints. This type of approach can further enhance safety, but is more conservative. 1) Policy Objective Optimization Constrained Policy Optimization (CPO) is frequently used to guide the generation of a safer driving policy [138].</div><div class="col-trans" id="trans-105">CMDPçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€Î¸ âˆˆ Î Cï¼Œä»¥æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±ï¼Œå…¶ä¸­Î C = {Ï€Î¸|CÏ€(s) â‰¤ Cthres}è¡¨ç¤ºå…·æœ‰æˆæœ¬é˜ˆå€¼Cthresçš„å®‰å…¨ç­–ç•¥é›†ã€‚åœ¨MoPä¸­åº”ç”¨çš„Safe RLé€šå¸¸å¯ä»¥å½’ç±»ä¸ºä»¥ä¸‹ä¸¤ç§ç±»å‹ï¼šiï¼‰ç­–ç•¥ç›®æ ‡ä¼˜åŒ–ï¼šè¿™ç§æ–¹æ³•åˆ©ç”¨è½¨è¿¹ä¸Šçš„ç´¯ç§¯æˆæœ¬å€¼æ¥æœç´¢å®‰å…¨ç­–ç•¥ï¼Œå¹¶é€æ­¥æ”¶æ•›åˆ°å®‰å…¨é›†åˆã€‚

iiï¼‰ç¡¬æ€§å®‰å…¨çº¦æŸï¼šé€šè¿‡é¢„å®šä¹‰çš„çº¦æŸï¼Œåœ¨è®­ç»ƒæˆ–æµ‹è¯•è¿‡ç¨‹ä¸­å¯¹æ¯ä¸€æ­¥çš„å®‰å…¨æ€§æå‡ºæ›´ä¸¥æ ¼çš„è¦æ±‚ã€‚è¿™ç§ç±»å‹çš„æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºå®‰å…¨æ€§ï¼Œä½†æ›´ä¸ºä¿å®ˆã€‚1) ç­–ç•¥ç›®æ ‡ä¼˜åŒ–

å—é™ç­–ç•¥ä¼˜åŒ–ï¼ˆCPOï¼‰ç»å¸¸è¢«ç”¨æ¥å¼•å¯¼ç”Ÿæˆä¸€ä¸ªæ›´å®‰å…¨çš„é©¾é©¶ç­–ç•¥ <a href="#ref-138" class="ref-link">[138]</a>ã€‚<div id="badge-105" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 105)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('105', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-106"><div class="row text-row"><div class="col-src">Wen et al. [139] employed parallel CPO agents to collect sufficient safe and feasible experiences for policy updates, mitigating the driving risks of CPO failures in hazardous situations. Additionally, Lagrangian-based methods transform constrained safety optimization problems into unconstrained problems via Lagrange multipliers. In [64], a Lagrangian network adaptively adjusted penalties for constraint violations, while a feasible value network evaluates policy feasibility.

Furthermore, inspired by the amygdala mechanism, Lv et al. [96] employ a fear model to recognize potential dangers and contingencies, aiming to maximize the expected return while adhering to the fear constraint. Many researchers integrate Control Lyapunov Functions (CLFs) [140] or Control Barrier Functions (CBFs) [141] as constraints.</div><div class="col-trans" id="trans-106">Wenç­‰<a href="#ref-139" class="ref-link">[139]</a>åˆ©ç”¨å¹¶è¡ŒCPOä»£ç†æ”¶é›†è¶³å¤Ÿçš„å®‰å…¨ä¸”å¯è¡Œçš„ç»éªŒï¼Œä»¥å‡è½»CPOæ•…éšœåœ¨å±é™©æƒ…å†µä¸‹çš„é©¾é©¶é£é™©ã€‚æ­¤å¤–ï¼ŒåŸºäºæ‹‰æ ¼æœ—æ—¥çš„æ–¹æ³•é€šè¿‡æ‹‰æ ¼rangeä¹˜å­å°†å—çº¦æŸçš„å®‰å…¨ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæ— çº¦æŸé—®é¢˜ã€‚åœ¨<a href="#ref-64" class="ref-link">[64]</a>ä¸­ï¼Œä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ç½‘ç»œè‡ªé€‚åº”åœ°è°ƒæ•´äº†å¯¹çº¦æŸè¿åçš„æƒ©ç½šåŠ›åº¦ï¼Œè€Œä¸€ä¸ªå¯è¡Œå€¼ç½‘ç»œè¯„ä¼°ç­–ç•¥çš„å¯è¡Œæ€§ã€‚

æ­¤å¤–ï¼Œå—åˆ°æä»æ ¸æœºåˆ¶çš„å¯å‘ï¼ŒLvç­‰<a href="#ref-96" class="ref-link">[96]</a>é‡‡ç”¨ææƒ§æ¨¡å‹æ¥è¯†åˆ«æ½œåœ¨å±é™©å’Œåæœï¼Œæ—¨åœ¨æœ€å¤§åŒ–é¢„æœŸå›æŠ¥çš„åŒæ—¶éµå®ˆææƒ§çº¦æŸã€‚è®¸å¤šç ”ç©¶è€…å°†æ§åˆ¶æé›…æ™®è¯ºå¤«å‡½æ•°ï¼ˆCLFsï¼‰<a href="#ref-140" class="ref-link">[140]</a>æˆ–æ§åˆ¶éšœç¢å‡½æ•°ï¼ˆCBFsï¼‰<a href="#ref-141" class="ref-link">[141]</a>ä½œä¸ºçº¦æŸæ¡ä»¶é›†æˆè¿›æ¥ã€‚<div id="badge-106" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 106)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('106', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-107"><div class="row text-row"><div class="col-src">In [142], a CLF based on the relative distance to obstacles is established, treating the collision probability as a risk factor in the critic with the policy gradient to improve safety. Udatha et al. [143] implement a distancebased probabilistic CBF, which is then converted into linear control constraints to ensure that policy updates adhere to safety requirements.

Moreover, Yang et al. [144] take this further by learning a barrier function from collected unsafe and initial states, eliminating the need for prior knowledge. Meanwhile, some works consider uncertainty in safety guiding the agentâ€™s exploration. In [145], RL policy is updated only when its performance confidence exceeds the baseline, achieving safer behaviors.</div><div class="col-trans" id="trans-107">åœ¨<a href="#ref-142" class="ref-link">[142]</a>ä¸­ï¼ŒåŸºäºç›¸å¯¹è·ç¦»åˆ°éšœç¢ç‰©çš„ä¸´ç•Œå‡½æ•°ï¼ˆCLFï¼‰è¢«å»ºç«‹èµ·æ¥ï¼Œå¹¶å°†ç¢°æ’æ¦‚ç‡è§†ä¸ºè¯„è®ºè€…ä¸­çš„ä¸€ä¸ªé£é™©å› ç´ ï¼Œé€šè¿‡ç­–ç•¥æ¢¯åº¦æ¥æé«˜å®‰å…¨æ€§ã€‚Udathaç­‰äºº<a href="#ref-143" class="ref-link">[143]</a>å®ç°äº†ä¸€ä¸ªåŸºäºè·ç¦»çš„æ¦‚ç‡CBFï¼ˆCollision-Based Feasibility Barrierï¼‰ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºçº¿æ€§æ§åˆ¶çº¦æŸä»¥ç¡®ä¿ç­–ç•¥æ›´æ–°ç¬¦åˆå®‰å…¨è¦æ±‚ã€‚

æ­¤å¤–ï¼ŒYangç­‰äºº<a href="#ref-144" class="ref-link">[144]</a>åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥å­¦ä¹ ä¸€ä¸ªéšœç¢å‡½æ•°ï¼Œä»æ”¶é›†åˆ°çš„ä¸å®‰å…¨çŠ¶æ€å’Œåˆå§‹çŠ¶æ€ä¸­è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œæ¶ˆé™¤å¯¹å…ˆéªŒçŸ¥è¯†çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œä¸€äº›ç ”ç©¶å·¥ä½œè€ƒè™‘äº†åœ¨å®‰å…¨æ€§å¼•å¯¼ä¸‹çš„ä»£ç†æ¢ç´¢ä¸ç¡®å®šæ€§ã€‚åœ¨<a href="#ref-145" class="ref-link">[145]</a>ä¸­ï¼Œä»…å½“å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è¡¨ç°ç½®ä¿¡åº¦è¶…è¿‡åŸºçº¿æ—¶æ‰æ›´æ–°ç­–ç•¥ï¼Œä»è€Œå®ç°æ›´å®‰å…¨çš„è¡Œä¸ºã€‚<div id="badge-107" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 107)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('107', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-108"><div class="row text-row"><div class="col-src">Zhang et al. [146] use variance from ensemble critic networks to encourage exploration and to determine when to switch from a Lagrangian-based approach to a rule-based approach. In [147], CVaR-based distributional critics facilitate the safety policy update, with the policy space adaptively expanding when actions near the boundary are identified as safe. 2) Hard Safety Constraint</div><div class="col-trans" id="trans-108">å¼ ç­‰.<a href="#ref-146" class="ref-link">[146]</a> ä½¿ç”¨é›†æˆæ‰¹è¯„ç½‘ç»œçš„æ–¹å·®æ¥é¼“åŠ±æ¢ç´¢ï¼Œå¹¶ç¡®å®šä½•æ—¶ä»åŸºäºæ‹‰æ ¼æœ—æ—¥çš„æ–¹æ³•åˆ‡æ¢åˆ°åŸºäºè§„åˆ™çš„æ–¹æ³•ã€‚åœ¨<a href="#ref-147" class="ref-link">[147]</a>ä¸­ï¼ŒåŸºäºCVaRçš„åˆ†å¸ƒæ‰¹è¯„è€…ä¿ƒè¿›äº†å®‰å…¨ç­–ç•¥çš„æ›´æ–°ï¼Œåœ¨è¯†åˆ«å‡ºæ¥è¿‘è¾¹ç•Œçš„åŠ¨ä½œæ˜¯å®‰å…¨çš„æƒ…å†µä¸‹ï¼Œæ”¿ç­–ç©ºé—´ä¼šè‡ªé€‚åº”åœ°æ‰©å±•ã€‚2) ç¡¬æ€§å®‰å…¨çº¦æŸ<div id="badge-108" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 108)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('108', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-109"><div class="row text-row"><div class="col-src">[[HEADER: 1) Policy Objective Optimization]]</div><div class="col-trans" id="trans-109"><b>1) æ”¿ç­–ç›®æ ‡ä¼˜åŒ–</b><div id="badge-109" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 109)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('109', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-110"><div class="row text-row"><div class="col-src">[[HEADER: i 2) Hard Safety Constraint]]</div><div class="col-trans" id="trans-110"><b>ii 2) ç¡¬å®‰å…¨çº¦æŸ</b><div id="badge-110" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 110)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('110', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-111"><div class="row text-row"><div class="col-src">Setting driving rules or rule-based MoP as a safety filter is an intuitive way to enhance the policyâ€™s safety [148]. Gu et al. [149] propose a method that combines traditional MoP method with RL, where the safety buffers around obstacles constrain the RL output to collision-free path points. Wang et al. [150] develop a CBF that account for both longitudinal and lateral constraints, combined with predefined traffic rules, to ensure EV safety.

Some filters are constructed based on conditional criteria. Reference [151] uses Linear Temporal Logic (LTL) based on prior safety rules to assess the current policyâ€™s safety, triggering a rule-based emergency response if the RL action is deemed unsafe. References [118] and [125] employ MPC-based longitudinal pre-planning to assess whether a safe and feasible acceleration can be generated.</div><div class="col-trans" id="trans-111">å°†é©¾é©¶è§„åˆ™æˆ–åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼ˆMoPï¼‰è®¾ç½®ä¸ºå®‰å…¨è¿‡æ»¤å™¨æ˜¯ä¸€ç§ç›´è§‚çš„æ–¹å¼ï¼Œä»¥å¢å¼ºç­–ç•¥çš„å®‰å…¨æ€§<a href="#ref-148" class="ref-link">[148]</a>ã€‚Guç­‰äºº<a href="#ref-149" class="ref-link">[149]</a>æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¼ ç»Ÿçš„MoPæ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå…¶ä¸­éšœç¢ç‰©å‘¨å›´çš„ç¼“å†²åŒºçº¦æŸäº†RLçš„è¾“å‡ºï¼Œä½¿å…¶ä»…äº§ç”Ÿæ— ç¢°æ’è·¯å¾„ç‚¹ã€‚Wangç­‰äºº<a href="#ref-150" class="ref-link">[150]</a>å¼€å‘äº†ä¸€ç§è€ƒè™‘çºµå‘å’Œæ¨ªå‘çº¦æŸæ¡ä»¶çš„CBFï¼Œå¹¶ç»“åˆé¢„å®šä¹‰çš„äº¤é€šè§„åˆ™ä»¥ç¡®ä¿ç”µåŠ¨æ±½è½¦çš„å®‰å…¨ã€‚

ä¸€äº›è¿‡æ»¤å™¨æ˜¯åŸºäºæ¡ä»¶æ ‡å‡†æ„å»ºçš„ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-151" class="ref-link">[151]</a>ä½¿ç”¨åŸºäºå…ˆéªŒå®‰å…¨è§„åˆ™çš„çº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰æ¥è¯„ä¼°å½“å‰ç­–ç•¥çš„å®‰å…¨æ€§ï¼Œå¦‚æœè®¤ä¸ºRLåŠ¨ä½œä¸å®‰å…¨ï¼Œåˆ™è§¦å‘åŸºäºè§„åˆ™çš„ç´§æ€¥å“åº”ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-118" class="ref-link">[118]</a>å’Œ<a href="#ref-125" class="ref-link">[125]</a>é‡‡ç”¨åŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰çš„çºµå‘é¢„è§„åˆ’æ¥è¯„ä¼°æ˜¯å¦å¯ä»¥ç”Ÿæˆä¸€ä¸ªå®‰å…¨å¯è¡Œçš„åŠ é€Ÿåº¦ã€‚<div id="badge-111" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 111)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('111', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-112"><div class="row text-row"><div class="col-src">If unsafe, the vehicle remains in its original lane, and the masked unsafe decision is fed back to update the DQNâ€™s network. Uncertainty can also be used in constraint design for safe RL, which typically includes aleatoric uncertainty and epistemic uncertainty [42]. Aleatoric uncertainty can be expressed as the risk from the scenario measured by the distribution of returns.

It allows the RL agent to balance risk and efficiency after convergence and achieve performance similar to those trained in a risk-sensitive way [152]. Epistemic uncertainty usually arises from the insufficient scene training and can be represented by the variance of the ensemble network. In [153], the RL policy reverts to a rule-based policy if the uncertainty evaluated exceeds a safety threshold. Predicted information can be leveraged to ensure safety over a long horizon.</div><div class="col-trans" id="trans-112">å¦‚æœæ£€æµ‹åˆ°ä¸å®‰å…¨æƒ…å†µï¼Œè½¦è¾†å°†ä¿æŒåœ¨åŸè½¦é“ä¸å˜ï¼Œå¹¶å°†éšè—çš„ä¸å®‰å…¨å†³ç­–åé¦ˆä»¥æ›´æ–°DQNçš„ç½‘ç»œã€‚ä¸ç¡®å®šæ€§ä¹Ÿå¯ä»¥ç”¨äºå®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆSafe RLï¼‰çš„çº¦æŸè®¾è®¡ä¸­ï¼Œé€šå¸¸åŒ…æ‹¬ aleatoric ä¸ç¡®å®šæ€§å’Œ epistemic ä¸ç¡®å®šæ€§[<a href="#ref-42" class="ref-link">[42]</a>]ã€‚aleatoric ä¸ç¡®å®šæ€§å¯ä»¥è¡¨ç¤ºä¸ºç”±å›æŠ¥åˆ†å¸ƒè¡¡é‡çš„é£é™©ã€‚

è¿™ä½¿å¾—åœ¨æ”¶æ•›åï¼ŒRL ä»£ç†èƒ½å¤Ÿå¹³è¡¡é£é™©ä¸æ•ˆç‡ï¼Œå¹¶å®ç°ç±»ä¼¼äºé£é™©æ•æ„Ÿè®­ç»ƒæ–¹å¼ä¸‹çš„æ€§èƒ½[<a href="#ref-152" class="ref-link">[152]</a>]ã€‚epistemic ä¸ç¡®å®šæ€§é€šå¸¸æºäºåœºæ™¯è®­ç»ƒä¸è¶³ï¼Œå¯ä»¥é€šè¿‡é›†æˆç½‘ç»œçš„æ–¹å·®æ¥è¡¨ç¤ºã€‚åœ¨æ–‡çŒ®[<a href="#ref-153" class="ref-link">[153]</a>]ä¸­ï¼Œå¦‚æœè¯„ä¼°çš„ä¸ç¡®å®šæ€§è¶…è¿‡å®‰å…¨é˜ˆå€¼ï¼Œåˆ™ RL ç­–ç•¥å°†å›é€€åˆ°åŸºäºè§„åˆ™çš„ç­–ç•¥ã€‚é¢„æµ‹ä¿¡æ¯å¯ä»¥ç”¨æ¥ç¡®ä¿é•¿æ—¶é—´èŒƒå›´å†…çš„å®‰å…¨æ€§ã€‚<div id="badge-112" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 112)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('112', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-113"><div class="row text-row"><div class="col-src">In [12], actions are mapped to trajectories, and the risk of each action is assessed based on the EVâ€™s own trajectory and the predicted trajectories of SVs, with highrisk actions being discarded and replaced by safer alternatives. The effectiveness of this approach is validated in real-world lane-change experiments with different vehicle speeds and gaps, significantly reducing the risky behavior of the RL agent.

Moreover, Krasowski et al. [154] and Gu et al. [64] introduce the concept of a safe action set based on a prediction embedded framework, which is used to replace the actions of RL with safe alternatives in the event of a failure in the following vehicle strategy. Additionally, [155] and [156] construct optimization-based filters to guarantee that the agent remains safe at all times, while minimizing modifications to the RL policy.

A related work [157] uses MPC as a filter to ensure that the agent always stays within a safe invariant set.</div><div class="col-trans" id="trans-113">åœ¨æ–‡çŒ®<a href="#ref-12" class="ref-link">[12]</a>ä¸­ï¼ŒåŠ¨ä½œè¢«æ˜ å°„åˆ°è½¨è¿¹ä¸Šï¼Œå¹¶åŸºäºè‡ªä¸»è½¦è¾†ï¼ˆEVï¼‰è‡ªèº«çš„è½¨è¿¹åŠå…¶é¢„æµ‹çš„å…¶ä»–è½¦è¾†ï¼ˆSVsï¼‰çš„è½¨è¿¹æ¥è¯„ä¼°æ¯ç§åŠ¨ä½œçš„é£é™©ã€‚é«˜é£é™©çš„åŠ¨ä½œä¼šè¢«ä¸¢å¼ƒå¹¶æ›¿æ¢ä¸ºæ›´å®‰å…¨çš„é€‰æ‹©ã€‚è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡ä¸åŒè½¦é€Ÿå’Œè½¦é—´è·çš„çœŸå®é“è·¯å˜é“å®éªŒå¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾è‘—å‡å°‘äº†RLä»£ç†çš„å±é™©è¡Œä¸ºã€‚

æ­¤å¤–ï¼ŒKrasowskiç­‰äºº<a href="#ref-154" class="ref-link">[154]</a>å’ŒGuç­‰äºº<a href="#ref-64" class="ref-link">[64]</a>æå‡ºäº†åŸºäºé¢„æµ‹åµŒå…¥æ¡†æ¶çš„å®‰å…¨åŠ¨ä½œé›†çš„æ¦‚å¿µï¼Œåœ¨åæ–¹è½¦è¾†ç­–ç•¥å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å®‰å…¨æ›¿ä»£æ–¹æ¡ˆæ›¿æ¢RLçš„åŠ¨ä½œã€‚å¦å¤–ï¼Œæ–‡çŒ®<a href="#ref-155" class="ref-link">[155]</a>å’Œ<a href="#ref-156" class="ref-link">[156]</a>æ„å»ºäº†åŸºäºä¼˜åŒ–çš„æ»¤æ³¢å™¨ï¼Œä»¥ç¡®ä¿ä»£ç†åœ¨æ‰€æœ‰æ—¶é—´ç‚¹éƒ½ä¿æŒå®‰å…¨çŠ¶æ€ï¼Œå¹¶å°½é‡å‡å°‘å¯¹RLç­–ç•¥çš„ä¿®æ”¹ã€‚

ç›¸å…³å·¥ä½œ<a href="#ref-157" class="ref-link">[157]</a>ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰ä½œä¸ºæ»¤æ³¢å™¨ï¼Œä»¥ç¡®ä¿ä»£ç†å§‹ç»ˆå¤„äºä¸€ä¸ªå®‰å…¨ä¸å˜é›†å†…ã€‚<div id="badge-113" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 113)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('113', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-114"><div class="row text-row"><div class="col-src">[[HEADER: B. Sample Efficiency]]</div><div class="col-trans" id="trans-114"><b>B. æ ·æœ¬æ•ˆç‡</b><div id="badge-114" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 114)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('114', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-115"><div class="row text-row"><div class="col-src">Owing to the interactive update paradigm of RL, a substantial number of samples are usually required to construct learning experiences with feedback rewards to generate feasible policies, which leads to sample efficiency problem. This problem is particularly evident in the AD MoP field because of the open interaction environment with large state space and hardto-collect long-tail data, which results in slow driving policy convergence and lower-than-expected driving performance.

Since the complexity of interactions within the environment, it is challenging to objectively and effectively obtain reward signals in an AD task. In addition, many driving tasks exhibit temporal correlations, which can further amplify the effects of delayed rewards. In addition, the RL agent must spend a considerable amount of time on constant trial-and-error in the massive exploration space.</div><div class="col-trans" id="trans-115">ç”±äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„äº¤äº’å¼æ›´æ–°èŒƒå¼ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ ·æœ¬æ¥æ„å»ºå¸¦æœ‰åé¦ˆå¥–åŠ±çš„å­¦ä¹ ä½“éªŒä»¥ç”Ÿæˆå¯è¡Œç­–ç•¥ï¼Œè¿™å¯¼è‡´äº†æ ·æœ¬æ•ˆç‡é—®é¢˜ã€‚è¿™ä¸€é—®é¢˜åœ¨AD MoPé¢†åŸŸå°¤ä¸ºæ˜æ˜¾ï¼Œå› ä¸ºå­˜åœ¨ä¸€ä¸ªå¼€æ”¾çš„äº¤äº’ç¯å¢ƒå’Œå·¨å¤§çš„çŠ¶æ€ç©ºé—´ï¼Œå¹¶ä¸”éš¾ä»¥æ”¶é›†é•¿å°¾æ•°æ®ï¼Œä»è€Œå¯¼è‡´é©¾é©¶ç­–ç•¥æ”¶æ•›é€Ÿåº¦ç¼“æ…¢ä»¥åŠä½äºé¢„æœŸçš„é©¾é©¶æ€§èƒ½ã€‚

ç”±äºç¯å¢ƒä¸­çš„äº¤äº’å¤æ‚æ€§ï¼Œå®¢è§‚æœ‰æ•ˆåœ°è·å–å¥–åŠ±ä¿¡å·åœ¨ADä»»åŠ¡ä¸­æå…·æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè®¸å¤šé©¾é©¶ä»»åŠ¡å…·æœ‰æ—¶é—´ç›¸å…³æ€§ï¼Œè¿™ä¼šè¿›ä¸€æ­¥æ”¾å¤§å»¶è¿Ÿå¥–åŠ±çš„å½±å“ã€‚å¦å¤–ï¼ŒRLä»£ç†å¿…é¡»èŠ±è´¹å¤§é‡æ—¶é—´åœ¨åºå¤§çš„æ¢ç´¢ç©ºé—´ä¸­è¿›è¡Œä¸æ–­çš„å°è¯•ä¸é”™è¯¯ã€‚<div id="badge-115" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 115)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('115', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-116"><div class="row text-row"><div class="col-src">Besides, it is difficult to gain valuable experience to further improve driving policy performance in the late training stage. These factors contribute to the sample data cost. To address these challenges, researchers have aimed to enable the RL agent to learn more driving experience from limited samples, thus improving the overall performance of MoP, and accelerating its deployment and application in AD.

1) Learning from Demonstration (LfD) To facilitate faster learning of optimal driving by the RL agent, learning from demonstration (LfD) takes an inspiration from human learning styles. LfD can effectively handle initial exploration where the reward signal is too sparse or the exploration space is too large to be covered. A demonstration can usually be a priori rule models or expert data from human drivers or, alternatively, pre-trained policies.</div><div class="col-trans" id="trans-116">æ­¤å¤–ï¼Œåœ¨åæœŸè®­ç»ƒé˜¶æ®µï¼Œå¾ˆéš¾è·å¾—æœ‰ä»·å€¼çš„ç»éªŒä»¥è¿›ä¸€æ­¥æé«˜é©¾é©¶ç­–ç•¥çš„è¡¨ç°ã€‚è¿™äº›å› ç´ å¯¼è‡´äº†æ ·æœ¬æ•°æ®æˆæœ¬çš„å¢åŠ ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜è‡´åŠ›äºä½¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†èƒ½å¤Ÿä»æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ æ›´å¤šçš„é©¾é©¶ç»éªŒï¼Œä»è€Œæé«˜MoPçš„æ•´ä½“æ€§èƒ½ï¼Œå¹¶åŠ é€Ÿå…¶åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚

1) ç¤ºèŒƒå­¦ä¹ ï¼ˆLfDï¼‰ ä¸ºäº†ä¿ƒè¿›RLä»£ç†æ›´å¿«åœ°å­¦ä¹ æœ€ä¼˜é©¾é©¶æ–¹å¼ï¼Œç¤ºèŒƒå­¦ä¹ ï¼ˆLfDï¼‰å€Ÿé‰´äº†äººç±»çš„å­¦ä¹ æ¨¡å¼ã€‚LfDå¯ä»¥åœ¨åˆå§‹æ¢ç´¢é˜¶æ®µæœ‰æ•ˆå‘æŒ¥ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ä¿¡å·ç¨€ç–æˆ–æ¢ç´¢ç©ºé—´è¿‡å¤§éš¾ä»¥è¦†ç›–çš„æƒ…å†µä¸‹ã€‚ä¸€ä¸ªç¤ºèŒƒé€šå¸¸å¯ä»¥æ˜¯å…ˆéªŒè§„åˆ™æ¨¡å‹ã€æ¥è‡ªäººç±»é©¾é©¶å‘˜çš„ä¸“å®¶æ•°æ®ï¼Œæˆ–è€…é¢„å…ˆè®­ç»ƒå¥½çš„ç­–ç•¥ã€‚<div id="badge-116" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 116)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('116', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-117"><div class="row text-row"><div class="col-src">a) Learning from a rule-based planner: The RL agent can be simply and directly guided through rule-based policy demonstration. Alighanbari et al. [158] generates switchable policy through the NMPC controller, and the experience generated by NMPC is used to guide the DDPG to speed up learning. Zhang et al. [13] design an optimization-based trajectory planner to offer the possible motion state data of the EV according to different decision parameter values.

When recalled in RL, the related planning parameters are quickly obtained via NelderMean search method. In [159], an expert system consisting of constrained iterative LQR and PID controllers is incorporated into RL training to improve sample efficiency in autonomous overtaking tasks. Similarly, Li et al. [12] design a formalized rule-based correction mechanism considering predicted risks, where multi-memory batches are set to store expert guidance experiences to further improve sample efficiency.</div><div class="col-trans" id="trans-117">a) ä»åŸºäºè§„åˆ™çš„è§„åˆ’å™¨å­¦ä¹ ï¼šRLæ™ºèƒ½ä½“å¯ä»¥é€šè¿‡åŸºäºè§„åˆ™çš„ç­–ç•¥æ¼”ç¤ºç®€å•ç›´æ¥åœ°è¿›è¡Œå¼•å¯¼ã€‚Alighanbariç­‰äºº<a href="#ref-158" class="ref-link">[158]</a>é€šè¿‡NMPCæ§åˆ¶å™¨ç”Ÿæˆå¯åˆ‡æ¢çš„ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨NMPCäº§ç”Ÿçš„ç»éªŒæ¥æŒ‡å¯¼DDPGåŠ é€Ÿå­¦ä¹ ã€‚Zhangç­‰äºº<a href="#ref-13" class="ref-link">[13]</a>è®¾è®¡äº†ä¸€ç§åŸºäºä¼˜åŒ–çš„è½¨è¿¹è§„åˆ’å™¨ï¼Œæ ¹æ®ä¸åŒçš„å†³ç­–å‚æ•°å€¼æä¾›ç”µåŠ¨æ±½è½¦å¯èƒ½çš„è¿åŠ¨çŠ¶æ€æ•°æ®ã€‚

åœ¨RLä¸­å›å¿†æ—¶ï¼Œç›¸å…³è§„åˆ’å‚æ•°å¯é€šè¿‡Nelder-Meadæœç´¢æ–¹æ³•è¿…é€Ÿè·å¾—ã€‚åœ¨<a href="#ref-159" class="ref-link">[159]</a>ä¸­ï¼Œå°†ä¸€ä¸ªç”±çº¦æŸè¿­ä»£LQRæ§åˆ¶å™¨å’ŒPIDæ§åˆ¶å™¨ç»„æˆçš„ä¸“å®¶ç³»ç»Ÿæ•´åˆåˆ°RLè®­ç»ƒä¸­ï¼Œä»¥æé«˜è‡ªä¸»è¶…è½¦ä»»åŠ¡ä¸­çš„æ ·æœ¬æ•ˆç‡ã€‚ç±»ä¼¼åœ°ï¼ŒLiç­‰äºº<a href="#ref-12" class="ref-link">[12]</a>è®¾è®¡äº†ä¸€ç§è€ƒè™‘é¢„æµ‹é£é™©çš„æ­£å¼åŒ–è§„åˆ™ä¿®æ­£æœºåˆ¶ï¼Œåœ¨æ­¤æœºåˆ¶ä¸­è®¾ç½®äº†å¤šè®°å¿†æ‰¹å¤„ç†æ¥å­˜å‚¨ä¸“å®¶æŒ‡å¯¼ç»éªŒï¼Œè¿›ä¸€æ­¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚<div id="badge-117" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 117)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('117', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-118"><div class="row text-row"><div class="col-src">b) Learning from Human-Guidance: Combining human guidance with RL can be a promising way to alleviate the sample efficiency issue. A common tactic is to use demonstrations from human experts as a sampling experience for the RL agent. DQfD [160] incorporates expert demonstrations into the replay buffer with extra priority.

Liu et al. [161] combine the objectives of reward maximization and expert imitation, and then sample the experiences from both the agentâ€™s self-exploration and the human demonstrations with an adaptive dynamic sampling ratio. Gao et al. [162] propose a unified Normalized Actor-Critic, where soft policy gradient formulations are used to reduce the Q-values of actions that were not observed from the demonstrations, thereby mitigating learning bias from low-quality demonstrations.</div><div class="col-trans" id="trans-118">b) ä»äººç±»æŒ‡å¯¼ä¸­å­¦ä¹ ï¼šå°†äººç±»æŒ‡å¯¼ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆå¯èƒ½æ˜¯ä¸€ç§ç¼“è§£æ ·æœ¬æ•ˆç‡é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ä¸€ç§å¸¸è§çš„ç­–ç•¥æ˜¯ä½¿ç”¨äººç±»ä¸“å®¶çš„æ¼”ç¤ºä½œä¸ºRLä»£ç†çš„ç»éªŒé‡‡æ ·æ¥æºã€‚DQfD <a href="#ref-160" class="ref-link">[160]</a> å°†ä¸“å®¶æ¼”ç¤ºæ•´åˆåˆ°é‡æ”¾ç¼“å†²åŒºä¸­ï¼Œå¹¶é™„åŠ ä¼˜å…ˆçº§ã€‚

åˆ˜ç­‰ <a href="#ref-161" class="ref-link">[161]</a> ç»“åˆäº†å¥–åŠ±æœ€å¤§åŒ–å’Œä¸“å®¶æ¨¡ä»¿çš„ç›®æ ‡ï¼Œç„¶åæ ¹æ®è‡ªé€‚åº”åŠ¨æ€é‡‡æ ·æ¯”ä¾‹ä»ä»£ç†çš„è‡ªæˆ‘æ¢ç´¢ç»éªŒå’Œäººç±»æ¼”ç¤ºä¸­æŠ½å–ç»éªŒã€‚é«˜ç­‰ <a href="#ref-162" class="ref-link">[162]</a> æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å½’ä¸€åŒ–æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼Œåœ¨æ­¤æ–¹æ³•ä¸­ä½¿ç”¨è½¯ç­–ç•¥æ¢¯åº¦å…¬å¼æ¥é™ä½æœªåœ¨æ¼”ç¤ºä¸­è§‚å¯Ÿåˆ°çš„åŠ¨ä½œçš„ä»·å€¼ï¼Œä»è€Œå‡è½»ç”±äºä½è´¨é‡æ¼”ç¤ºè€Œå¯¼è‡´çš„å­¦ä¹ åå·®ã€‚<div id="badge-118" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 118)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('118', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-119"><div class="row text-row"><div class="col-src">In [125], human online interventions are triggered when an agent outputs unfavorable actions, which can limit unsafe exploration during training, and provide demonstrations in complex scenarios. Similarly, Wu et al. [163] establish an integrated framework including human/RL action switch mechanism, advantagebased prioritized experience, and human-intervention reward shaping. Its unique discriminatory ability for the quality of human guidance contributes to better learning performance.

c) Learning with a pre-trained policy: A near-optimal policy</div><div class="col-trans" id="trans-119">åœ¨<a href="#ref-125" class="ref-link">[125]</a>ä¸­ï¼Œå½“æ™ºèƒ½ä½“è¾“å‡ºä¸åˆ©è¡Œä¸ºæ—¶ä¼šè§¦å‘äººç±»åœ¨çº¿å¹²é¢„ï¼Œè¿™å¯ä»¥é™åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸å®‰å…¨æ¢ç´¢ï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯ä¸­æä¾›ç¤ºèŒƒã€‚ç±»ä¼¼åœ°ï¼ŒWuç­‰äºº<a href="#ref-163" class="ref-link">[163]</a>å»ºç«‹äº†ä¸€ä¸ªé›†æˆæ¡†æ¶ï¼ŒåŒ…æ‹¬äººç±»/å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŠ¨ä½œåˆ‡æ¢æœºåˆ¶ã€åŸºäºä¼˜åŠ¿çš„ç»éªŒä¼˜å…ˆçº§å¤„ç†ä»¥åŠäººç±»å¹²é¢„å¥–åŠ±å¡‘é€ ã€‚å…¶ç‹¬ç‰¹çš„å¯¹äººç±»æŒ‡å¯¼è´¨é‡çš„åŒºåˆ†èƒ½åŠ›æœ‰åŠ©äºæé«˜å­¦ä¹ æ€§èƒ½ã€‚

c) ä½¿ç”¨é¢„è®­ç»ƒç­–ç•¥çš„å­¦ä¹ ï¼šè¿‘æœ€ä¼˜ç­–ç•¥<div id="badge-119" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 119)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('119', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-120"><div class="row text-row"><div class="col-src">[[HEADER: , g p y 1) Learning from Demonstration (LfD)]]</div><div class="col-trans" id="trans-120"><b>, g p y 1) ç¤ºæ•™å­¦ä¹ ï¼ˆLfDï¼‰</b><div id="badge-120" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 120)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('120', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-121"><div class="row text-row"><div class="col-src">extracted from offline demonstrations can be effectively used for online fine-tuning [164]. Huang et al. [165] distill human prior knowledge into imitative expert policy using Behavior Cloning (BC). Subsequently, a penalty term based on Kullbackâ€“Leibler (KL) divergence is added to the reward function, making it fast close to the expert policy in online learning.

Shi et al. [117] employ DAGGER to train an IL agent for online RL initialization, which only requires a small amount of scene data to address the learning inefficiency under sparse rewards. In [166], Decision Transformer [167], which is an approach lying in between BC and offline RL, is used to extract a lightweight policy from large-scale offline guidance strategies during online interactions.</div><div class="col-trans" id="trans-121">ä»ç¦»çº¿æ¼”ç¤ºä¸­æå–çš„æ•°æ®å¯ä»¥æœ‰æ•ˆç”¨äºåœ¨çº¿å¾®è°ƒ[<a href="#ref-164" class="ref-link">[164]</a>]ã€‚é»„ç­‰[<a href="#ref-165" class="ref-link">[165]</a>]åˆ©ç”¨è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰å°†äººç±»å…ˆéªŒçŸ¥è¯†æç‚¼ä¸ºæ¨¡ä»¿ä¸“å®¶ç­–ç•¥ã€‚éšåï¼Œåœ¨å¥–åŠ±å‡½æ•°ä¸­æ·»åŠ äº†ä¸€ä¸ªåŸºäºKullback-Leiblerï¼ˆKLï¼‰æ•£åº¦çš„æƒ©ç½šé¡¹ï¼Œä½¿å…¶åœ¨åœ¨çº¿å­¦ä¹ è¿‡ç¨‹ä¸­å¿«é€Ÿæ¥è¿‘ä¸“å®¶ç­–ç•¥ã€‚

çŸ³ç­‰[<a href="#ref-117" class="ref-link">[117]</a>]é‡‡ç”¨DAGGERè®­ç»ƒILä»£ç†è¿›è¡Œåœ¨çº¿RLåˆå§‹åŒ–ï¼Œä»…éœ€å°‘é‡åœºæ™¯æ•°æ®å³å¯è§£å†³ç¨€ç–å¥–åŠ±ä¸‹çš„å­¦ä¹ æ•ˆç‡é—®é¢˜ã€‚åœ¨<a href="#ref-166" class="ref-link">[166]</a>ä¸­ï¼Œä½¿ç”¨ä»‹äºè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆoffline RLï¼‰ä¹‹é—´çš„å†³ç­–è½¬æ¢å™¨ï¼ˆDecision Transformerï¼‰[<a href="#ref-167" class="ref-link">[167]</a>]ä»å¤§è§„æ¨¡çš„ç¦»çº¿æŒ‡å¯¼ç­–ç•¥ä¸­æå–è½»é‡çº§ç­–ç•¥ï¼Œåœ¨åœ¨çº¿äº¤äº’è¿‡ç¨‹ä¸­è¿›è¡Œåˆ©ç”¨ã€‚<div id="badge-121" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 121)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('121', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-122"><div class="row text-row"><div class="col-src">It outperforms policy initialization via both BC and offline RL for safety-critical navigation and AD tasks. 2) Task Differentiation It challenging to directly learn an effective driving policy in a complex MoP task. Decomposing the task into different parts is a feasible way [168]. Instead of learning to deal with the whole task directly starting from a complex environment, the agent learns the different sub-tasks in stages.

The initial task guides the RL agent to perform better on the final task [169], reducing the learning complexity and improving convergence and sample efficiency. a) Curriculum Learning: Curriculum Learning (CL) is a training technique that breaks down the learning process into tasks of increasing complexity. It enables incremental learning, which helps premature failure under high complexity and enhances learning efficiency.</div><div class="col-trans" id="trans-122">å®ƒåœ¨å®‰å…¨å…³é”®çš„å¯¼èˆªå’ŒADä»»åŠ¡ä¸­ï¼Œæ— è®ºæ˜¯é€šè¿‡è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰è¿˜æ˜¯ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆoffline RLï¼‰ï¼Œéƒ½è¡¨ç°å‡ºæ›´ä¼˜çš„ç­–ç•¥åˆå§‹åŒ–æ•ˆæœã€‚2) ä»»åŠ¡å·®å¼‚åŒ–
ç›´æ¥åœ¨ä¸€ä¸ªå¤æ‚çš„å¤šç›®æ ‡è§„åˆ’ï¼ˆMoPï¼‰ä»»åŠ¡ä¸­å­¦ä¹ æœ‰æ•ˆçš„é©¾é©¶ç­–ç•¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸åŒçš„éƒ¨åˆ†æ˜¯ä¸€ç§å¯è¡Œçš„æ–¹æ³•[<a href="#ref-168" class="ref-link">[168]</a>]ã€‚ä¸ä»ä¸€ä¸ªå¤æ‚ç¯å¢ƒç›´æ¥å¼€å§‹å­¦ä¹ æ•´ä¸ªä»»åŠ¡ä¸åŒï¼Œä»£ç†åˆ†é˜¶æ®µå­¦ä¹ ä¸åŒçš„å­ä»»åŠ¡ã€‚

åˆå§‹ä»»åŠ¡å¯ä»¥å¼•å¯¼RLä»£ç†åœ¨æœ€ç»ˆä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½<a href="#ref-169" class="ref-link">[169]</a>ï¼Œä»è€Œé™ä½å­¦ä¹ å¤æ‚åº¦ã€æé«˜æ”¶æ•›æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚a) é€çº§å­¦ä¹ ï¼šé€çº§å­¦ä¹ ï¼ˆCLï¼‰æ˜¯ä¸€ç§è®­ç»ƒæŠ€æœ¯ï¼Œå°†å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºé€æ­¥å¢åŠ å¤æ‚æ€§çš„ä»»åŠ¡ã€‚å®ƒä½¿å¢é‡å­¦ä¹ æˆä¸ºå¯èƒ½ï¼Œåœ¨é«˜å¤æ‚æ€§ä¸‹é¿å…è¿‡æ—©å¤±è´¥å¹¶å¢å¼ºå­¦ä¹ æ•ˆç‡ã€‚<div id="badge-122" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 122)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('122', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-123"><div class="row text-row"><div class="col-src">Traditionally, CL relies on manually defined stages, with task difficulties set by human experts. Shi et al. [170] design three-stage curriculum RL including adaptive cruise control, lane changing, and overtaking tasks with different reward functions. Anzalone et al. [106] progress from static to complex traffic and weather conditions through five stages, with data augmentation in the final phases to learn complex behaviors.

Research on automatic curriculum generation has emerged to overcome the limitations of manual curriculum design. Banerjee et al. [111] used Bayesian optimization to automatically select curriculum through probabilistic inference on curriculum-reward functions. Niu et al. [171] decompose driving policy optimization into evaluation, scenario selection, and training.</div><div class="col-trans" id="trans-123">ä¼ ç»Ÿä¸Šï¼Œå…ƒå­¦ä¹ ï¼ˆCLï¼‰ä¾èµ–äºäººå·¥å®šä¹‰çš„é˜¶æ®µï¼Œå¹¶ç”±äººç±»ä¸“å®¶è®¾å®šä»»åŠ¡éš¾åº¦ã€‚Shiç­‰äºº[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]è®¾è®¡äº†ä¸€ä¸ªåŒ…å«è‡ªé€‚åº”å·¡èˆªæ§åˆ¶ã€å˜é“å’Œè¶…è½¦ç­‰ä¸åŒå¥–åŠ±å‡½æ•°çš„ä»»åŠ¡ä¸‰é˜¶æ®µè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚Anzaloneç­‰äºº[[LINK: <a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]åˆ™ä»é™æ€äº¤é€šæ¡ä»¶é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚äº¤é€šå’Œå¤©æ°”æ¡ä»¶ï¼Œé€šè¿‡äº”ä¸ªé˜¶æ®µçš„å­¦ä¹ ï¼Œå¹¶åœ¨åæœŸä½¿ç”¨æ•°æ®å¢å¼ºæ¥å­¦ä¹ å¤æ‚çš„é©¾é©¶è¡Œä¸ºã€‚

ä¸ºäº†å…‹æœæ‰‹åŠ¨è®¾è®¡è¯¾ç¨‹çš„å±€é™æ€§ï¼Œè‡ªåŠ¨è¯¾ç¨‹ç”Ÿæˆçš„ç ”ç©¶å·²ç»å…´èµ·ã€‚Banerjeeç­‰äºº[[LINK: <a href="#Equation_1" class="eq-link" onclick="highlightAsset('Equation_1'); return false;">Eq. 1</a>|<a href="#Equation_1" class="eq-link" onclick="highlightAsset('Equation_1'); return false;">Eq. 1</a>]]åˆ©ç”¨è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨é€‰æ‹©è¯¾ç¨‹ï¼Œé€šè¿‡å¯¹è¯¾ç¨‹-å¥–åŠ±å‡½æ•°è¿›è¡Œæ¦‚ç‡æ¨æ–­æ¥è¿›è¡Œé€‰æ‹©ã€‚Niuç­‰äºº[[LINK: Table_1|<a href="#Table_1" class="tab-link" onclick="highlightAsset('Table_1'); return false;">Table1</a>]]å°†é©¾é©¶ç­–ç•¥ä¼˜åŒ–åˆ†è§£ä¸ºè¯„ä¼°ã€åœºæ™¯é€‰æ‹©å’Œè®­ç»ƒä¸‰ä¸ªæ­¥éª¤ã€‚<div id="badge-123" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 123)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('123', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-124"><div class="row text-row"><div class="col-src">By dynamically estimating failure probabilities and resampling historical scenarios, this method provides real-time curriculum adaptation, improving learning robustness. Reference [172] introduce a task-driven labeled PAMDP based on LTL progression, which decomposes the training task at an abstract level and informs the RL agent of its current task progress. This technique enhances the exploration efficiency but so far limited to robotic grasping, with no application to complex MoP tasks in AD.

b) Transfer Learning: Transfer learning (TL) leverages knowledge reuse techniques [173] to utilize knowledge learned from related tasks. Originally, TL was intended to effectively transfer policies to new environments for better generalizability, which will be described in later Sec V.C later. At the same time, TL also contributes to sample efficiency, by allowing accelerating the learning process of new tasks with fewer samples. Specifically, given a set of source domain MS and</div><div class="col-trans" id="trans-124">é€šè¿‡åŠ¨æ€ä¼°è®¡æ•…éšœæ¦‚ç‡å¹¶é‡æ–°é‡‡æ ·å†å²åœºæ™¯ï¼Œè¯¥æ–¹æ³•æä¾›äº†å®æ—¶è¯¾ç¨‹é€‚åº”æ€§ï¼Œä»è€Œæé«˜å­¦ä¹ çš„é²æ£’æ€§ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-172" class="ref-link">[172]</a>ä»‹ç»äº†ä¸€ç§åŸºäºLTLæ¼”è¿›çš„ä»»åŠ¡é©±åŠ¨æ ‡è®°PAMDPï¼ˆPartially Observable Markov Decision Processï¼‰ï¼Œå®ƒåœ¨æŠ½è±¡çº§åˆ«ä¸Šåˆ†è§£è®­ç»ƒä»»åŠ¡ï¼Œå¹¶å‘RLï¼ˆReinforcement Learningï¼‰ä»£ç†å‘ŠçŸ¥å…¶å½“å‰ä»»åŠ¡è¿›åº¦ã€‚è¯¥æŠ€æœ¯æé«˜äº†æ¢ç´¢æ•ˆç‡ï¼Œä½†ç›®å‰ä»…é™äºæœºå™¨äººæŠ“å–ä»»åŠ¡ï¼Œå°šæœªåº”ç”¨äºADï¼ˆAutonomous Drivingï¼‰ä¸­çš„å¤æ‚MoPï¼ˆMotion Planningï¼‰ä»»åŠ¡ã€‚

b) è¿ç§»å­¦ä¹ ï¼šè¿ç§»å­¦ä¹ ï¼ˆTLï¼‰åˆ©ç”¨çŸ¥è¯†é‡ç”¨æŠ€æœ¯<a href="#ref-173" class="ref-link">[173]</a>æ¥åˆ©ç”¨ä»ç›¸å…³ä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†ã€‚æœ€åˆï¼ŒTLæ—¨åœ¨æœ‰æ•ˆå°†ç­–ç•¥è½¬ç§»åˆ°æ–°ç¯å¢ƒä¸­ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å°†åœ¨åç»­çš„Sec V.Cä¸­è¿›è¡Œæè¿°ã€‚åŒæ—¶ï¼ŒTLä¹Ÿæœ‰åŠ©äºæ ·æœ¬æ•ˆç‡ï¼Œé€šè¿‡ä½¿ç”¨è¾ƒå°‘çš„æ•°æ®åŠ å¿«æ–°ä»»åŠ¡çš„å­¦ä¹ è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæºåŸŸMSï¼ˆSource Domainï¼‰é›†åˆï¼Œ<div id="badge-124" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 124)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('124', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-125"><div class="row text-row"><div class="col-src">[[HEADER: 2) Task Differentiation]]</div><div class="col-trans" id="trans-125"><b>2ï¼‰ä»»åŠ¡å·®å¼‚åŒ–</b><div id="badge-125" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 125)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('125', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-126"><div class="row text-row"><div class="col-src">target domain Mt, TL learns the optimal policy Ï€âˆ—and the target domain by utilizing exterior information from MS and interior information from Mt. For example, Yan et al. [174] use the policy trained in the source domain as the initialization policy for the target domain, thereby improving the policy performance and convergence speed in the target domain. Shu et al. [121] improve the control performance and learning efficiency of the Dueling DQN through three transfer rules.

c) Hierarchical Learning: Hierarchical learning architecture (as discussed in Sec IV.B) decomposes the overall task at the action-output level, which can also enhance the feasibility of rapidly learning policy for complex tasks [172].</div><div class="col-trans" id="trans-126">ç›®æ ‡åŸŸMtï¼Œé€šè¿‡åˆ©ç”¨MSçš„å¤–éƒ¨ä¿¡æ¯å’ŒMtè‡ªèº«çš„å†…éƒ¨ä¿¡æ¯æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥Ï€âˆ—åŠç›®æ ‡åŸŸã€‚ä¾‹å¦‚ï¼ŒYanç­‰äºº<a href="#ref-174" class="ref-link">[174]</a>ä½¿ç”¨æºåŸŸä¸­è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥ä½œä¸ºç›®æ ‡åŸŸåˆå§‹åŒ–ç­–ç•¥ï¼Œä»è€Œåœ¨ç›®æ ‡åŸŸä¸­æé«˜äº†ç­–ç•¥æ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ã€‚Shuç­‰äºº<a href="#ref-121" class="ref-link">[121]</a>é€šè¿‡ä¸‰ç§è½¬ç§»è§„åˆ™æ”¹è¿›äº† Dueling DQN çš„æ§åˆ¶æ€§èƒ½å’Œå­¦ä¹ æ•ˆç‡ã€‚

c) åˆ†å±‚å­¦ä¹ ï¼šåˆ†å±‚å­¦ä¹ æ¶æ„ï¼ˆå¦‚ç¬¬å››èŠ‚Béƒ¨åˆ†æ‰€è¿°ï¼‰åœ¨åŠ¨ä½œ-è¾“å‡ºå±‚é¢åˆ†è§£æ•´ä½“ä»»åŠ¡ï¼Œè¿™ä¹Ÿæœ‰åŠ©äºå¿«é€Ÿå­¦ä¹ å¤æ‚ä»»åŠ¡çš„ç­–ç•¥<a href="#ref-172" class="ref-link">[172]</a>ã€‚<div id="badge-126" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 126)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('126', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-127"><div class="row text-row"><div class="col-src">For MoP for AD, generating only the steering angle usually results in the vehicle deviating far from the lane centerline, because it is difficult for an RL agent to quickly distinguish between lanechanging and lane-following behaviors during the learning process [175]. High-level discrete semantic behaviors and lowlevel control commands can be combined well to achieve more precise and flexible motion control while ensuring clear driving objectives[123].

Xia et al. [176] also define highlevel semantic behavior, and they couple them to low-level control actions, which computes fine-grained actions based on coarse-grained decisions that output them synchronously. The parameterized action space has achieved promising results in learning manipulation skills [177], but it has not been explored much in MoP for AD.</div><div class="col-trans" id="trans-127">å¯¹äºADä¸­çš„MoPï¼ˆMotion Planningï¼‰ï¼Œä»…ç”Ÿæˆè½¬å‘è§’é€šå¸¸ä¼šå¯¼è‡´è½¦è¾†åç¦»è½¦é“ä¸­å¿ƒçº¿ï¼Œå› ä¸ºåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼ŒRLï¼ˆReinforcement Learningï¼‰ä»£ç†éš¾ä»¥è¿…é€ŸåŒºåˆ†å˜é“è¡Œä¸ºå’Œè·Ÿéšè½¦é“è¡Œä¸º<a href="#ref-175" class="ref-link">[175]</a>ã€‚å°†é«˜å±‚ç¦»æ•£è¯­ä¹‰è¡Œä¸ºä¸ä½å±‚æ§åˆ¶å‘½ä»¤ç›¸ç»“åˆå¯ä»¥å®ç°æ›´ç²¾ç¡®å’Œçµæ´»çš„è¿åŠ¨æ§åˆ¶ï¼ŒåŒæ—¶ç¡®ä¿æ˜ç¡®çš„é©¾é©¶ç›®æ ‡<a href="#ref-123" class="ref-link">[123]</a>ã€‚

Xiaç­‰äºº<a href="#ref-176" class="ref-link">[176]</a>ä¹Ÿå®šä¹‰äº†é«˜å±‚è¯­ä¹‰è¡Œä¸ºï¼Œå¹¶å°†å…¶è€¦åˆåˆ°ä½å±‚æ§åˆ¶åŠ¨ä½œä¸­ï¼Œåœ¨ç²—ç²’åº¦å†³ç­–è¾“å‡ºçš„åŒæ—¶è®¡ç®—ç»†ç²’åº¦çš„åŠ¨ä½œã€‚å‚æ•°åŒ–çš„åŠ¨ä½œç©ºé—´åœ¨å­¦ä¹ æ“ä½œæŠ€èƒ½æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœ<a href="#ref-177" class="ref-link">[177]</a>ï¼Œä½†åœ¨ADçš„MoPä¸­å°šæœªå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚<div id="badge-127" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 127)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('127', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-128"><div class="row text-row"><div class="col-src">3) Promoting Exploration Efficiently exploring the environment and gathering informative experiences is also important for accelerating learning toward the optimal policy [42]. Uncertainty-oriented exploration generally considers epistemic uncertainty and aleatoric uncertainty, similar to the safety considerations discussed in Sec. V.A.

Lee et al. [178] leverage epistemic uncertainty to guide the policy in exploring unknown environments with high-uncertainty, allowing the RL agent to develop a more comprehensive understanding of the surroundings. Both types of uncertainty are considered in [179] within a single system to enhance the robustness of exploration against environmental noise. Intrinsic motivation-oriented exploration typically heuristically utilizes various types of reward-agnostic information to promote exploration.</div><div class="col-trans" id="trans-128">3) æœ‰æ•ˆæ¢ç´¢ç¯å¢ƒå¹¶æ”¶é›†æœ‰ä»·å€¼çš„ç»éªŒå¯¹äºåŠ é€Ÿå‘æœ€ä¼˜ç­–ç•¥çš„å­¦ä¹ ä¹Ÿéå¸¸é‡è¦<a href="#ref-42" class="ref-link">[42]</a>ã€‚é¢å‘ä¸ç¡®å®šæ€§çš„æ¢ç´¢é€šå¸¸è€ƒè™‘å…ˆéªŒä¸ç¡®å®šæ€§ä¸ç»Ÿè®¡ä¸ç¡®å®šæ€§ï¼Œç±»ä¼¼äºç¬¬V.AèŠ‚ä¸­è®¨è®ºçš„å®‰å…¨æ€§è€ƒé‡ã€‚

Leeç­‰äºº<a href="#ref-178" class="ref-link">[178]</a>åˆ©ç”¨å…ˆéªŒä¸ç¡®å®šæ€§æ¥å¼•å¯¼æ™ºèƒ½ä½“åœ¨é«˜ä¸ç¡®å®šæ€§æœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œä½¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†èƒ½å¤Ÿæ›´å…¨é¢åœ°äº†è§£å‘¨å›´ç¯å¢ƒã€‚è€Œåœ¨<a href="#ref-179" class="ref-link">[179]</a>ä¸­ï¼Œè¿™ä¸¤ç§ç±»å‹çš„ä¸ç¡®å®šæ€§è¢«æ•´åˆåˆ°ä¸€ä¸ªç³»ç»Ÿä¸­ï¼Œä»¥å¢å¼ºå¯¹ç¯å¢ƒå™ªå£°çš„é²æ£’æ€§æ¢ç´¢èƒ½åŠ›ã€‚å†…åœ¨åŠ¨æœºé©±åŠ¨çš„æ¢ç´¢é€šå¸¸é€šè¿‡å„ç§å¥–åŠ±æ— å…³çš„ä¿¡æ¯æ¥å¯å‘å¼åœ°ä¿ƒè¿›æ¢ç´¢ã€‚<div id="badge-128" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 128)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('128', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-129"><div class="row text-row"><div class="col-src">In the absence of an explicit reward signal, the RL agent can use intrinsic motivation to evaluate the quality of its actions. Ma et al. [180] use intrinsic curiosity to drive the agent to explore the environment in advance and collect experience. Curiosity here is represented as the error in predicting the outcome of the agentâ€™s actions in its current state, i.e., the agent learns from the prediction error of forward dynamics.

Wu et al. [181] use recurrent neural network to generate an intrinsic reward to encounter the RL agent to explore environment, improving exploration efficiency.</div><div class="col-trans" id="trans-129">åœ¨ç¼ºä¹æ˜ç¡®å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹ï¼ŒRLä»£ç†å¯ä»¥åˆ©ç”¨å†…åœ¨åŠ¨æœºæ¥è¯„ä¼°å…¶è¡Œä¸ºçš„è´¨é‡ã€‚Maç­‰<a href="#ref-180" class="ref-link">[180]</a>ä½¿ç”¨å†…åœ¨çš„å¥½å¥‡å¿ƒä¿ƒä½¿ä»£ç†æå‰æ¢ç´¢ç¯å¢ƒå¹¶æ”¶é›†ç»éªŒã€‚è¿™é‡Œçš„å¥½å¥‡å¿ƒè¡¨ç°ä¸ºé¢„æµ‹ä»£ç†åœ¨å…¶å½“å‰çŠ¶æ€ä¸‹è¡ŒåŠ¨ç»“æœçš„è¯¯å·®ï¼Œå³ä»£ç†é€šè¿‡å‰å‘åŠ¨åŠ›å­¦çš„é¢„æµ‹è¯¯å·®è¿›è¡Œå­¦ä¹ ã€‚

Wuç­‰<a href="#ref-181" class="ref-link">[181]</a>åˆ©ç”¨å¾ªç¯ç¥ç»ç½‘ç»œç”Ÿæˆå†…åœ¨å¥–åŠ±ä»¥æ¿€åŠ±RLä»£ç†æ¢ç´¢ç¯å¢ƒï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚<div id="badge-129" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 129)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('129', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-130"><div class="row text-row"><div class="col-src">[[HEADER: 3) Promoting Explorationi]]</div><div class="col-trans" id="trans-130"><b>3) ä¿ƒè¿›æ¢ç´¢</b><div id="badge-130" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 130)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('130', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-131"><div class="row text-row"><div class="col-src">[[HEADER: C. Generalization Capability]]</div><div class="col-trans" id="trans-131"><b>C. ä¸€èˆ¬åŒ–èƒ½åŠ›</b><div id="badge-131" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 131)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('131', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-132"><div class="row text-row"><div class="col-src">As noted in Sec. III, most studies have been conducted in low-cost simulation environments tailored to single-task settings. However, task variability can lead to policy failure when applied across different environments. Furthermore, owing to the inherent incomplete limitations of the RL training process, it has poor generalization ability in rare scenarios [182].

The ability for long-term multi-task learning is required to enhance the generalization ability of RL agents to the variety of ever-changing complex scenarios that real-world AD applications may face. The generalization ability includes both the policy that can be transferred to various driving tasks, and the robustness in response to perturbations during the execution [183].</div><div class="col-trans" id="trans-132">å¦‚ç¬¬ä¸‰èŠ‚æ‰€è¿°ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨é’ˆå¯¹å•ä¸€ä»»åŠ¡è®¾ç½®å®šåˆ¶çš„ä½æˆæœ¬ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œã€‚ç„¶è€Œï¼Œå½“è¿™äº›ç­–ç•¥åº”ç”¨åˆ°ä¸åŒçš„ç¯å¢ƒæ—¶ï¼Œç”±äºä»»åŠ¡å˜åŒ–æ€§å¯èƒ½å¯¼è‡´ç­–ç•¥å¤±æ•ˆã€‚æ­¤å¤–ï¼Œç”±äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¿‡ç¨‹å›ºæœ‰çš„ä¸å®Œæ•´æ€§é™åˆ¶ï¼Œå…¶åœ¨ç½•è§åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·® <a href="#ref-182" class="ref-link">[182]</a>ã€‚

ä¸ºäº†å¢å¼ºRLä»£ç†åœ¨ä¸æ–­å˜åŒ–çš„å¤æ‚ç°å®ä¸–ç•ŒADåº”ç”¨åœºæ™¯ä¸­é¢å¯¹å„ç§é©¾é©¶ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å…·å¤‡é•¿æœŸå¤šä»»åŠ¡å­¦ä¹ çš„èƒ½åŠ›ã€‚è¿™ç§æ³›åŒ–èƒ½åŠ›åŒ…æ‹¬èƒ½å¤Ÿè½¬ç§»åˆ°ä¸åŒé©¾é©¶ä»»åŠ¡çš„ç­–ç•¥ï¼Œä»¥åŠæ‰§è¡Œè¿‡ç¨‹ä¸­å¯¹å¹²æ‰°çš„é²æ£’æ€§å“åº” <a href="#ref-183" class="ref-link">[183]</a>ã€‚<div id="badge-132" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 132)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('132', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-133"><div class="row text-row"><div class="col-src">Some cutting-edge techniques explored for the generalization capability of RL, but they have not yet been well applied in the field of MoP in AD. 1) Knowledge Transfer a) Transfer Learning: Knowledge reuse in TL can improve generalization across different but related or similar tasks. Balakrishnan et al. [184] apply a domain randomization technique to generalize the policy learned in the simple WiseMove environment to the high-fidelity simulator WiseSim.

Furthermore, Kevin et al. [185] combine domain adaptation and domain randomization techniques. By integrating virtual training with real-world data, they reduce the sim-to-real transfer gap in AD applications.

Hieu et al. [186] pre-train on demonstration data using a combination of temporal difference and supervised loss, and then continuously update the policy by incorporating demonstration data with newly collected data, resulting in strong performance across different road conditions and weather conditions.</div><div class="col-trans" id="trans-133">ä¸€äº›å‰æ²¿æŠ€æœ¯è¢«æ¢ç´¢ç”¨äºå¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›æ–¹æ³•å°šæœªåœ¨ADä¸­çš„MoPé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚1) çŸ¥è¯†è¿ç§»

a) è½¬ç§»å­¦ä¹ ï¼šé€šè¿‡åœ¨TLä¸­é‡ç”¨çŸ¥è¯†å¯ä»¥æé«˜ä¸åŒä½†ç›¸å…³æˆ–ç›¸ä¼¼ä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚Balakrishnanç­‰äºº[[LINK: <a href="#ref-184" class="ref-link">[184]</a>|ç­‰]]åº”ç”¨åŸŸéšæœºåŒ–æŠ€æœ¯ï¼Œå°†ç®€å•WiseMoveç¯å¢ƒä¸­å­¦åˆ°çš„ç­–ç•¥æ¨å¹¿åˆ°é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨WiseSimã€‚

æ­¤å¤–ï¼ŒKevinç­‰äºº[[LINK: <a href="#ref-185" class="ref-link">[185]</a>|ç­‰]]ç»“åˆäº†é¢†åŸŸé€‚åº”å’ŒåŸŸéšæœºåŒ–æŠ€æœ¯ã€‚é€šè¿‡æ•´åˆè™šæ‹Ÿè®­ç»ƒä¸ç°å®ä¸–ç•Œæ•°æ®ï¼Œä»–ä»¬åœ¨ADåº”ç”¨ä¸­å‡å°‘äº†ä»ä»¿çœŸåˆ°ç°å®ä¸–ç•Œçš„è½¬ç§»å·®è·ã€‚

Hieuç­‰äºº[[LINK: <a href="#ref-186" class="ref-link">[186]</a>|ç­‰]]åœ¨ä½¿ç”¨æ—¶é—´å·®åˆ†å’Œç›‘ç£æŸå¤±çš„ç»„åˆè¿›è¡Œæ¼”ç¤ºæ•°æ®é¢„è®­ç»ƒåï¼Œä¸æ–­é€šè¿‡ç»“åˆæ–°æ”¶é›†çš„æ•°æ®æ›´æ–°ç­–ç•¥ï¼Œä»è€Œåœ¨ä¸åŒé“è·¯æ¡ä»¶å’Œå¤©æ°”æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ã€‚<div id="badge-133" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 133)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('133', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-134"><div class="row text-row"><div class="col-src">Shoeleh et al. [187] propose a skill-based transfer learning and domain adaptation method, which helps the agent discover the state-action mapping that represents the relationship between the source and target tasks, thereby providing knowledge generalization across multiple tasks. b) Meta Learning: Meta-RL aims to producing a broadly generalizable policy [188].

Generally, Meta-RL consists of an inner loop, which focuses on learning the specific task, and an outer loop, where the agent extracts knowledge from multiple tasks to improve its adaptability. In [189], following meta-training on the lane-change task under different traffic densities, the policy is able to safely handle meta-testing scenarios with high traffic densities.</div><div class="col-trans" id="trans-134">Shoelehç­‰<a href="#ref-187" class="ref-link">[187]</a>æå‡ºäº†ä¸€ç§åŸºäºæŠ€èƒ½çš„è¿ç§»å­¦ä¹ å’Œé¢†åŸŸé€‚åº”æ–¹æ³•ï¼Œå¸®åŠ©ä»£ç†å‘ç°è¡¨ç¤ºæºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡ä¹‹é—´å…³ç³»çš„çŠ¶æ€-åŠ¨ä½œæ˜ å°„ï¼Œä»è€Œåœ¨å¤šä¸ªä»»åŠ¡ä¸­æä¾›çŸ¥è¯†æ³›åŒ–ã€‚b) å…ƒå­¦ä¹ ï¼šå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆMeta-RLï¼‰æ—¨åœ¨ç”Ÿæˆå¹¿æ³›é€‚ç”¨çš„ç­–ç•¥<a href="#ref-188" class="ref-link">[188]</a>ã€‚

é€šå¸¸è€Œè¨€ï¼Œå…ƒå¼ºåŒ–å­¦ä¹ ç”±ä¸€ä¸ªå†…éƒ¨å¾ªç¯ç»„æˆï¼Œè¯¥å¾ªç¯ä¸“æ³¨äºå­¦ä¹ ç‰¹å®šä»»åŠ¡ï¼›ä»¥åŠä¸€ä¸ªå¤–éƒ¨å¾ªç¯ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ä»£ç†ä»å¤šä¸ªä»»åŠ¡ä¸­æå–çŸ¥è¯†ä»¥æé«˜å…¶é€‚åº”æ€§ã€‚åœ¨<a href="#ref-189" class="ref-link">[189]</a>çš„ç ”ç©¶ä¸­ï¼Œç»è¿‡ä¸åŒäº¤é€šå¯†åº¦ä¸‹çš„å˜é“ä»»åŠ¡è®­ç»ƒåï¼Œç­–ç•¥èƒ½å¤Ÿå®‰å…¨åœ°åº”å¯¹é«˜äº¤é€šå¯†åº¦çš„å…ƒæµ‹è¯•åœºæ™¯ã€‚<div id="badge-134" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 134)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('134', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-135"><div class="row text-row"><div class="col-src">Deng et al. [190] utilize parallel unfolding and multi-task objectives, meanwhile they design a two-stage constraint adaptation strategy to achieve rapid adaptation to new tasks by reusing meta-training data. c) Continual Learning: Continual RL (Cont-RL) aims to address the challenge of the ability limitation in handling new tasks without forgetting previously acquired knowledge, which enables continuous learning and adaptation to new environments [191].

Cont-RL requires an appropriate balance between the old and new tasks, with adequate generalizability to accommodate their distributional differences. Wei et al. [192] propose a shared feature extractor with an EWC loss to mitigate catastrophic forgetting and perform velocity control tasks across different environments.</div><div class="col-trans" id="trans-135">Dengç­‰<a href="#ref-190" class="ref-link">[190]</a>åˆ©ç”¨å¹¶è¡Œå±•å¼€å’Œå¤šä»»åŠ¡ç›®æ ‡ï¼ŒåŒæ—¶è®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µçº¦æŸé€‚åº”ç­–ç•¥ï¼Œé€šè¿‡é‡ç”¨å…ƒè®­ç»ƒæ•°æ®å®ç°å¯¹æ–°ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”ã€‚c) æŒç»­å­¦ä¹ ï¼šæŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆCont-RLï¼‰æ—¨åœ¨è§£å†³å¤„ç†æ–°ä»»åŠ¡è€Œä¸å¿˜è®°ä¹‹å‰è·å¾—çš„çŸ¥è¯†çš„èƒ½åŠ›é™åˆ¶é—®é¢˜ï¼Œä»è€Œå®ç°è¿ç»­å­¦ä¹ å¹¶é€‚åº”æ–°çš„ç¯å¢ƒ<a href="#ref-191" class="ref-link">[191]</a>ã€‚

Cont-RLéœ€è¦åœ¨æ—§ä»»åŠ¡å’Œæ–°ä»»åŠ¡ä¹‹é—´å–å¾—é€‚å½“çš„å¹³è¡¡ï¼Œå¹¶å…·å¤‡è¶³å¤Ÿçš„æ³›åŒ–èƒ½åŠ›ä»¥åº”å¯¹å®ƒä»¬ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚Weiç­‰<a href="#ref-192" class="ref-link">[192]</a>æå‡ºäº†ä¸€ç§å…±äº«ç‰¹å¾æå–å™¨ä¸EWCæŸå¤±ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨ä¸åŒç¯å¢ƒä¸­æ‰§è¡Œé€Ÿåº¦æ§åˆ¶ä»»åŠ¡ã€‚<div id="badge-135" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 135)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('135', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-136"><div class="row text-row"><div class="col-src">Cao et al. [193] introduce a disengagement-case imagination augment continual learning (DICL) method, which is capable of constructing imaginationbased environments corresponding to disengagement cases and then improving driving policies within them. 2) Policy Stability a) Disturbance Robustness: Robust RL focuses on learning policies that exhibit performance robustness against system external disturbances, such as model mismatch and environ-</div><div class="col-trans" id="trans-136">Caoç­‰<a href="#ref-193" class="ref-link">[193]</a>æå‡ºäº†ä¸€ç§æ–­å¼€æƒ…å†µæƒ³è±¡å¢å¼ºæŒç»­å­¦ä¹ ï¼ˆDICLï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ„å»ºä¸æ–­å¼€æƒ…å†µç›¸å¯¹åº”çš„åŸºäºæƒ³è±¡çš„ç¯å¢ƒï¼Œå¹¶åœ¨æ­¤ç¯å¢ƒä¸­æ”¹è¿›é©¾é©¶ç­–ç•¥ã€‚2) ç­–ç•¥ç¨³å®šæ€§ a) æ‰°åŠ¨é²æ£’æ€§ï¼šé²æ£’å¼ºåŒ–å­¦ä¹ å…³æ³¨äºå­¦ä¹ åœ¨ç³»ç»Ÿå¤–éƒ¨æ‰°åŠ¨ï¼ˆå¦‚æ¨¡å‹ä¸åŒ¹é…å’Œç¯å¢ƒå˜åŒ–ï¼‰ä¸‹ä»èƒ½ä¿æŒæ€§èƒ½ç¨³å®šæ€§çš„ç­–ç•¥ã€‚<div id="badge-136" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 136)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('136', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-137"><div class="row text-row"><div class="col-src">[[HEADER: well applied in the field of 1) Knowledge Transfer]]</div><div class="col-trans" id="trans-137"><b>well applied in the field of 1) Knowledge Transfer</b><div id="badge-137" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 137)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('137', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-138"><div class="row text-row"><div class="col-src">[[HEADER: p g 2) Policy Stability]]</div><div class="col-trans" id="trans-138"><pg 2) æ”¿ç­–ç¨³å®šæ€§><div id="badge-138" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 138)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('138', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-139"><div class="row text-row"><div class="col-src">mental perturbations. Typically, robust RL is modeled as a two-player zero-sum Markov game, where an adversarial agent trains alongside the ego agent to maximize disturbances, forcing the RL agent to develop a robust policy in response to these disturbances. He et al. [194] develop an adversarial agent that maximizes the Jensen-Shannon (JS) divergence between the policy and the original policy under observation disturbances.

The RL agent incorporates the JS divergence as a constraint and use Lagrangian dual optimization to update its policy, thereby ensuring robustness to observation disturbances. Similarly in [86], the White-Box Adversarial Attack technique is employed to amplify the disturbance of each observation. Then, the policies of both the ego agent and the adversarial agent are weighted to output mixed actions, simulating the disturbances caused by environmental changes.</div><div class="col-trans" id="trans-139">å¿ƒç†å¹²æ‰°ã€‚é€šå¸¸ï¼Œé²æ£’å¼ºåŒ–å­¦ä¹ è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªä¸¤ç©å®¶é›¶å’Œé©¬å°”å¯å¤«åšå¼ˆï¼Œåœ¨è¿™ç§åšå¼ˆä¸­ï¼Œå¯¹æ‰‹ä»£ç†ä¸è‡ªæˆ‘ä»£ç†ä¸€åŒè®­ç»ƒä»¥æœ€å¤§åŒ–å¹²æ‰°ï¼Œè¿«ä½¿RLä»£ç†å‘å±•å‡ºé’ˆå¯¹è¿™äº›å¹²æ‰°çš„ç¨³å¥ç­–ç•¥ã€‚Heç­‰äºº[[LINK: ID|He et al. <a href="#ref-194" class="ref-link">[194]</a>]]å¼€å‘äº†ä¸€ä¸ªæœ€å¤§åŒ–åœ¨è§‚æµ‹å¹²æ‰°ä¸‹ç­–ç•¥ä¸åŸå§‹ç­–ç•¥ä¹‹é—´æ°ä¼¦-é¦™å†œï¼ˆJSï¼‰æ•£åº¦çš„å¯¹æ‰‹ä»£ç†ã€‚

å¼ºåŒ–å­¦ä¹ ä»£ç†å°†JSæ•£åº¦ä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œå¹¶ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶ä¼˜åŒ–æ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»è€Œç¡®ä¿å¯¹å…¶å®ƒè§‚æµ‹å¹²æ‰°å…·æœ‰é²æ£’æ€§ã€‚ç±»ä¼¼åœ°ï¼Œåœ¨[[LINK: ID|Ref. <a href="#ref-86" class="ref-link">[86]</a>]]ä¸­ï¼Œç™½ç›’å¯¹æŠ—æ”»å‡»æŠ€æœ¯è¢«ç”¨æ¥æ”¾å¤§æ¯ä¸ªè§‚æµ‹çš„å¹²æ‰°ã€‚ç„¶åï¼Œè‡ªæˆ‘ä»£ç†å’Œå¯¹æ‰‹ä»£ç†çš„ç­–ç•¥è¢«åŠ æƒä»¥è¾“å‡ºæ··åˆåŠ¨ä½œï¼Œæ¨¡æ‹Ÿç”±ç¯å¢ƒå˜åŒ–å¼•èµ·çš„å¹²æ‰°ã€‚<div id="badge-139" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 139)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('139', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-140"><div class="row text-row"><div class="col-src">Additionally, high-uncertainty RL policies can be replaced with more stable baseline policies [153], enabling timely adaptation to changes in the environment . b) Uncertainty Adaptation: Uncertainty can also be leveraged to improve RL generalization performance. Epistemic uncertainty can reveal the test scenarios that are underrepresented in the training.

Lutjens et al. [195] use MC-Dropout and Bootstrapping to achieve parallelized epistemic uncertainty estimation to promote more cautious actions in unknown environments, thereby improving policy generalization. In addition, Hoi et al. [196] propose a risk-conditioned distributional soft actor-critic method that learns risk-sensitive policies based on aleatoric uncertainty. It supports the adjustment of risklevel sensitivity without retraining, enabling safe generalization across various scenarios.</div><div class="col-trans" id="trans-140">æ­¤å¤–ï¼Œé«˜ä¸ç¡®å®šæ€§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥å¯ä»¥è¢«æ›´ç¨³å®šçš„åŸºçº¿ç­–ç•¥æ‰€æ›¿ä»£[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]ï¼Œä»è€Œèƒ½å¤ŸåŠæ—¶é€‚åº”ç¯å¢ƒçš„å˜åŒ–ã€‚b) ä¸ç¡®å®šæ€§é€‚åº”ï¼šä¸ç¡®å®šæ€§ä¹Ÿå¯ä»¥ç”¨æ¥æé«˜RLçš„æ³›åŒ–æ€§èƒ½ã€‚å…ˆéªŒä¸ç¡®å®šæ€§å¯ä»¥æ­ç¤ºåœ¨è®­ç»ƒä¸­æœªå……åˆ†ä»£è¡¨çš„æµ‹è¯•åœºæ™¯ã€‚

Lutjensç­‰äºº[[LINK: <a href="#Table_2" class="tab-link" onclick="highlightAsset('Table_2'); return false;">Tab.2</a>|<a href="#Table_2" class="tab-link" onclick="highlightAsset('Table_2'); return false;">Tab.2</a>]]ä½¿ç”¨MC-Dropoutå’Œè‡ªåŠ©æ³•æ¥å®ç°å¹¶è¡ŒåŒ–çš„å…ˆéªŒä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»è€Œåœ¨æœªçŸ¥ç¯å¢ƒä¸­ä¿ƒä½¿é‡‡å–æ›´ä¸ºè°¨æ…çš„åŠ¨ä½œï¼Œè¿›è€Œæé«˜ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHoiç­‰äºº[[LINK: <a href="#Equation_3" class="eq-link" onclick="highlightAsset('Equation_3'); return false;">Eq. 3</a>|<a href="#Equation_3" class="eq-link" onclick="highlightAsset('Equation_3'); return false;">Eq. 3</a>]]æå‡ºäº†ä¸€ç§åŸºäºå¶ç„¶ä¸ç¡®å®šæ€§çš„é£é™©æ¡ä»¶åˆ†å¸ƒè½¯æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ é£é™©æ•æ„Ÿæ€§ç­–ç•¥ã€‚å®ƒæ”¯æŒæ— éœ€é‡æ–°è®­ç»ƒå³å¯è°ƒæ•´é£é™©æ°´å¹³çš„çµæ•åº¦ï¼Œä»è€Œåœ¨å„ç§åœºæ™¯ä¸‹å®ç°å®‰å…¨æ³›åŒ–ã€‚<div id="badge-140" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 140)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('140', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-141"><div class="row text-row"><div class="col-src">Some works further attempt to leverage both epistemic and aleatoric uncertainty within a system to improve adaptability to the environment [197]. Hoel et al. [152] propose Ensemble Quantile Networks (EQN), where Bayesian estimates of epistemic uncertainty are obtained through model ensemble methods, which are used to select actions with lower risk in unknown environments.

Meanwhile, aleatoric uncertainty is implicitly learned through quantile functions, balancing risk and time efficiency, thereby further enhancing the generalization capability. 3) Scenario Representation Establishing an effective and compact feature representation from observations that supports subsequent policy reasoning is also a key to improving generalization performance.</div><div class="col-trans" id="trans-141">ä¸€äº›ç ”ç©¶è¿›ä¸€æ­¥å°è¯•åœ¨ç³»ç»Ÿä¸­åŒæ—¶åˆ©ç”¨æœ¬ä½“è®ºä¸ç¡®å®šæ€§ï¼ˆepistemic uncertaintyï¼‰å’Œå¶ç„¶æ€§ä¸ç¡®å®šæ€§ï¼ˆaleatoric uncertaintyï¼‰ï¼Œä»¥æé«˜å¯¹ç¯å¢ƒçš„é€‚åº”èƒ½åŠ›[<a href="#ref-197" class="ref-link">[197]</a>]ã€‚Hoelç­‰äºº[<a href="#ref-152" class="ref-link">[152]</a>]æå‡ºäº†ä¸€ç§é›†æˆåˆ†ä½æ•°ç½‘ç»œï¼ˆEnsemble Quantile Networks, EQNï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹é›†æˆæ–¹æ³•è·å¾—æœ¬ä½“è®ºä¸ç¡®å®šæ€§ä¸‹çš„è´å¶æ–¯ä¼°è®¡ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¼°è®¡é€‰æ‹©åœ¨æœªçŸ¥ç¯å¢ƒä¸­é£é™©è¾ƒä½çš„åŠ¨ä½œã€‚

åŒæ—¶ï¼Œå¶ç„¶æ€§ä¸ç¡®å®šæ€§æ˜¯é€šè¿‡åˆ†ä½æ•°å‡½æ•°éšå¼å­¦ä¹ çš„ï¼Œè¿™åœ¨å¹³è¡¡é£é™©å’Œæ—¶é—´æ•ˆç‡æ–¹é¢èµ·åˆ°äº†ä½œç”¨ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚3) æƒ…æ™¯è¡¨ç¤ºï¼šä»è§‚æµ‹ä¸­å»ºç«‹ä¸€ä¸ªæœ‰æ•ˆä¸”ç´§å‡‘çš„åŠŸèƒ½è¡¨ç¤ºå½¢å¼ï¼Œå¹¶æ”¯æŒåç»­ç­–ç•¥æ¨ç†ä¹Ÿæ˜¯æé«˜æ³›åŒ–æ€§èƒ½çš„å…³é”®ã€‚<div id="badge-141" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 141)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('141', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-142"><div class="row text-row"><div class="col-src">Duan et al. [198] map surrounding vehiclesâ€™ features into encoding vectors, which are then summed element-wise to create a representation set, ensuring that the policy network remains unaffected by vehicle arrangement and preventing strategy fluctuations. Similarly, an MLP encoding function and summation operator are constructed to handle traffic flows with participants of varying types and quantities [95]. The transformer is commonly employed to enhance scene understanding. G.

Zhan et al. [107] construct a transformation module to extract observations from surrounding participants, the ego vehicle, and traffic lights, and use an Aggregation Module to combine these features into a fixed-dimensional representation, enabling the agent to adapt to dynamic environments with varying numbers of traffic participants. LLMs present a promising avenue for improving scenario</div><div class="col-trans" id="trans-142">Duanç­‰<a href="#ref-198" class="ref-link">[198]</a>å°†å‘¨å›´è½¦è¾†çš„ç‰¹å¾æ˜ å°„åˆ°ç¼–ç å‘é‡ä¸­ï¼Œç„¶åé€å…ƒç´ ç›¸åŠ ä»¥åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºé›†ï¼Œä»è€Œç¡®ä¿ç­–ç•¥ç½‘ç»œä¸å—è½¦è¾†æ’åˆ—çš„å½±å“ï¼Œå¹¶é˜²æ­¢ç­–ç•¥æ³¢åŠ¨ã€‚ç±»ä¼¼åœ°ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç¼–ç å‡½æ•°å’Œæ±‚å’Œè¿ç®—ç¬¦æ¥å¤„ç†ä¸åŒç±»å‹çš„å‚ä¸è€…å’Œæ•°é‡çš„äº¤é€šæµ<a href="#ref-95" class="ref-link">[95]</a>ã€‚é€šå¸¸ä½¿ç”¨å˜å‹å™¨æ¥å¢å¼ºåœºæ™¯ç†è§£ã€‚

Zhanç­‰<a href="#ref-107" class="ref-link">[107]</a>æ„å»ºäº†ä¸€ä¸ªè½¬æ¢æ¨¡å—ä»å‘¨å›´å‚ä¸è€…ã€egoè½¦è¾†ä»¥åŠäº¤é€šç¯ä¸­æå–è§‚å¯Ÿä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨èšåˆæ¨¡å—å°†è¿™äº›ç‰¹å¾ç»„åˆæˆå›ºå®šç»´åº¦çš„è¡¨ç¤ºï¼Œä»è€Œä½¿ä»£ç†èƒ½å¤Ÿé€‚åº”ä¸åŒæ•°é‡äº¤é€šå‚ä¸è€…çš„åŠ¨æ€ç¯å¢ƒã€‚å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæé«˜åœºæ™¯ç†è§£å’Œåº”å¯¹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚<div id="badge-142" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 142)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('142', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-143"><div class="row text-row"><div class="col-src">[[HEADER: g g 3) Scenario Representation]]</div><div class="col-trans" id="trans-143"><b>g g 3) åœºæ™¯è¡¨ç¤º</b><div id="badge-143" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 143)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('143', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-144"><div class="row text-row"><div class="col-src">understanding in MoP [199]. To release the agent from the burden of understanding the multi-modal data, LLMs can be used to extract meaningful feature representations and translate semantic or task information. Reference [200] model graphstructured reasoning through perception, prediction and planning question-answer pairs to mimic the human reasoning process, enabling the agent to correctly handle unseen deployment on Waymo after training only on NuScenes.

Reference [201] argues that the absence of task-relevant representations may hinder the mapping of the network from state to reward. Based on this motivation, they employ LLMs to generate task-related state representations accompanied by intrinsic reward functions for RL, which apply to both continuous and discontinuous reward scenarios, improving the adaptability of RL to new tasks.</div><div class="col-trans" id="trans-144">ä¸ºäº†å‡è½»ä»£ç†å¯¹å¤šæ¨¡æ€æ•°æ®ç†è§£çš„è´Ÿæ‹…ï¼Œå¯ä»¥ä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æå–æœ‰æ„ä¹‰çš„ç‰¹å¾è¡¨ç¤ºå¹¶ç¿»è¯‘è¯­ä¹‰æˆ–ä»»åŠ¡ä¿¡æ¯ã€‚å‚è€ƒæ–‡çŒ®<a href="#ref-200" class="ref-link">[200]</a>é€šè¿‡æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®é¢˜-ç­”æ¡ˆå¯¹è¿›è¡Œå›¾ç»“æ„æ¨ç†ï¼Œæ¨¡ä»¿äººç±»çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨ä»…åœ¨NuScenesä¸Šè®­ç»ƒåæ­£ç¡®å¤„ç†Waymoä¸Šçš„æœªè§è¿‡çš„éƒ¨ç½²ã€‚

å‚è€ƒæ–‡çŒ®<a href="#ref-201" class="ref-link">[201]</a>è®¤ä¸ºï¼Œç¼ºä¹ä¸ä»»åŠ¡ç›¸å…³çš„è¡¨ç°å¯èƒ½ä¼šé˜»ç¢ç½‘ç»œä»çŠ¶æ€åˆ°å¥–åŠ±çš„æ˜ å°„ã€‚åŸºäºè¿™ä¸€åŠ¨æœºï¼Œä»–ä»¬ä½¿ç”¨LLMsç”Ÿæˆä¸ä»»åŠ¡ç›¸å…³çš„çŠ¶æ€è¡¨ç¤ºï¼Œå¹¶ä¼´éšå†…ç”Ÿå¥–åŠ±å‡½æ•°åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œé€‚ç”¨äºè¿ç»­å’Œéè¿ç»­å¥–åŠ±åœºæ™¯ï¼Œä»è€Œæé«˜RLå¯¹æ–°ä»»åŠ¡çš„é€‚åº”æ€§ã€‚<div id="badge-144" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 144)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('144', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-145"><div class="row text-row"><div class="col-src">[[HEADER: D. Open Challenges and Outlook]]</div><div class="col-trans" id="trans-145">å¼€æ”¾æŒ‘æˆ˜ä¸å±•æœ›

åœ¨æœ¬ç ”ç©¶çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†å‡ ä¸ªå…³é”®çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚é¦–å…ˆï¼Œå°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•å·²ç»åœ¨å¤šä¸ªåœºæ™¯ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»é¢ä¸´ä¸€äº›æŠ€æœ¯éš¾é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤æ‚å…‰ç…§æ¡ä»¶ä¸‹çš„é²æ£’æ€§é—®é¢˜ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å®æ—¶å¤„ç†éœ€æ±‚ä»¥åŠå¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆå¤„ç†ç­‰ã€‚å…¶æ¬¡ï¼Œå¦‚ä½•è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æ­¤å¤–ï¼Œéšç€ç‰©è”ç½‘å’Œè¾¹ç¼˜è®¡ç®—çš„å‘å±•ï¼Œå¦‚ä½•å°†è¿™äº›æ™ºèƒ½ç®—æ³•æ— ç¼é›†æˆåˆ°å®é™…ç³»ç»Ÿä¸­ä¹Ÿæ˜¯ä¸€ä¸ªå€¼å¾—æ¢ç´¢çš„é—®é¢˜ã€‚

å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬å·¥ä½œèƒ½å¤Ÿä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„å‚è€ƒï¼Œå¹¶æ¿€å‘æ›´å¤šå­¦è€…å‚ä¸åˆ°è¿™ä¸€å‰æ²¿è¯¾é¢˜çš„ç ”ç©¶ä¸­æ¥ã€‚<div id="badge-145" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 145)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('145', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-146"><div class="row text-row"><div class="col-src">[[HEADER: 1) Safety Consideration]]</div><div class="col-trans" id="trans-146"><b>1) å®‰å…¨è€ƒè™‘</b><div id="badge-146" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 146)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('146', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-147"><div class="row text-row"><div class="col-src">) Safety Evaluation: Both policy objective optimization and safety hard constraints require an evaluation of the risk of unsafety. Most methods rely on the assumption of a priori environment dynamics, such as simple safety rules, safety sets, state predictions, etc. [12], [64]. It is difficult to validate these assumptions until the autonomous vehicle is in action, which results in a model mismatch in the explicit knowledge of environmental dynamics in the open interactive environment.

We believe that learning environment dynamics offers a promising solution to alleviate this problem by performing several step forward simulation with learned dynamics. The mismatch problem can be mitigated while retaining the inherent utility of the models and providing better generalizability.</div><div class="col-trans" id="trans-147">å®‰å…¨æ€§è¯„ä¼°ï¼šæ”¿ç­–ç›®æ ‡ä¼˜åŒ–å’Œå®‰å…¨ç¡¬çº¦æŸéƒ½éœ€è¦å¯¹æ½œåœ¨é£é™©è¿›è¡Œè¯„ä¼°ã€‚å¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºå…ˆéªŒç¯å¢ƒåŠ¨æ€çš„å‡è®¾ï¼Œä¾‹å¦‚ç®€å•çš„å®‰å…¨è§„åˆ™ã€å®‰å…¨é›†ã€çŠ¶æ€é¢„æµ‹ç­‰[<a href="#ref-12" class="ref-link">[12]</a>,<a href="#ref-64" class="ref-link">[64]</a>]ã€‚ç›´åˆ°è‡ªåŠ¨é©¾é©¶è½¦è¾†å®é™…è¿è¡Œæ—¶æ‰éš¾ä»¥éªŒè¯è¿™äº›å‡è®¾ï¼Œä»è€Œå¯¼è‡´åœ¨å¼€æ”¾äº¤äº’ç¯å¢ƒä¸­æ˜¾å¼çŸ¥è¯†ä¸­çš„æ¨¡å‹å¤±é…é—®é¢˜ã€‚

æˆ‘ä»¬è®¤ä¸ºé€šè¿‡åˆ©ç”¨å­¦ä¹ åˆ°çš„åŠ¨åŠ›å­¦è¿›è¡Œè‹¥å¹²æ­¥å‰ç»æ¨¡æ‹Ÿæ¥è§£å†³è¿™ä¸€é—®é¢˜å…·æœ‰å‰æ™¯ã€‚è¿™æ ·å¯ä»¥å‡è½»æ¨¡å‹å¤±é…çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹å›ºæœ‰çš„æ•ˆç”¨å¹¶æä¾›æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚<div id="badge-147" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 147)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('147', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-148"><div class="row text-row"><div class="col-src">Moreover, further incorporating RLâ€™s learning uncertainty into the risk evaluation process allows for a more comprehensive identification of dangerous scenarios and safety vulnerabilities with limited priori knowledge. b) Trade-off between Safety and Rewards: It is crucial to consider the trade-off between rewards and safety performance in an AD environment with complex interactions. On the one hand, if strict conservative cost functions are adopted, this may lead to poor reward utility.

In contrast, open constraints and costs can lead to unsafe policy [202]. On the other hand, policy-objective optimization encourages the agent to converge on a highly rewarding policy that satisfies the constraints, but it lacks theoretical safety guarantees. Hard safety constraints can limit the exploration space and tend toward an overly conservative driving policy [19]. Research on effectively combining the strengths of the two approaches is urgently needed.</div><div class="col-trans" id="trans-148">æ­¤å¤–ï¼Œè¿›ä¸€æ­¥å°†RLçš„å­¦ä¹ ä¸ç¡®å®šæ€§çº³å…¥é£é™©è¯„ä¼°è¿‡ç¨‹ï¼Œå¯ä»¥åœ¨æœ‰é™çš„å…ˆéªŒçŸ¥è¯†ä¸‹æ›´å…¨é¢åœ°è¯†åˆ«å±é™©åœºæ™¯å’Œå®‰å…¨æ¼æ´ã€‚b) å®‰å…¨ä¸å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ï¼šåœ¨å…·æœ‰å¤æ‚äº¤äº’ä½œç”¨çš„ADç¯å¢ƒä¸­ï¼Œè€ƒè™‘å®‰å…¨æ€§èƒ½ä¸å¥–åŠ±ä¹‹é—´çš„æƒè¡¡è‡³å…³é‡è¦ã€‚ä¸€æ–¹é¢ï¼Œå¦‚æœé‡‡ç”¨ä¸¥æ ¼çš„ä¿å®ˆæˆæœ¬å‡½æ•°ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¾ƒå·®çš„å¥–åŠ±æ•ˆç”¨ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼€æ”¾çº¦æŸå’Œæˆæœ¬å¯èƒ½å¯¼è‡´ä¸å®‰å…¨çš„ç­–ç•¥[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]ã€‚å¦ä¸€æ–¹é¢ï¼Œç­–ç•¥ç›®æ ‡ä¼˜åŒ–é¼“åŠ±ä»£ç†æ”¶æ•›äºä¸€ä¸ªé«˜åº¦å¥–åŠ±ä¸”æ»¡è¶³çº¦æŸçš„ç­–ç•¥ï¼Œä½†ç¼ºä¹ç†è®ºä¸Šçš„å®‰å…¨æ€§ä¿è¯ã€‚ç¡¬æ€§å®‰å…¨çº¦æŸå¯èƒ½é™åˆ¶æ¢ç´¢ç©ºé—´ï¼Œå¹¶å€¾å‘äºè¿‡äºä¿å®ˆçš„é©¾é©¶ç­–ç•¥[[LINK: <a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]ã€‚è¿«åˆ‡éœ€è¦ç ”ç©¶æœ‰æ•ˆç»“åˆè¿™ä¸¤ç§æ–¹æ³•ä¼˜åŠ¿çš„æ–¹æ³•ã€‚<div id="badge-148" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 148)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('148', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-149"><div class="row text-row"><div class="col-src">Importantly, less conservative but equally meaningful safety assurances depicting practically acceptable assumptions [203] is likely to become a technical priority. 2) Efficiently Learning a) Knowledge Integration: Knowledge integration, i.e., the incorporation of external knowledge into the RL training, is an important direction for improve sample efficiency. Embedding human or prior knowledge into RL policy has been shown to be effective in improving sample efficiency.

However, the performance of the current policy extracted from expert</div><div class="col-trans" id="trans-149">é‡è¦çš„æ˜¯ï¼Œè™½ç„¶æ›´åŠ ä¿å®ˆä½†åŒæ ·å…·æœ‰å®é™…æ„ä¹‰çš„å®‰å…¨ä¿è¯[[LINK: <a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>|<a href="#Figure_1" class="fig-link" onclick="highlightAsset('Figure_1'); return false;">Fig.1</a>]]å¯èƒ½ä¼šæˆä¸ºæŠ€æœ¯ä¸Šçš„ä¼˜å…ˆäº‹é¡¹ã€‚2) é«˜æ•ˆå­¦ä¹ a) çŸ¥è¯†æ•´åˆï¼šçŸ¥è¯†æ•´åˆï¼Œå³åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä¸­èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œæ˜¯æé«˜æ ·æœ¬æ•ˆç‡çš„é‡è¦æ–¹å‘ã€‚å°†äººç±»æˆ–å…ˆéªŒçŸ¥è¯†åµŒå…¥åˆ°RLç­–ç•¥ä¸­å·²è¢«è¯æ˜èƒ½å¤Ÿæœ‰æ•ˆæå‡æ ·æœ¬æ•ˆç‡ã€‚

ç„¶è€Œï¼Œå½“å‰ä»ä¸“å®¶é‚£é‡Œæå–çš„ç­–ç•¥æ€§èƒ½ä»æœ‰å¾…æ”¹è¿›[[LINK: <a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>|<a href="#Figure_2" class="fig-link" onclick="highlightAsset('Figure_2'); return false;">Fig.2</a>]]ã€‚<div id="badge-149" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 149)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('149', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-150"><div class="row text-row"><div class="col-src">[[HEADER: 2) Efficiently Learning]]</div><div class="col-trans" id="trans-150"><b>2) æ•ˆç‡å­¦ä¹ </b><div id="badge-150" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 150)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('150', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-151"><div class="row text-row"><div class="col-src">demonstrations remains a concern. RL with human feedback (RLHF) [163], [204] is a popular approach, where reward models are learned from human-evaluation data. However, it becomes difficult to maintain when dealing with extensive training tasks. This area holds promise for further exploration. Future research should focus on how to efficiently extract quality human driving knowledge and effectively combine offline and online learning methods to integrate this prior knowledge into RL models.

b) Efficient Exploration: In environments with sparse, delayed rewards, several exploration methods have yielded promising results through uncertainty guidance or intrinsic motivation. However, most related research has not been applied to MoP tasks for AD. The currently dominant RLbased MoP paradigm integrates large multi-source observation inputs, and the difficulty of exploration increases as the size and complexity of the state-action space grow.</div><div class="col-trans" id="trans-151">æ¼”ç¤ºä»ç„¶å­˜åœ¨ä¸€äº›æ‹…å¿§ã€‚å¸¦æœ‰äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰<a href="#ref-163" class="ref-link">[163]</a>ã€<a href="#ref-204" class="ref-link">[204]</a> æ˜¯ä¸€ä¸ªæµè¡Œçš„æ–¹æ³•ï¼Œå…¶ä¸­å¥–åŠ±æ¨¡å‹æ˜¯ä»äººç±»è¯„ä¼°æ•°æ®ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¤§é‡è®­ç»ƒä»»åŠ¡æ—¶ç»´æŠ¤èµ·æ¥å˜å¾—å›°éš¾ã€‚è¿™ä¸€é¢†åŸŸä¸ºæœªæ¥çš„æ¢ç´¢æä¾›äº†æ½œåŠ›ã€‚æœªæ¥çš„ç ”ç©¶åº”é›†ä¸­åœ¨å¦‚ä½•é«˜æ•ˆåœ°æå–é«˜è´¨é‡çš„äººç±»é©¾é©¶çŸ¥è¯†ï¼Œå¹¶æœ‰æ•ˆåœ°ç»“åˆç¦»çº¿å’Œåœ¨çº¿å­¦ä¹ æ–¹æ³•ï¼Œå°†è¿™äº›å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°RLæ¨¡å‹ä¸­ã€‚

b) æœ‰æ•ˆæ¢ç´¢ï¼šåœ¨ç¨€ç–ä¸”å»¶è¿Ÿå¥–åŠ±çš„ç¯å¢ƒä¸­ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æŒ‡å¯¼æˆ–å†…åœ¨åŠ¨æœºå·²ç»äº§ç”Ÿäº†å‡ ç§æœ‰æ•ˆçš„æ¢ç´¢æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç›¸å…³ç ”ç©¶å°šæœªåº”ç”¨äºADï¼ˆè‡ªä¸»é©¾é©¶ï¼‰ä¸­çš„MoPä»»åŠ¡ã€‚å½“å‰å ä¸»å¯¼åœ°ä½çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„MoPèŒƒå¼ç»“åˆäº†å¤§é‡å¤šæºè§‚æµ‹è¾“å…¥ï¼Œéšç€çŠ¶æ€-åŠ¨ä½œç©ºé—´è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œæ¢ç´¢éš¾åº¦ä¹Ÿéšä¹‹å¢å¤§ã€‚<div id="badge-151" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 151)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('151', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-152"><div class="row text-row"><div class="col-src">This is accompanied by a substantial increase in the computational cost of learning uncertainty, while the intrinsic motivation for facilitating exploration becomes more challenging to construct. How efficient exploration can be achieved in a large and complex state-action space remains unclear.

On the basis of task differentiation, one promising way is to establish a universal approach to extract the hierarchical structure of different environments, such as the recently popular parameterized action space design. In addition, representation learning [205] has been leveraged in several recent studies that include improving policy performance in environments with image states and hybrid actions.</div><div class="col-trans" id="trans-152">è¿™å°†ä¼´éšç€å­¦ä¹ ä¸ç¡®å®šæ€§æ‰€éœ€è®¡ç®—æˆæœ¬çš„æ˜¾è‘—å¢åŠ ï¼Œè€Œä¿ƒè¿›æ¢ç´¢çš„å†…åœ¨åŠ¨æœºä¹Ÿå˜å¾—æ›´åŠ éš¾ä»¥æ„å»ºã€‚åœ¨åºå¤§ä¸”å¤æ‚çš„çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­å¦‚ä½•å®ç°é«˜æ•ˆçš„æ¢ç´¢ä»ç„¶ä¸æ¸…æ¥šã€‚

åŸºäºä»»åŠ¡åˆ†åŒ–ï¼Œä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•æ˜¯å»ºç«‹ä¸€ä¸ªé€šç”¨æ–¹æ³•æ¥æå–ä¸åŒç¯å¢ƒçš„å±‚æ¬¡ç»“æ„ï¼Œä¾‹å¦‚æœ€è¿‘æµè¡Œçš„å‚æ•°åŒ–åŠ¨ä½œç©ºé—´è®¾è®¡ã€‚æ­¤å¤–ï¼Œåœ¨ä¸€äº›è¿‘æœŸçš„ç ”ç©¶ä¸­å·²ç»åˆ©ç”¨äº†è¡¨ç¤ºå­¦ä¹ <a href="#ref-205" class="ref-link">[205]</a>ï¼Œè¿™äº›ç ”ç©¶åŒ…æ‹¬åœ¨å…·æœ‰å›¾åƒçŠ¶æ€å’Œæ··åˆåŠ¨ä½œçš„ç¯å¢ƒä¸­æé«˜ç­–ç•¥æ€§èƒ½ã€‚<div id="badge-152" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 152)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('152', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-153"><div class="row text-row"><div class="col-src">3) Generalization a) Simulation Fidelity: Deviations between simulations and the real world may lead to policy bias during implementation. Therefore, focusing on the development of more realistic and diverse simulation environments becomes critical. Many early works focused on accurately modeling the kinematic and dynamic behaviors of vehicles, creating simulation scenarios that do not adequately reflect the real world.

In recent years, game engine-based simulators have provided more physically and visually realistic data. The collection of multi-modal data from the real world and then reconstructing the environment have been widely explored recently. Data-driven simulators are beginning to replace game engine-based simulators by generating synthetic data directly from real data, achieving high fidelity with low costs [206].</div><div class="col-trans" id="trans-153">3) é€šç”¨åŒ– a) æ¨¡æ‹Ÿä¿çœŸåº¦ï¼šæ¨¡æ‹Ÿä¸ç°å®ä¸–ç•Œçš„åå·®å¯èƒ½å¯¼è‡´å®æ–½è¿‡ç¨‹ä¸­å‡ºç°æ”¿ç­–åè§ã€‚å› æ­¤ï¼Œä¸“æ³¨äºå¼€å‘æ›´åŠ çœŸå®å’Œå¤šæ ·çš„æ¨¡æ‹Ÿç¯å¢ƒå˜å¾—è‡³å…³é‡è¦ã€‚è®¸å¤šæ—©æœŸçš„å·¥ä½œé›†ä¸­åœ¨å‡†ç¡®å»ºæ¨¡è½¦è¾†çš„è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦è¡Œä¸ºä¸Šï¼Œåˆ›å»ºçš„æ¨¡æ‹Ÿåœºæ™¯æœªèƒ½å……åˆ†åæ˜ ç°å®ä¸–ç•Œã€‚

è¿‘å¹´æ¥ï¼ŒåŸºäºæ¸¸æˆå¼•æ“çš„æ¨¡æ‹Ÿå™¨æä¾›äº†æ›´ç‰©ç†å’Œè§†è§‰çœŸå®çš„æ•°æ®ã€‚ä»ç°å®ä¸–ç•Œæ”¶é›†å¤šæ¨¡æ€æ•°æ®å¹¶é‡æ–°æ„å»ºç¯å¢ƒçš„åšæ³•å·²è¢«å¹¿æ³›æ¢ç´¢ã€‚æ•°æ®é©±åŠ¨çš„æ¨¡æ‹Ÿå™¨å¼€å§‹é€šè¿‡ç›´æ¥ä»çœŸå®æ•°æ®ç”Ÿæˆåˆæˆæ•°æ®æ¥å–ä»£åŸºäºæ¸¸æˆå¼•æ“çš„æ¨¡æ‹Ÿå™¨ï¼Œä»è€Œä»¥è¾ƒä½çš„æˆæœ¬å®ç°é«˜ä¿çœŸåº¦ <a href="#ref-206" class="ref-link">[206]</a>ã€‚<div id="badge-153" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 153)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('153', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-154"><div class="row text-row"><div class="col-src">Despite the high data fidelity achieved through learning from real data, online interaction capabilities remain insufficient in current technologies. Combining training in low-fidelity simulators with validation in high-fidelity real data, or collecting data in realistic field testing to continuously improve the policy are seemingly viable technical routes. In general, Closed-loop interaction model generation still needs to be further explored.

b) Reasoning Ability: RL agents lack a basic understanding of the world and often rely on extensive trial-and-error to make rational decisions and understand the correlations between different factors. They may also face difficulties in recognizing and learning invariant mechanisms from limited environments and tasks [207]. Causal models, which study how relevant features of the world interact with each other,</div><div class="col-trans" id="trans-154">å°½ç®¡é€šè¿‡å­¦ä¹ çœŸå®æ•°æ®å¯ä»¥å®ç°é«˜æ•°æ®ä¿çœŸåº¦ï¼Œä½†å½“å‰æŠ€æœ¯çš„åœ¨çº¿äº¤äº’èƒ½åŠ›ä»ç„¶ä¸è¶³ã€‚ç»“åˆåœ¨ä½ä¿çœŸåº¦æ¨¡æ‹Ÿå™¨ä¸­çš„è®­ç»ƒä¸åœ¨é«˜ä¿çœŸåº¦çœŸå®æ•°æ®ä¸­çš„éªŒè¯ï¼Œæˆ–è€…åœ¨ç°å®åœºæµ‹è¯•ä¸­æ”¶é›†æ•°æ®ä»¥æŒç»­æ”¹è¿›ç­–ç•¥ä¼¼ä¹æ˜¯å¯è¡Œçš„æŠ€æœ¯é€”å¾„ã€‚æ€»ä½“è€Œè¨€ï¼Œé—­ç¯äº¤äº’æ¨¡å‹ç”Ÿæˆä»éœ€è¿›ä¸€æ­¥æ¢ç´¢ã€‚

b) åŸå› æ¨ç†èƒ½åŠ›ï¼šRLä»£ç†ç¼ºä¹å¯¹ä¸–ç•Œçš„åŸºæœ¬ç†è§£ï¼Œå¾€å¾€éœ€è¦é€šè¿‡å¤§é‡çš„å°è¯•å’Œé”™è¯¯æ¥åšå‡ºç†æ€§çš„å†³ç­–å¹¶ç†è§£ä¸åŒå› ç´ ä¹‹é—´çš„å…³è”ã€‚å®ƒä»¬è¿˜å¯èƒ½éš¾ä»¥ä»æœ‰é™çš„ç¯å¢ƒå’Œä»»åŠ¡ä¸­è¯†åˆ«å’Œå­¦ä¹ ä¸å˜æœºåˆ¶ <a href="#ref-207" class="ref-link">[207]</a>ã€‚å› æœæ¨¡å‹ç ”ç©¶çš„æ˜¯ä¸–ç•Œç›¸å…³ç‰¹å¾ä¹‹é—´å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚<div id="badge-154" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 154)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('154', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-155"><div class="row text-row"><div class="col-src">[[HEADER: g y 3) Generalization]]</div><div class="col-trans" id="trans-155"><b>g y 3) ä¸€èˆ¬åŒ–</b><div id="badge-155" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 155)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('155', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-156"><div class="row text-row"><div class="col-src">formalize knowledge in a formative way and use invariance for effective knowledge transfer. Causal reasoning mechanisms that incorporate additional assumptions or prior knowledge to analyze and understand behaviors and their consequences enable agents to imagine and gain insights from scenarios that are missing from the collected data.

Given the success of causal reasoning in various fields such as computer vision [186], causal RL has been recognized as an understudied but significant research direction with the potential to significantly improve the performance of RL-based MoP in generalization problems. c) Evaluation of Evolution: For tasks with clear feature distinctions, such as overtaking, merging, and intersections, the generalization ability of RL-based MoP can be measured by comparing individual task metrics.</div><div class="col-trans" id="trans-156">ä»¥å½¢æˆæ€§çš„æ–¹å¼ formalize çŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨ä¸å˜æ€§è¿›è¡Œæœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ã€‚åŒ…å«é¢å¤–å‡è®¾æˆ–å…ˆéªŒçŸ¥è¯†çš„å› æœæ¨ç†æœºåˆ¶èƒ½å¤Ÿåˆ†æå’Œç†è§£è¡Œä¸ºåŠå…¶åæœï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨æ”¶é›†çš„æ•°æ®ä¸­ç¼ºå¤±çš„æƒ…æ™¯ä¸­æƒ³è±¡å¹¶è·å¾—è§è§£ã€‚

é‰´äºå› æœæ¨ç†åœ¨è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸ <a href="#ref-186" class="ref-link">[186]</a> çš„æˆåŠŸï¼Œå› æœå¼ºåŒ–å­¦ä¹ ï¼ˆcausal RLï¼‰å·²è¢«è§†ä¸ºä¸€ä¸ªå°šæœªå……åˆ†ç ”ç©¶ä½†æå…·æ½œåŠ›çš„ç ”ç©¶æ–¹å‘ï¼Œæœ‰å¯èƒ½æ˜¾è‘—æé«˜åŸºäºRLçš„æ–¹æ³•åœ¨æ³›åŒ–é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚c) è¯„ä¼°è¿›åŒ–ï¼šå¯¹äºç‰¹å¾åŒºåˆ†æ˜æ˜¾çš„ä»»åŠ¡ï¼ˆå¦‚è¶…è½¦ã€å¹¶çº¿å’Œäº¤å‰å£ï¼‰ï¼Œå¯ä»¥é€šè¿‡æ¯”è¾ƒå„ä¸ªä»»åŠ¡æŒ‡æ ‡æ¥è¡¡é‡åŸºäºRLçš„æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚<div id="badge-156" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 156)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('156', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-157"><div class="row text-row"><div class="col-src">However, real-world driving tasks are continuously changing and involve complex, coupled combinations of scenarios. Thus, the agent needs to flexibly and efficiently update and evolve to cope with edge cases encountered during driving. Having separate validation or testing phases may not effectively reflect the effectiveness of continuous evolution.

Designing a broad and novel set of metrics to enhance generalization capabilities for RL agents is valuable, and at the same time a challenge that still needs to be explored more deeply by the research community. d) LLMs Enhancement: The emergence of LLMs represents an important milestone in the field of natural language processing, and they have shown powerful capabilities in many real-world applications [208].</div><div class="col-trans" id="trans-157">ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„é©¾é©¶ä»»åŠ¡æ˜¯ä¸æ–­å˜åŒ–çš„ï¼Œå¹¶ä¸”æ¶‰åŠå¤æ‚çš„ã€ç›¸äº’å…³è”çš„æƒ…æ™¯ç»„åˆã€‚å› æ­¤ï¼Œæ™ºèƒ½ä½“éœ€è¦çµæ´»é«˜æ•ˆåœ°æ›´æ–°å’Œè¿›åŒ–ä»¥åº”å¯¹é©¾é©¶è¿‡ç¨‹ä¸­é‡åˆ°çš„è¾¹ç¼˜æƒ…å†µã€‚å•ç‹¬è®¾ç½®éªŒè¯æˆ–æµ‹è¯•é˜¶æ®µå¯èƒ½æ— æ³•æœ‰æ•ˆåæ˜ è¿ç»­è¿›åŒ–çš„æœ‰æ•ˆæ€§ã€‚

è®¾è®¡ä¸€å¥—å¹¿æ³›ä¸”æ–°é¢–çš„åº¦é‡æ ‡å‡†æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†çš„ä¸€èˆ¬åŒ–èƒ½åŠ›æ˜¯æœ‰ä»·å€¼çš„ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œä»éœ€ç ”ç©¶ç•Œè¿›ä¸€æ­¥æ·±å…¥æ¢ç´¢ã€‚d) å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¢å¼ºï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°æ ‡å¿—ç€è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ï¼Œå¹¶ä¸”å®ƒä»¬åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ› <a href="#ref-208" class="ref-link">[208]</a>ã€‚<div id="badge-157" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 157)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('157', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-158"><div class="row text-row"><div class="col-src">References [209], [210] exemplify integrating LLMs to enhance the interaction and generalization of AD systems. With extensive pre-trained knowledge and a high level of generalizability, the integration of LLMs and RL is considered a key development.

LLMs could offer several capabilities to facilitate the generalization of RL-based MoP: i) enhanceing multi-modal information understanding to provide predictions or suggestions from the context, thus reducing the need for RL agents to interact with a broad range of environments; ii) designing integrated rewards based on multi-disciplinary attributes and adaptively adjusting them based on scenario understanding, enhancing the multi-objective adaptability of learned policies;</div><div class="col-trans" id="trans-158">å‚è€ƒæ–‡çŒ®<a href="#ref-209" class="ref-link">[209]</a>ã€<a href="#ref-210" class="ref-link">[210]</a>å±•ç¤ºäº†å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆåˆ°ADç³»ç»Ÿä¸­ä»¥å¢å¼ºäº¤äº’å’Œæ³›åŒ–èƒ½åŠ›çš„æ–¹å¼ã€‚å‡­å€Ÿå¹¿æ³›é¢„è®­ç»ƒçš„çŸ¥è¯†å’Œé«˜åº¦çš„æ³›åŒ–æ€§ï¼Œå°†LLMsä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆè¢«è®¤ä¸ºæ˜¯å…³é”®çš„å‘å±•æ–¹å‘ã€‚

LLMså¯ä»¥ä¸ºåŸºäºRLçš„æ–¹æ³•æä¾›å¤šç§èƒ½åŠ›ï¼ši) æå‡å¤šæ¨¡æ€ä¿¡æ¯ç†è§£èƒ½åŠ›ï¼Œä»è€Œä»ä¸Šä¸‹æ–‡ä¸­æä¾›é¢„æµ‹æˆ–å»ºè®®ï¼Œå‡å°‘RLä»£ç†éœ€è¦ä¸å„ç§ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ï¼›ii) æ ¹æ®è·¨å­¦ç§‘å±æ€§è®¾è®¡é›†æˆå¥–åŠ±ï¼Œå¹¶æ ¹æ®åœºæ™¯ç†è§£åŠ¨æ€è°ƒæ•´è¿™äº›å¥–åŠ±ï¼Œå¢å¼ºå­¦ä¹ ç­–ç•¥çš„å¤šç›®æ ‡é€‚åº”æ€§ã€‚<div id="badge-158" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 158)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('158', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-159"><div class="row text-row"><div class="col-src">and iii) providing decision-level demonstration or guidance and further extracting driving knowledge for RL agents. The application of LLMs to AD is still in its infancy, mainly using interface calls and model fine-tuning [210], [211], with no well-developed work yet combining LLMs deeply with RL-based MoP method. However, preliminary efforts show promise in achieving advanced functionality that facilitates the generalization capabilities of RL-based MoP.

Importantly, the biases and hallucinations inherent in LLMs that may lead to distorted or inaccurate interpretations of multi-modal inputs [208], as well as the large computational costs and response times required by LLMs are currently significant challenges for applying LLMs in practice.</div><div class="col-trans" id="trans-159">å¹¶ä¸”iii) æä¾›å†³ç­–çº§çš„ç¤ºèŒƒæˆ–æŒ‡å¯¼ï¼Œå¹¶è¿›ä¸€æ­¥ä¸ºRLä»£ç†æå–é©¾é©¶çŸ¥è¯†ã€‚å°†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºADä»å¤„äºèµ·æ­¥é˜¶æ®µï¼Œä¸»è¦é€šè¿‡æ¥å£è°ƒç”¨å’Œæ¨¡å‹å¾®è°ƒ<a href="#ref-210" class="ref-link">[210]</a>ã€<a href="#ref-211" class="ref-link">[211]</a>è¿›è¡Œï¼Œå°šæœªæœ‰æ·±å…¥ç»“åˆLLMsä¸åŸºäºRLçš„æ–¹æ³•çš„MoPå·¥ä½œã€‚ç„¶è€Œï¼Œåˆæ­¥å°è¯•æ˜¾ç¤ºäº†å®ç°é«˜çº§åŠŸèƒ½çš„æ½œåŠ›ï¼Œè¿™äº›åŠŸèƒ½æœ‰åŠ©äºå¢å¼ºåŸºäºRLçš„MoPçš„ä¸€èˆ¬åŒ–èƒ½åŠ›ã€‚

é‡è¦çš„æ˜¯ï¼Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­å›ºæœ‰çš„åå·®å’Œå¹»è§‰å¯èƒ½å¯¼è‡´å¤šæ¨¡æ€è¾“å…¥çš„æ‰­æ›²æˆ–ä¸å‡†ç¡®è§£é‡Š<a href="#ref-208" class="ref-link">[208]</a>ï¼Œä»¥åŠæ‰€éœ€çš„å·¨å¤§è®¡ç®—æˆæœ¬å’Œå“åº”æ—¶é—´ç›®å‰æ˜¯å®é™…åº”ç”¨LLMsçš„é‡è¦æŒ‘æˆ˜ã€‚<div id="badge-159" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 159)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('159', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-160"><div class="row text-row"><div class="col-src">[[HEADER: VI. CONCLUSION]]</div><div class="col-trans" id="trans-160"><b>VI. ç»“è®º</b><div id="badge-160" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 160)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('160', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-161"><div class="row text-row"><div class="col-src">With its ability to explore and optimize policies in complex, dynamic decision-making tasks, reinforcement learning (RL) has emerged as a promising approach for addressing motion planning (MoP) challenges in autonomous driving (AD). This survey provides a comprehensive review of RL-based MoP for AD, focusing on lessons learned from the driving task perspective. We outline the basic theory of RL methodologies, and then delve into their applications in MoP for diverse driving tasks.

Scenario-specific features and task requirements are analyzed to illuminate their influence on RL design. On this basis, we summarize key experiences and extract insights for future implementations. Furthermore, we discuss three key frontier issues in RL-based MoP for AD, summarize how some representative emerging technologies are trying to solve them (especially over the past three years), and propose related open issues and future outlooks.</div><div class="col-trans" id="trans-161">å‡­å€Ÿå…¶åœ¨å¤æ‚ã€åŠ¨æ€å†³ç­–ä»»åŠ¡ä¸­æ¢ç´¢å’Œä¼˜åŒ–ç­–ç•¥çš„èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè§£å†³è‡ªä¸»é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸè¿åŠ¨è§„åˆ’ï¼ˆMoPï¼‰æŒ‘æˆ˜çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚æœ¬ç»¼è¿°ä»é©¾é©¶ä»»åŠ¡çš„è§’åº¦å‡ºå‘ï¼Œå¯¹åŸºäºRLçš„MoPæ–¹æ³•è¿›è¡Œäº†å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬é¦–å…ˆæ¦‚è¿°äº†RLæ–¹æ³•çš„åŸºæœ¬ç†è®ºï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å…¶åœ¨ä¸åŒé©¾é©¶ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

é€šè¿‡å¯¹ç‰¹å®šåœºæ™¯ç‰¹å¾å’Œä»»åŠ¡è¦æ±‚çš„åˆ†æï¼Œé˜æ˜äº†å®ƒä»¬å¯¹RLè®¾è®¡çš„å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ€»ç»“äº†å…³é”®ç»éªŒå¹¶æç‚¼å‡ºå¯¹æœªæ¥å®æ–½çš„è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºäºRLçš„MoPåœ¨ADé¢†åŸŸé¢ä¸´çš„ä¸‰ä¸ªå…³é”®æŠ€æœ¯é—®é¢˜ï¼Œæ€»ç»“äº†è¿‘å¹´æ¥ä¸€äº›ä»£è¡¨æ€§æ–°å…´æŠ€æœ¯å¦‚ä½•å°è¯•è§£å†³è¿™äº›é—®é¢˜ï¼ˆå°¤å…¶æ˜¯è¿‡å»ä¸‰å¹´ï¼‰ï¼Œå¹¶æå‡ºäº†ç›¸å…³å¼€æ”¾é—®é¢˜å’Œæœªæ¥å±•æœ›ã€‚<div id="badge-161" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 161)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('161', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-162"><div class="row text-row"><div class="col-src">We observe that other approaches and technologies in the field of artificial intelligence are crucial for facilitating the development of RL-based MoP. Future research directions will explore the integration of these advanced methods with frontier issues to promote RL to build AD systems with a better understanding of the world.</div><div class="col-trans" id="trans-162">æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºåŸºç¡€çš„å¤šä»£ç†ç³»ç»Ÿï¼ˆMoPï¼‰çš„å‘å±•è¿‡ç¨‹ä¸­ï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„å…¶ä»–æ–¹æ³•å’ŒæŠ€æœ¯ä¹Ÿè‡³å…³é‡è¦ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘å°†æ¢ç´¢å°†è¿™äº›å…ˆè¿›æ–¹æ³•ä¸å‰æ²¿é—®é¢˜ç›¸ç»“åˆï¼Œä»¥ä¿ƒè¿›RLæŠ€æœ¯åœ¨æ„å»ºå…·æœ‰æ›´æ·±åˆ»ä¸–ç•Œç†è§£èƒ½åŠ›çš„è‡ªä¸»å†³ç­–ç³»ç»Ÿï¼ˆADç³»ç»Ÿï¼‰æ–¹é¢çš„åº”ç”¨ã€‚<div id="badge-162" class="hint-badge">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 162)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('162', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div></div><div class="ref-section"><h2>å‚è€ƒæ–‡çŒ® (References)</h2><div class="ref-content"><br><span id="ref-1" class="ref-anchor">[1]</span> R. S. Sutton, A. G. Barto, et al., Reinforcement learning: An introduction, vol. 1. MIT press Cambridge, 1998. <br><span id="ref-2" class="ref-anchor">[2]</span> B. Zheng, S. Verma, J. Zhou, I. W. Tsang, and F. Chen, â€œImitation learning: Progress, taxonomies and challenges,â€ IEEE Trans. Neural Netw. Learn. Syst., vol. 35, no. 5, pp. 6322â€“6337, 2024. <br><span id="ref-3" class="ref-anchor">[3]</span> B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani, and P. PÂ´erez, â€œDeep reinforcement learning for autonomous driving: A survey,â€ IEEE Trans. Intell. Transp.

Syst., vol. 23, no. 6, pp. 4909â€“4926, 2022. <br><span id="ref-4" class="ref-anchor">[4]</span> D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., â€œMastering the game of go without human knowledge,â€ Nature, vol. 550, no. 7676, pp. 354â€“359, 2017. <br><span id="ref-5" class="ref-anchor">[5]</span> O. Vinyals et al., â€œGrandmaster level in starcraft ii using multi-agent reinforcement learning,â€ Nature, vol. 575, no. 7782, pp. 350â€“354, 2019. <br><span id="ref-6" class="ref-anchor">[6]</span> P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J.

Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs, et al., â€œOutracing champion gran turismo drivers with deep reinforcement learning,â€ Nature, vol. 602, no. 7896, pp. 223â€“228, 2022. <br><span id="ref-7" class="ref-anchor">[7]</span> E. Kaufmann, L. Bauersfeld, A. Loquercio, M. MÂ¨uller, V. Koltun, and D. Scaramuzza, â€œChampion-level drone racing using deep reinforcement learning,â€ Nature, vol. 620, no. 7976, pp. 982â€“987, 2023. <br><span id="ref-8" class="ref-anchor">[8]</span> S. Teng, X. Hu, P. Deng, B. Li, Y. Li, Y. Ai, D. Yang, L. Li, Z. Xuanyuan, F. Zhu, and L.

Chen, â€œMotion planning for autonomous driving: The state of the art and future perspectives,â€ IEEE Trans. Intell. Veh., vol. 8, no. 6, pp. 3692â€“3711, 2023. <br><span id="ref-9" class="ref-anchor">[9]</span> A. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad, â€œA survey of end-to-end driving: Architectures and training methods,â€ IEEE Trans. Neural Netw. Learn. Sys., vol. 33, no. 4, pp. 1364â€“1384, 2022. <br><span id="ref-10" class="ref-anchor">[10]</span> W. Schwarting, J. Alonso-Mora, and D. Rus, â€œPlanning and decisionmaking for autonomous vehicles,â€ Annu. Rev. Control Robot.

Auton. Syst., vol. 1, no. 1, pp. 187â€“210, 2018. <br><span id="ref-11" class="ref-anchor">[11]</span> Z. Li, J. Hu, B. Leng, L. Xiong, and Z. Fu, â€œAn integrated of decision making and motion planning framework for enhanced oscillation-free capability,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 6, pp. 5718â€“ 5732, 2024. <br><span id="ref-12" class="ref-anchor">[12]</span> Z. Li, L. Xiong, B. Leng, P. Xu, and Z. Fu, â€œSafe reinforcement learning of lane change decision making with risk-fused constraint,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 1313â€“1319, 2023. <br><span id="ref-13" class="ref-anchor">[13]</span> Y.

Zhang, B. Gao, L. Guo, H. Guo, and H. Chen, â€œAdaptive decisionmaking for automated vehicles under roundabout scenarios using optimization embedded reinforcement learning,â€ IEEE Trans. Neural Netw. Learn. Syst., vol. 32, no. 12, pp. 5526â€“5538, 2020. <br><span id="ref-14" class="ref-anchor">[14]</span> L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li, Z. Hu, X. Na, Z. Li, S. Teng, C. Lv, J. Wang, D. Cao, N. Zheng, and F.-Y. Wang, â€œMilestones in autonomous driving and intelligent vehicles: Survey of surveys,â€ IEEE Trans. Intell.

Veh., vol. 8, no. 2, pp. 1046â€“1056, 2023.

<br><span id="ref-15" class="ref-anchor">[15]</span> R. Zhao, Y. Li, Y. Fan, F. Gao, M. Tsukada, and Z. Gao, â€œA survey on recent advancements in autonomous driving using deep reinforcement learning: Applications, challenges, and solutions,â€ IEEE Trans. Intell. Transp. Syst., pp. 1â€“34, 2024. <br><span id="ref-16" class="ref-anchor">[16]</span> Z. Zhu and H. Zhao, â€œA survey of deep rl and il for autonomous driving policy learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 9, pp. 14043â€“14065, 2022. <br><span id="ref-17" class="ref-anchor">[17]</span> J. Wu, C. Huang, H. Huang, C. Lv, Y. Wang, and F.-Y.

Wang, â€œRecent advances in reinforcement learning-based autonomous driving behavior planning: A survey,â€ Transp. Res. Part C Emerg. Technol., vol. 164, p. 104654, 2024. <br><span id="ref-18" class="ref-anchor">[18]</span> S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, and A. Knoll, â€œA review of safe reinforcement learning: Methods, theories, and applications,â€ IEEE Trans. Pattern Anal. Mach. Intell., vol. 46, no. 12, pp. 11216â€“11235, 2024. <br><span id="ref-19" class="ref-anchor">[19]</span> W. Zhao, T. He, R. Chen, T. Wei, and C.

Liu, â€œState-wise safe reinforcement learning: A survey,â€ arXiv:2302.03122, 2023. <br><span id="ref-20" class="ref-anchor">[20]</span> J. Xing, D. Wei, S. Zhou, T. Wang, Y. Huang, and H. Chen, â€œA comprehensive study on self-learning methods and implications to autonomous driving,â€ IEEE Trans. Neural Netw. Learn. Sys., 2024. <br><span id="ref-21" class="ref-anchor">[21]</span> J. Duan, Y. Guan, S. E. Li, Y. Ren, Q. Sun, and B. Cheng, â€œDistributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors,â€ IEEE Trans. Neural Netw. Learn.

Syst., vol. 33, no. 11, pp. 6584â€“6598, 2022. <br><span id="ref-22" class="ref-anchor">[22]</span> G. Shani, J. Pineau, and R. Kaplow, â€œA survey of point-based pomdp solvers,â€ Auton. Agents and Multi-Agent Syst., vol. 27, pp. 1â€“51, 2013. <br><span id="ref-23" class="ref-anchor">[23]</span> R. A. Howard, Dynamic Programming and Markov Processes. Cambridge, MA, USA: MIT Press, 1960. <br><span id="ref-24" class="ref-anchor">[24]</span> Q. Huang, â€œModel-based or model-free, a review of approaches in reinforcement learning,â€ in International Conference on Computing and Data Science (CDS), pp. 219â€“221, 2020. <br><span id="ref-25" class="ref-anchor">[25]</span> R. S.

Sutton, â€œReinforcement learning: An introduction,â€ A Bradford Book, 2018. <br><span id="ref-26" class="ref-anchor">[26]</span> R. S. Sutton, â€œLearning to predict by the methods of temporal differences,â€ Machine learning, vol. 3, pp. 9â€“44, 1988. <br><span id="ref-27" class="ref-anchor">[27]</span> V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ Nature, vol. 518, no. 7540, pp. 529â€“533, 2015. <br><span id="ref-28" class="ref-anchor">[28]</span> H. Van Hasselt, A. Guez, and D.

Silver, â€œDeep reinforcement learning with double q-learning,â€ Proc. AAAI Conf. Artif. Intell., vol. 30, 2016. <br><span id="ref-29" class="ref-anchor">[29]</span> Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, â€œDueling network architectures for deep reinforcement learning,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 1995â€“2003, 2016. <br><span id="ref-30" class="ref-anchor">[30]</span> M. GÂ¨ok, â€œDynamic path planning via dueling double deep q-network (d3qn) with prioritized experience replay,â€ Applied Soft Computing, vol. 158, p. 111503, 2024. <br><span id="ref-31" class="ref-anchor">[31]</span> R. J.

Williams, â€œSimple statistical gradient-following algorithms for connectionist reinforcement learning,â€ Machine learning, vol. 8, pp. 229â€“256, 1992. <br><span id="ref-32" class="ref-anchor">[32]</span> J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHighdimensional continuous control using generalized advantage estimation,â€ arXiv:1506.02438, 2015. <br><span id="ref-33" class="ref-anchor">[33]</span> D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, â€œDeterministic policy gradient algorithms,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 387â€“395, 2014.

<br><span id="ref-34" class="ref-anchor">[34]</span> T. Lillicrap, â€œContinuous control with deep reinforcement learning,â€ in Proc. Int. Conf. Learn. Representations (ICLR), 2016. <br><span id="ref-35" class="ref-anchor">[35]</span> J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal policy optimization algorithms,â€ arXiv:1707.06347, 2017. <br><span id="ref-36" class="ref-anchor">[36]</span> T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 1861â€“1870, 2018. <br><span id="ref-37" class="ref-anchor">[37]</span> X. Wang, S.

Wang, X. Liang, D. Zhao, J. Huang, X. Xu, B. Dai, and Q. Miao, â€œDeep reinforcement learning: A survey,â€ IEEE Trans. Neural Netw. Learn. Sys., vol. 35, no. 4, pp. 5064â€“5078, 2024. <br><span id="ref-38" class="ref-anchor">[38]</span> J. Lai, J. Wei, and X. Chen, â€œOverview of hierarchical reinforcement learning,â€ Comput. Eng. Appl, vol. 57, no. 3, pp. 72â€“79, 2021. <br><span id="ref-39" class="ref-anchor">[39]</span> R. S. Sutton, D. Precup, and S. Singh, â€œBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,â€ Artif. Intell., vol. 112, no. 1-2, pp.

181â€“211, 1999. <br><span id="ref-40" class="ref-anchor">[40]</span> T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, â€œHierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 29, 2016.

<br><span id="ref-41" class="ref-anchor">[41]</span> L. Chen, Y. He, W. Pan, F. R. Yu, and Z. Ming, â€œA novel generalized meta hierarchical reinforcement learning method for autonomous vehicles,â€ IEEE Network, vol. 37, no. 4, pp. 230â€“236, 2023. <br><span id="ref-42" class="ref-anchor">[42]</span> J. Hao et al., â€œExploration in deep reinforcement learning: From singleagent to multiagent domain,â€ IEEE Trans. Neural Netw. Learn. Sys., 2023. <br><span id="ref-43" class="ref-anchor">[43]</span> L. M. Schmidt, J. Brosig, A. Plinge, B. M. Eskofier, and C.

Mutschler, â€œAn introduction to multi-agent reinforcement learning and review of its application to autonomous mobility,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 1342â€“1349, 2022. <br><span id="ref-44" class="ref-anchor">[44]</span> R. Zhang, J. Hou, F. Walter, S. Gu, J. Guan, F. RÂ¨ohrbein, Y. Du, P. Cai, G. Chen, and A. Knoll, â€œMulti-agent reinforcement learning for autonomous driving: A survey,â€ arXiv:2408.09675, 2024. <br><span id="ref-45" class="ref-anchor">[45]</span> R. Xu, W. Chen, H. Xiang, X. Xia, L. Liu, and J.

Ma, â€œModel-agnostic multi-agent perception framework,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1471â€“1478, 2023. <br><span id="ref-46" class="ref-anchor">[46]</span> R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, â€œMorel: Model-based offline reinforcement learning,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp. 21810â€“21823, 2020. <br><span id="ref-47" class="ref-anchor">[47]</span> T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma, â€œMopo: Model-based offline policy optimization,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp.

14129â€“14142, 2020. <br><span id="ref-48" class="ref-anchor">[48]</span> T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn, â€œCombo: Conservative offline model-based policy optimization,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, pp. 28954â€“28967, 2021. <br><span id="ref-49" class="ref-anchor">[49]</span> S. Fujimoto, D. Meger, and D. Precup, â€œOff-policy deep reinforcement learning without exploration,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 2052â€“2062, 2019. <br><span id="ref-50" class="ref-anchor">[50]</span> A. Kumar, J. Fu, M. Soh, G. Tucker, and S.

Levine, â€œStabilizing off-policy q-learning via bootstrapping error reduction,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 32, 2019. <br><span id="ref-51" class="ref-anchor">[51]</span> A. Kumar, A. Zhou, G. Tucker, and S. Levine, â€œConservative q-learning for offline reinforcement learning,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp. 1179â€“1191, 2020. <br><span id="ref-52" class="ref-anchor">[52]</span> L. Claussmann, M. Revilloud, D. Gruyer, and S. Glaser, â€œA review of motion planning for highway autonomous driving,â€ IEEE Trans. Intell. Transp. Syst., vol. 21, no. 5, pp.

1826â€“1848, 2020. <br><span id="ref-53" class="ref-anchor">[53]</span> B. Peng, Q. Sun, S. E. Li, D. Kum, Y. Yin, J. Wei, and T. Gu, â€œEndto-end autonomous driving through dueling double deep q-network,â€ Automotive Innovation, vol. 4, pp. 328â€“337, 2021. <br><span id="ref-54" class="ref-anchor">[54]</span> Q. Chen, W. Zhao, L. Li, C. Wang, and F. Chen, â€œEs-dqn: A learning method for vehicle intelligent speed control strategy under uncertain cut-in scenario,â€ IEEE Trans. Veh. Technol., vol. 71, no. 3, pp. 2472â€“ 2484, 2022. <br><span id="ref-55" class="ref-anchor">[55]</span> S. Nageshrao, H. E. Tseng, and D.

Filev, â€œAutonomous highway driving using deep reinforcement learning,â€ in Proc. IEEE Int. Conf. Syst. Man Cybern. (SMC), pp. 2326â€“2331, 2019. <br><span id="ref-56" class="ref-anchor">[56]</span> X. Zhang, L. Wu, H. Liu, Y. Wang, H. Li, and B. Xu, â€œHigh-speed ramp merging behavior decision for autonomous vehicles based on multi-agent reinforcement learning,â€ IEEE Internet of Things Journal, 2023. <br><span id="ref-57" class="ref-anchor">[57]</span> B. Sousa, T. Ribeiro, J. Coelho, G. Lopes, and A. F.

Ribeiro, â€œParallel, angular and perpendicular parking for self-driving cars using deep reinforcement learning,â€ in 2022 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), pp. 40â€“46, 2022. <br><span id="ref-58" class="ref-anchor">[58]</span> X. Tang, Y. Yang, T. Liu, X. Lin, K. Yang, and S. Li, â€œPath planning and tracking control for parking via soft actor-critic under non-ideal scenarios,â€ IEEE/CAA J. Autom. Sin., 2023. <br><span id="ref-59" class="ref-anchor">[59]</span> J. Duan, Y. Kong, C. Jiao, Y. Guan, S. E. Li, C. Chen, B. Nie, and K.

Li, â€œDistributional soft actor-critic for decision-making in on-ramp merge scenarios,â€ Automotive Innovation, vol. 7, no. 3, pp. 403â€“417, 2024. <br><span id="ref-60" class="ref-anchor">[60]</span> Z. Huang, J. Wu, and C. Lv, â€œEfficient deep reinforcement learning with imitative expert priors for autonomous driving,â€ IEEE Trans. Neural Netw. Learn. Sys., vol. 34, no. 10, pp. 7391â€“7403, 2022. <br><span id="ref-61" class="ref-anchor">[61]</span> J. Peng, S. Zhang, Y. Zhou, and Z.

Li, â€œAn integrated model for autonomous speed and lane change decision-making based on deep reinforcement learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 11, pp. 21848â€“21860, 2022. <br><span id="ref-62" class="ref-anchor">[62]</span> H. Lu, C. Lu, Y. Yu, G. Xiong, and J. Gong, â€œAutonomous overtaking for intelligent vehicles considering social preference based on hierarchical reinforcement learning,â€ Automotive Innovation, vol. 5, no. 2, pp. 195â€“208, 2022. <br><span id="ref-63" class="ref-anchor">[63]</span> Z. Qiao, J. Schneider, and J. M.

Dolan, â€œBehavior planning at urban intersections through hierarchical reinforcement learning,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 2667â€“2673, 2021.

<br><span id="ref-64" class="ref-anchor">[64]</span> Z. Gu, L. Gao, H. Ma, S. E. Li, S. Zheng, W. Jing, and J. Chen, â€œSafe-state enhancement method for autonomous driving via direct hierarchical reinforcement learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 9, pp. 9966â€“9983, 2023. <br><span id="ref-65" class="ref-anchor">[65]</span> T. Shi, Y. Ai, O. ElSamadisy, and B. Abdulhai, â€œBilateral deep reinforcement learning approach for better-than-human car-following,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 3986â€“3992, 2022. <br><span id="ref-66" class="ref-anchor">[66]</span> S. Chen, M. Wang, W. Song, Y. Yang, and M.

Fu, â€œMulti-agent reinforcement learning-based decision making for twin-vehicles cooperative driving in stochastic dynamic highway environments,â€ IEEE Trans. Veh. Technol., vol. 72, no. 10, pp. 12615â€“12627, 2023. <br><span id="ref-67" class="ref-anchor">[67]</span> M. Zhu, Y. Wang, Z. Pu, J. Hu, X. Wang, and R. Ke, â€œSafe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving,â€ Transp. Res. Part C Emerg. Technol., vol. 117, p. 102662, 2020. <br><span id="ref-68" class="ref-anchor">[68]</span> A.

Kendall et al., â€œLearning to drive in a day,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 8248â€“8254, 2019. <br><span id="ref-69" class="ref-anchor">[69]</span> Y. Tian, X. Cao, K. Huang, C. Fei, Z. Zheng, and X. Ji, â€œLearning to drive like human beings: A method based on deep reinforcement learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 7, pp. 6357â€“ 6367, 2021. <br><span id="ref-70" class="ref-anchor">[70]</span> H. Jula, E. B. Kosmatopoulos, and P. A. Ioannou, â€œCollision avoidance analysis for lane changing and merging,â€ IEEE Trans. veh. tech., vol. 49, no. 6, pp.

2295â€“2308, 2000. <br><span id="ref-71" class="ref-anchor">[71]</span> Y. Yu, C. Lu, L. Yang, Z. Li, F. Hu, and J. Gong, â€œHierarchical reinforcement learning combined with motion primitives for automated overtaking,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp. 1â€“6, 2020. <br><span id="ref-72" class="ref-anchor">[72]</span> N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and A. R. Girard, â€œGame theoretic modeling of driver and vehicle interactions for verification and validation of autonomous vehicle control systems,â€ IEEE Trans. Control Syst. Tech., vol. 26, no. 5, pp.

1782â€“1797, 2018. <br><span id="ref-73" class="ref-anchor">[73]</span> S. Li, C. Wei, and Y. Wang, â€œCombining decision making and trajectory planning for lane changing using deep reinforcement learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 9, pp. 16110â€“16136, 2022. <br><span id="ref-74" class="ref-anchor">[74]</span> M. U. Yavas, T. Kumbasar, and N. K. Ure, â€œA real-world reinforcement learning framework for safe and human-like tactical decision-making,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 11, pp. 11773â€“11784, 2023. <br><span id="ref-75" class="ref-anchor">[75]</span> Z. Qi, T. Wang, J. Chen, D. Narang, Y.

Wang, and H. Yang, â€œLearningbased path planning and predictive control for autonomous vehicles with low-cost positioning,â€ IEEE Trans. Intell. Veh., vol. 8, no. 2, pp. 1093â€“1104, 2023. <br><span id="ref-76" class="ref-anchor">[76]</span> M. Kaushik, N. Singhania, and K. M. Krishna, â€œParameter sharing reinforcement learning architecture for multi agent driving,â€ in Proceedings of the 2019 4th International Conference on Advances in Robotics, pp. 1â€“7, 2019. <br><span id="ref-77" class="ref-anchor">[77]</span> T. Feng, X. Xu, X. Zhang, and X.

Zhang, â€œAn improved ddpg algorithm with barrier function for lane-change decision-making of intelligent vehicles,â€ in Proc. Artif. Intell.: First CAAI Int. Conf. (CICAI), pp. 127â€“ 139, 2021. <br><span id="ref-78" class="ref-anchor">[78]</span> X. Lu, F. X. Fan, and T. Wang, â€œAction and trajectory planning for urban autonomous driving with hierarchical reinforcement learning,â€ arXiv:2306.15968, 2023. <br><span id="ref-79" class="ref-anchor">[79]</span> X. Wang and M. Althoff, â€œSafe reinforcement learning for automated vehicles via online reachability analysis,â€ IEEE Trans. Intell. Veh., pp.

1â€“15, 2023. <br><span id="ref-80" class="ref-anchor">[80]</span> W. Huang, F. Braghin, and Z. Wang, â€œLearning to drive via apprenticeship learning and deep reinforcement learning,â€ in Proc. IEEE Int. Conf. Tools Artif. Intell. (ICTAI), pp. 1536â€“1540, 2019. <br><span id="ref-81" class="ref-anchor">[81]</span> R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, â€œRobustness and adaptability of reinforcement learning-based cooperative autonomous driving in mixed-autonomy traffic,â€ IEEE Open J. Intell. Transp. Syst., vol. 3, pp. 397â€“410, 2022. <br><span id="ref-82" class="ref-anchor">[82]</span> S. Hwang, K. Lee, H. Jeon, and D.

Kum, â€œAutonomous vehicle cutin algorithm for lane-merging scenarios via policy-based reinforcement learning nested within finite-state machine,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 10, pp. 17594â€“17606, 2022. <br><span id="ref-83" class="ref-anchor">[83]</span> L. Ye, Z. Wen, H. Zhang, and B. Ran, â€œA general drl-based framework using mode-selection tangent time projection for mixed on-ramp merging,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 3207â€“3214, 2023. <br><span id="ref-84" class="ref-anchor">[84]</span> T. Tram, I. Batkovic, M. Ali, and J.

SjÂ¨oberg, â€œLearning when to drive in intersections by combining reinforcement learning and model predictive control,â€ in Proc. IEEE Intell.t Transp. Syst.Conf. (ITSC), pp. 3263â€“3268, 2019.

<br><span id="ref-85" class="ref-anchor">[85]</span> Y. Lin, J. McPhee, and N. L. Azad, â€œAnti-jerk on-ramp merging using deep reinforcement learning,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp. 7â€“14, 2020. <br><span id="ref-86" class="ref-anchor">[86]</span> X. He, B. Lou, H. Yang, and C. Lv, â€œRobust decision making for autonomous vehicles at highway on-ramps: A constrained adversarial reinforcement learning approach,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 4, pp. 4103â€“4113, 2022. <br><span id="ref-87" class="ref-anchor">[87]</span> C. Jiang, H. Liu, C. Qiu, S. Zhang, and W.

Zhuang, â€œRamp merging sequence and trajectory optimization for connected and autonomous vehicles using deep reinforcement learning,â€ in Proc. IEEE Int. Conf. Adv. Motion Control (AMC), pp. 1â€“7, 2024. <br><span id="ref-88" class="ref-anchor">[88]</span> B. M. Albaba and Y. Yildiz, â€œDriver modeling through deep reinforcement learning and behavioral game theory,â€ IEEE Transactions on Control Systems Technology, vol. 30, no. 2, pp. 885â€“892, 2021. <br><span id="ref-89" class="ref-anchor">[89]</span> D. Chen, M. R. Hajidavalloo, Z. Li, K. Chen, Y. Wang, L. Jiang, and Y.

Wang, â€œDeep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 11, pp. 11623â€“11638, 2023. <br><span id="ref-90" class="ref-anchor">[90]</span> X. Zhang, L. Wu, H. Liu, Y. Wang, H. Li, and B. Xu, â€œHigh-speed ramp merging behavior decision for autonomous vehicles based on multiagent reinforcement learning,â€ IEEE Internet of Things Journal, vol. 10, no. 24, pp. 22664â€“22672, 2023. <br><span id="ref-91" class="ref-anchor">[91]</span> E. Leurent and J.

Mercat, â€œSocial attention for autonomous decisionmaking in dense traffic,â€ arXiv:1911.12250, 2019. <br><span id="ref-92" class="ref-anchor">[92]</span> W. Xiao, Y. Yang, X. Mu, Y. Xie, X. Tang, D. Cao, and T. Liu, â€œDecision-making for autonomous vehicles in random task scenarios at unsignalized intersection using deep reinforcement learning,â€ IEEE Trans. Veh. Technol., vol. 73, no. 6, pp. 7812â€“7825, 2024. <br><span id="ref-93" class="ref-anchor">[93]</span> Z. Qiao, K. Muelling, J. Dolan, P. Palanisamy, and P.

Mudalige, â€œPomdp and hierarchical options mdp with continuous actions for autonomous driving at intersections,â€ in Proc. Intell. Transp. Syst. Conf. (ITSC), pp. 2377â€“2382, 2018. <br><span id="ref-94" class="ref-anchor">[94]</span> J. Li, L. Sun, J. Chen, M. Tomizuka, and W. Zhan, â€œA safe hierarchical planning framework for complex driving scenarios based on reinforcement learning,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 2660â€“2666, 2021. <br><span id="ref-95" class="ref-anchor">[95]</span> Y. Ren, J. Jiang, G. Zhan, S. E. Li, C. Chen, K. Li, and J.

Duan, â€œSelflearned intelligence for integrated decision and control of automated vehicles at signalized intersections,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 12, pp. 24145â€“24156, 2022. <br><span id="ref-96" class="ref-anchor">[96]</span> X. He, J. Wu, Z. Huang, Z. Hu, J. Wang, A. Sangiovanni-Vincentelli, and C. Lv, â€œFear-neuro-inspired reinforcement learning for safe autonomous driving,â€ IEEE Trans. Pattern Anal. Mach. Intell., 2023. <br><span id="ref-97" class="ref-anchor">[97]</span> S. Wang, Z. Wang, R. Jiang, R. Yan, and L.

Du, â€œTrajectory jerking suppression for mixed traffic flow at a signalized intersection: A trajectory prediction based deep reinforcement learning method,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 10, pp. 18989â€“19000, 2022. <br><span id="ref-98" class="ref-anchor">[98]</span> Y. Liu, Y. Gao, Q. Zhang, D. Ding, and D. Zhao, â€œMulti-task safe reinforcement learning for navigating intersections in dense traffic,â€ J. Frankl. Inst., vol. 360, no. 17, pp. 13737â€“13760, 2023. <br><span id="ref-99" class="ref-anchor">[99]</span> J. Jiang, Y. Ren, Y. Guan, S. E. Li, Y. Yin, D. Yu, and X.

Jin, â€œIntegrated decision and control at multi-lane intersections with mixed traffic flow,â€ in Journal of Physics: Conference Series, vol. 2234, p. 012015, 2022. <br><span id="ref-100" class="ref-anchor">[100]</span> Z. Bai, P. Hao, W. Shangguan, B. Cai, and M. J. Barth, â€œHybrid reinforcement learning-based eco-driving strategy for connected and automated vehicles at signalized intersections,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 9, pp. 15850â€“15863, 2022. <br><span id="ref-101" class="ref-anchor">[101]</span> G.-P. Antonio and C.

Maria-Dolores, â€œMulti-agent deep reinforcement learning to manage connected autonomous vehicles at tomorrowâ€™s intersections,â€ IEEE Trans. Veh. Technol., vol. 71, no. 7, pp. 7033â€“ 7043, 2022. <br><span id="ref-102" class="ref-anchor">[102]</span> R. Zhao, Y. Li, F. Gao, Z. Gao, and T. Zhang, â€œMulti-agent constrained policy optimization for conflict-free management of connected autonomous vehicles at unsignalized intersections,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 6, pp. 5374â€“5388, 2024. <br><span id="ref-103" class="ref-anchor">[103]</span> J. Hu, Y. Feng, S. Li, H. Wang, J.

So, and J. Zheng, â€œMirroring the parking target: An optimal-control-based parking motion planner with strengthened parking reliability and faster parking completion,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 11, pp. 16157â€“16170, 2024. <br><span id="ref-104" class="ref-anchor">[104]</span> J. Shi, K. Li, C. Piao, J. Gao, and L. Chen, â€œModel-based predictive control and reinforcement learning for planning vehicle-parking trajectories for vertical parking spaces,â€ Sensors, vol. 23, no. 16, 2023. <br><span id="ref-105" class="ref-anchor">[105]</span> J. Chen, B. Yuan, and M.

Tomizuka, â€œModel-free deep reinforcement learning for urban autonomous driving,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 2765â€“2771, 2019. <br><span id="ref-106" class="ref-anchor">[106]</span> L. Anzalone, P. Barra, S. Barra, A. Castiglione, and M. Nappi, â€œAn end-to-end curriculum learning approach for autonomous driving scenarios,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 10, pp. 19817â€“ 19826, 2022. <br><span id="ref-107" class="ref-anchor">[107]</span> G. Zhan, Y. Jiang, S. E. Li, Y. Lyu, X. Zhang, and Y.

Yin, â€œA transformation-aggregation framework for state representation of autonomous driving systems,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 7, pp. 7311â€“7322, 2024. <br><span id="ref-108" class="ref-anchor">[108]</span> Y.-L. Jin, Z.-Y. Ji, D. Zeng, and X.-P. Zhang, â€œVwp:an efficient drlbased autonomous driving model,â€ IEEE Transactions on Multimedia, vol. 26, pp. 2096â€“2108, 2024. <br><span id="ref-109" class="ref-anchor">[109]</span> Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H.

Li, â€œPlanning-oriented autonomous driving,â€ arXiv:2212.10156, 2023. <br><span id="ref-110" class="ref-anchor">[110]</span> Y. Song, H. Lin, E. Kaufmann, P. DÂ¨urr, and D. Scaramuzza, â€œAutonomous overtaking in gran turismo sport using curriculum reinforcement learning,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 9403â€“9409, 2021. <br><span id="ref-111" class="ref-anchor">[111]</span> R. Banerjee, P. Ray, and M.

Campbell, â€œImproving environment robustness of deep reinforcement learning approaches for autonomous racing using bayesian optimization-based curriculum learning,â€ arXiv:2312.10557, 2023. <br><span id="ref-112" class="ref-anchor">[112]</span> S. Huang, X. Wu, and G. Huang, â€œDeep reinforcement learning-based multi-objective path planning on the off-road terrain environment for ground vehicles,â€ arXiv:2305.13783, 2023. <br><span id="ref-113" class="ref-anchor">[113]</span> J. Zhao, Y. Wang, Y. Zhang, M. Wu, and R.

Li, â€œA multi-objective deep reinforcement learning method for path planning in shovel loading scenario,â€ in Proc. IEEE Int. Conf. Unmanned Syst., pp. 913â€“918, 2023. <br><span id="ref-114" class="ref-anchor">[114]</span> Y. Zhang and C. Li, â€œOn hierarchical path planning based on deep reinforcement learning in off- road environments,â€ in Proc. Int. Conf. Autom., Robot. Applications (ICARA), pp. 461â€“465, 2024. <br><span id="ref-115" class="ref-anchor">[115]</span> S. J. Wang, H. Zhu, and A. M.

Johnson, â€œPay attention to how you drive: Safe and adaptive model-based reinforcement learning for off-road driving,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 16954â€“16960, 2024. <br><span id="ref-116" class="ref-anchor">[116]</span> J. Shi, T. Zhang, Z. Zong, S. Chen, J. Xin, and N. Zheng, â€œTaskdriven autonomous driving: Balanced strategies integrating curriculum reinforcement learning and residual policy,â€ IEEE Robot. Autom. Lett., 2024. <br><span id="ref-117" class="ref-anchor">[117]</span> J. Shi, T. Zhang, J. Zhan, S. Chen, J. Xin, and N.

Zheng, â€œEfficient lanechanging behavior planning via reinforcement learning with imitation learning initialization,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp. 1â€“8, 2023. <br><span id="ref-118" class="ref-anchor">[118]</span> K. Yuan, Y. Huang, S. Yang, Z. Zhou, Y. Wang, D. Cao, and H. Chen, â€œEvolutionary decision-making and planning for autonomous driving based on safe and rational exploration and exploitation,â€ Engineering, vol. 33, pp. 108â€“120, 2024. <br><span id="ref-119" class="ref-anchor">[119]</span> H. Tian, K. Reddy, Y. Feng, M. Quddus, Y. Demiris, and P.

Angeloudis, â€œEnhancing autonomous vehicle training with language model integration and critical scenario generation,â€ arXiv:2404.08570, 2024. <br><span id="ref-120" class="ref-anchor">[120]</span> K. B. Naveed, Z. Qiao, and J. M. Dolan, â€œTrajectory planning for autonomous vehicles using hierarchical reinforcement learning,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 601â€“606, 2021. <br><span id="ref-121" class="ref-anchor">[121]</span> H. Shu, T. Liu, X. Mu, and D.

Cao, â€œDriving tasks transfer using deep reinforcement learning for decision-making of autonomous vehicles in unsignalized intersection,â€ IEEE Trans. Veh. Technol., vol. 71, no. 1, pp. 41â€“52, 2022. <br><span id="ref-122" class="ref-anchor">[122]</span> G. Jin, Z. Li, B. Leng, W. Han, and L. Xiong, â€œStability enhanced hierarchical reinforcement learning for autonomous driving with parameterized trajectory action,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), 2024. <br><span id="ref-123" class="ref-anchor">[123]</span> L. Chen, Y. He, Q. Wang, W. Pan, and Z.

Ming, â€œJoint optimization of sensing, decision-making and motion-controlling for autonomous vehicles: A deep reinforcement learning approach,â€ IEEE Trans. Veh. Technol., vol. 71, no. 5, pp. 4642â€“4654, 2022. <br><span id="ref-124" class="ref-anchor">[124]</span> J. Xiong, Q. Wang, Z. Yang, P. Sun, L. Han, Y. Zheng, H. Fu, T. Zhang, J. Liu, and H. Liu, â€œParametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space,â€ arXiv:1810.06394, 2018. <br><span id="ref-125" class="ref-anchor">[125]</span> K. Yuan, Y. Huang, S. Yang, M. Wu, D. Cao, Q.

Chen, and H. Chen, â€œEvolutionary decision-making and planning for autonomous driving: A hybrid augmented intelligence framework,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 7, pp. 7339â€“7351, 2024. <br><span id="ref-126" class="ref-anchor">[126]</span> Z. Li, G. Jin, R. Yu, B. Leng, and L. Xiong, â€œInteraction-aware deep reinforcement learning approach based on hybrid parameterized action space for autonomous driving,â€ in Proc. SAE Intell. Connected Veh. Symposium (SAE ICVS), 2024.

<br><span id="ref-127" class="ref-anchor">[127]</span> W. B. Knox, A. Allievi, H. Banzhaf, F. Schmitt, and P. Stone, â€œReward (mis)design for autonomous driving,â€ Artificial Intelligence, vol. 316, p. 103829, 2023. <br><span id="ref-128" class="ref-anchor">[128]</span> G. Li, Y. Yang, S. Li, X. Qu, N. Lyu, and S. E. Li, â€œDecision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness,â€ Transp. Res. Part C Emerg. Technol., vol. 134, p. 103452, 2022. <br><span id="ref-129" class="ref-anchor">[129]</span> M. Zhu, D. Piga, and A.

Bemporad, â€œC-GLISp: Preference-based global optimization under unknown constraints with applications to controller calibration,â€ IEEE Trans. Control Syst. Tech., vol. 30, pp. 2176â€“2187, Sept. 2022. <br><span id="ref-130" class="ref-anchor">[130]</span> Z. Wu, L. Sun, W. Zhan, C. Yang, and M. Tomizuka, â€œEfficient sampling-based maximum entropy inverse reinforcement learning with application to autonomous driving,â€ IEEE Robot. Autom. Lett., vol. 5, no. 4, pp. 5355â€“5362, 2020. <br><span id="ref-131" class="ref-anchor">[131]</span> X. Wen, S. Jian, and D.

He, â€œModeling the effects of autonomous vehicles on human driver car-following behaviors using inverse reinforcement learning,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 12, pp. 13903â€“13915, 2023. <br><span id="ref-132" class="ref-anchor">[132]</span> S. Mysore, G. Cheng, Y. Zhao, K. Saenko, and M. Wu, â€œMulti-critic actor learning: Teaching RL policies to act with style,â€ in Proc. Int. Conf. Learn. Representations (ICLR), 2022. <br><span id="ref-133" class="ref-anchor">[133]</span> W. Yuan, M. Yang, Y. He, C. Wang, and B.

Wang, â€œMulti-reward architecture based reinforcement learning for highway driving policies,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 3810â€“3815, 2019. <br><span id="ref-134" class="ref-anchor">[134]</span> R. T. Icarte, T. Q. Klassen, R. Valenzano, and S. A. McIlraith, â€œReward machines: Exploiting reward function structure in reinforcement learning,â€ J. Artif. Intell. Res., vol. 73, pp. 173â€“208, 2022. <br><span id="ref-135" class="ref-anchor">[135]</span> S. J. Russell and P. Norvig, Artificial intelligence: a modern approach. Pearson, 2016. <br><span id="ref-136" class="ref-anchor">[136]</span> L.-C. Wu, Z. Zhang, S.

Haesaert, Z. Ma, and Z. Sun, â€œRisk-aware reward shaping of reinforcement learning agents for autonomous driving,â€ in Proc. Annu. Conf. IEEE Ind. Electronics Soc., pp. 1â€“6, 2023. <br><span id="ref-137" class="ref-anchor">[137]</span> H. Ma, C. Liu, S. E. Li, S. Zheng, W. Sun, and J. Chen, â€œLearn zeroconstraint-violation safe policy in model-free constrained reinforcement learning,â€ IEEE Trans.Neural Netw. Learn. Syst., vol. early access, 2024. <br><span id="ref-138" class="ref-anchor">[138]</span> X. Chen, B. Xu, M. Hu, Y. Bian, Y. Li, and X.

Xu, â€œSafe efficient policy optimization algorithm for unsignalized intersection navigation,â€ IEEE/CAA J. Autom. Sin., vol. 11, no. 9, pp. 2011â€“2026, 2024. <br><span id="ref-139" class="ref-anchor">[139]</span> L. Wen, J. Duan, S. E. Li, S. Xu, and H. Peng, â€œSafe reinforcement learning for autonomous vehicles through parallel constrained policy optimization,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 1â€“7, 2020. <br><span id="ref-140" class="ref-anchor">[140]</span> T. J. Perkins and A. G.

Barto, â€œLyapunov design for safe reinforcement learning,â€ Journal of Machine Learning Research, vol. 3, no. Dec, pp. 803â€“832, 2002. <br><span id="ref-141" class="ref-anchor">[141]</span> A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath, and P. Tabuada, â€œControl barrier functions: Theory and applications,â€ in 2019 18th European control conference (ECC), pp. 3420â€“3431, 2019. <br><span id="ref-142" class="ref-anchor">[142]</span> L. Zhang, R. Zhang, T. Wu, R. Weng, M. Han, and Y.

Zhao, â€œSafe reinforcement learning with stability guarantee for motion planning of autonomous vehicles,â€ IEEE Trans. Neural Netw. Learn. Sys., vol. 32, no. 12, pp. 5435â€“5444, 2021. <br><span id="ref-143" class="ref-anchor">[143]</span> S. Udatha, Y. Lyu, and J. Dolan, â€œReinforcement learning with probabilistically safe control barrier functions for ramp merging,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 5625â€“5630, 2023. <br><span id="ref-144" class="ref-anchor">[144]</span> Y. Yang, Y. Jiang, Y. Liu, J. Chen, and S. E.

Li, â€œModel-free safe reinforcement learning through neural barrier certificate,â€ IEEE Robot. Autom. Lett., vol. 8, no. 3, pp. 1295â€“1302, 2023. <br><span id="ref-145" class="ref-anchor">[145]</span> S. Nageshrao et al., â€œRobust ai driving strategy for autonomous vehicles,â€ in AI-enabled Technologies for Autonomous and Connected Vehicles, pp. 161â€“212, Springer, 2022. <br><span id="ref-146" class="ref-anchor">[146]</span> Z. Zhang, Q. Liu, Y. Li, K. Lin, and L. Li, â€œSafe reinforcement learning in autonomous driving with epistemic uncertainty estimation,â€ IEEE Trans. Intell. Transp. Syst., 2024.

<br><span id="ref-147" class="ref-anchor">[147]</span> K. Stachowicz and S. Levine, â€œRacer: Epistemic risk-sensitive rl enables fast driving with fewer crashes,â€ arXiv:2405.04714, 2024. <br><span id="ref-148" class="ref-anchor">[148]</span> A. Baheri, S. Nageshrao, H. E. Tseng, I. Kolmanovsky, A. Girard, and D. Filev, â€œDeep reinforcement learning with enhanced safety for autonomous highway driving,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp. 1550â€“1555, 2020. <br><span id="ref-149" class="ref-anchor">[149]</span> S. Gu, G. Chen, L. Zhang, J. Hou, Y. Hu, and A.

Knoll, â€œConstrained reinforcement learning for vehicle motion planning with topological reachability analysis,â€ Robotics, vol. 11, no. 4, p. 81, 2022.

<br><span id="ref-150" class="ref-anchor">[150]</span> X. Wang, â€œEnsuring safety of learning-based motion planners using control barrier functions,â€ IEEE Robot. Autom. Lett., vol. 7, no. 2, pp. 4773â€“4780, 2022. <br><span id="ref-151" class="ref-anchor">[151]</span> B. Gangopadhyay, H. Soora, and P. Dasgupta, â€œHierarchical programtriggered reinforcement learning agents for automated driving,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 8, pp. 10902â€“10911, 2021. <br><span id="ref-152" class="ref-anchor">[152]</span> C.-J. Hoel, K. Wolff, and L.

Laine, â€œEnsemble quantile networks: Uncertainty-aware reinforcement learning with applications in autonomous driving,â€ IEEE Trans. Intell. Transp. Syst., vol. 24, no. 6, pp. 6030â€“6041, 2023. <br><span id="ref-153" class="ref-anchor">[153]</span> K. Yang, X. Tang, S. Qiu, S. Jin, Z. Wei, and H. Wang, â€œTowards robust decision-making for autonomous driving on highway,â€ IEEE Trans. Veh. Technol., vol. 72, no. 9, pp. 11251â€“11263, 2023. <br><span id="ref-154" class="ref-anchor">[154]</span> H. Krasowski, Y. Zhang, and M.

Althoff, â€œSafe reinforcement learning for urban driving using invariably safe braking sets,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 2407â€“2414, 2022. <br><span id="ref-155" class="ref-anchor">[155]</span> N. Li, Y. Li, and I. Kolmanovsky, â€œA unified safety protection and extension governor,â€ IEEE Transactions on Automatic Control, 2024. <br><span id="ref-156" class="ref-anchor">[156]</span> Y. Li, N. Li, H. E. Tseng, A. Girard, D. Filev, and I. Kolmanovsky, â€œSafe reinforcement learning using robust action governor,â€ in Learning for Dynamics and Control, pp.

1093â€“1104, PMLR, 2021. <br><span id="ref-157" class="ref-anchor">[157]</span> B. Tearle, K. P. Wabersich, A. Carron, and M. N. Zeilinger, â€œA predictive safety filter for learning-based racing control,â€ IEEE Robot. Autom. Lett., vol. 6, no. 4, pp. 7635â€“7642, 2021. <br><span id="ref-158" class="ref-anchor">[158]</span> S. Alighanbari and N. L. Azad, â€œDeep reinforcement learning with nmpc assistance nash switching for urban autonomous driving,â€ IEEE Trans. Intell. Veh., vol. 8, no. 3, pp. 2604â€“2615, 2022. <br><span id="ref-159" class="ref-anchor">[159]</span> J. Lu, G. Alcan, and V.

Kyrki, â€œIntegrating expert guidance for efficient learning of safe overtaking in autonomous driving using deep reinforcement learning,â€ arXiv:2308.09456, 2023. <br><span id="ref-160" class="ref-anchor">[160]</span> T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al., â€œDeep q-learning from demonstrations,â€ in AAAI Conf. Artif. Intell., vol. 32, 2018. <br><span id="ref-161" class="ref-anchor">[161]</span> H. Liu, Z. Huang, J. Wu, and C.

Lv, â€œImproved deep reinforcement learning with expert demonstrations for urban autonomous driving,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp. 921â€“928, 2022. <br><span id="ref-162" class="ref-anchor">[162]</span> Y. Gao, H. Xu, J. Lin, F. Yu, S. Levine, and T. Darrell, â€œReinforcement learning from imperfect demonstrations,â€ arXiv:1802.05313, 2018. <br><span id="ref-163" class="ref-anchor">[163]</span> J. Wu, Z. Huang, W. Huang, and C. Lv, â€œPrioritized experience-based reinforcement learning with human guidance for autonomous driving,â€ IEEE Trans. Neural Netw. Learn.

Sys., vol. 35, no. 1, pp. 855â€“869, 2024. <br><span id="ref-164" class="ref-anchor">[164]</span> I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, C. Fu, C. Ma, J. Jiao, et al., â€œJump-start reinforcement learning,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 34556â€“34583, 2023. <br><span id="ref-165" class="ref-anchor">[165]</span> Z. Huang, J. Wu, and C. Lv, â€œEfficient deep reinforcement learning with imitative expert priors for autonomous driving,â€ IEEE Trans. Neural Netw. Learn. Sys., vol. 34, no. 10, pp. 7391â€“7403, 2023. <br><span id="ref-166" class="ref-anchor">[166]</span> J. Li, X. Liu, B. Zhu, J. Jiao, M. Tomizuka, C.

Tang, and W. Zhan, â€œGuided online distillation: Promoting safe reinforcement learning by offline demonstration,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 7447â€“7454, 2024. <br><span id="ref-167" class="ref-anchor">[167]</span> T. Ota, â€œDecision mamba: Reinforcement learning via sequence modeling with selective state spaces,â€ arXiv:2403.19925, 2024. <br><span id="ref-168" class="ref-anchor">[168]</span> M. Guo and M. BÂ¨urger, â€œGeometric task networks: Learning efficient and explainable skill coordination for object manipulation,â€ IEEE Transactions on Robotics, vol. 38, no. 3, pp.

1723â€“1734, 2022. <br><span id="ref-169" class="ref-anchor">[169]</span> X. Wang, Y. Chen, and W. Zhu, â€œA survey on curriculum learning,â€ IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4555â€“4576, 2022. <br><span id="ref-170" class="ref-anchor">[170]</span> J. Shi, T. Zhang, Z. Zong, S. Chen, J. Xin, and N. Zheng, â€œTaskdriven autonomous driving: Balanced strategies integrating curriculum reinforcement learning and residual policy,â€ IEEE Robot. Autom. Lett., vol. 9, no. 11, pp. 9454â€“9461, 2024. <br><span id="ref-171" class="ref-anchor">[171]</span> H. Niu, Y. Xu, X. Jiang, and J.

Hu, â€œContinual driving policy optimization with closed-loop individualized curricula,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 6850â€“6857, 2024. <br><span id="ref-172" class="ref-anchor">[172]</span> H. Wang, H. Zhang, L. Li, Z. Kan, and Y. Song, â€œTask-driven reinforcement learning with action primitives for long-horizon manipulation skills,â€ IEEE Trans. Cybern., vol. 54, no. 8, pp. 4513â€“4526, 2024. <br><span id="ref-173" class="ref-anchor">[173]</span> F. L. Da Silva and A. H. R. Costa, â€œA survey on transfer learning for multiagent reinforcement learning systems,â€ J. Artif. Intell.

Res., vol. 64, pp. 645â€“703, 2019. <br><span id="ref-174" class="ref-anchor">[174]</span> Z. Yan and C. Wu, â€œReinforcement learning for mixed autonomy intersections,â€ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 2089â€“ 2094, 2021.

<br><span id="ref-175" class="ref-anchor">[175]</span> X. Xiao, B. Liu, G. Warnell, and P. Stone, â€œMotion planning and control for mobile robot navigation using machine learning: a survey,â€ Autonomous Robots, vol. 46, no. 5, pp. 569â€“597, 2022. <br><span id="ref-176" class="ref-anchor">[176]</span> Y. Xia, S. Liu, Q. Yu, L. Deng, Y. Zhang, H. Su, and K. Zheng, â€œParameterized decision-making with multi-modality perception for autonomous driving,â€ in Proc. IEEE Int. Conf. Data Eng. (ICDE), pp. 4463â€“4476, 2024. <br><span id="ref-177" class="ref-anchor">[177]</span> M. Dalal, D. Pathak, and R. R.

Salakhutdinov, â€œAccelerating robotic reinforcement learning via parameterized action primitives,â€ in Proc. Adv. Neural Inf. Proces. Syst., vol. 34, pp. 21847â€“21859, 2021. <br><span id="ref-178" class="ref-anchor">[178]</span> K. Lee, M. Laskin, A. Srinivas, and P. Abbeel, â€œSunrise: A simple unified framework for ensemble learning in deep reinforcement learning,â€ in Proc. Int. Conf. Mach. Learn. (ICML), pp. 6131â€“6141, 2021. <br><span id="ref-179" class="ref-anchor">[179]</span> T. Kanazawa, H. Wang, and C.

Gupta, â€œDistributional actor-critic ensemble for uncertainty-aware continuous control,â€ in Proc. Int. Joint Conf. Neural Networks, pp. 1â€“10, 2022. <br><span id="ref-180" class="ref-anchor">[180]</span> Z. Ma, X. Liu, and Y. Huang, â€œUnsupervised reinforcement learning for multi-task autonomous driving: Expanding skills and cultivating curiosity,â€ IEEE Trans. Intell. Transp. Syst., vol. 25, no. 10, pp. 14209â€“ 14219, 2024. <br><span id="ref-181" class="ref-anchor">[181]</span> Y. Wu, S. Liao, X. Liu, Z. Li, and R.

Lu, â€œDeep reinforcement learning on autonomous driving policy with auxiliary critic network,â€ IEEE Trans. Neural Netw. Learn. Syst., vol. 34, no. 7, pp. 3680â€“3690, 2023. <br><span id="ref-182" class="ref-anchor">[182]</span> Y. Lu et al., â€œImitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios,â€ in Proc. IEEE/RSJ Int. Conf. Intell. Rob. Syst (IROS), pp. 7553â€“7560, 2023. <br><span id="ref-183" class="ref-anchor">[183]</span> M. Everett, B. LÂ¨utjens, and J. P.

How, â€œCertifiable robustness to adversarial state uncertainty in deep reinforcement learning,â€ IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 9, pp. 4184â€“4198, 2022. <br><span id="ref-184" class="ref-anchor">[184]</span> A. Balakrishnan, J. Lee, A. Gaurav, K. Czarnecki, and S. Sedwards, â€œTransfer reinforcement learning for autonomous driving: From wisemove to wisesim,â€ ACM Trans. Model. Comput. Simul. (TOMACS), vol. 31, no. 3, pp. 1â€“26, 2021. <br><span id="ref-185" class="ref-anchor">[185]</span> K. L. Voogd, J. P. Allamaa, J. Alonso-Mora, and T. D.

Son, â€œReinforcement learning from simulation to real world autonomous driving using digital twin,â€ IFAC-PapersOnLine, vol. 56, no. 2, pp. 1510â€“1515, 2023. <br><span id="ref-186" class="ref-anchor">[186]</span> C. Gao, Y. Zheng, W. Wang, F. Feng, X. He, and Y. Li, â€œCausal inference in recommender systems: A survey and future directions,â€ ACM Transactions on Information Systems, vol. 42, no. 4, pp. 1â€“32, 2024. <br><span id="ref-187" class="ref-anchor">[187]</span> F. Shoeleh and M.

Asadpour, â€œSkill based transfer learning with domain adaptation for continuous reinforcement learning domains,â€ Applied Intelligence, vol. 50, no. 2, pp. 502â€“518, 2020. <br><span id="ref-188" class="ref-anchor">[188]</span> J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. Zintgraf, C. Finn, and S. Whiteson, â€œA survey of meta-reinforcement learning,â€ arXiv:2301.08028, 2023. <br><span id="ref-189" class="ref-anchor">[189]</span> F. Ye, P. Wang, C.-Y. Chan, and J. Zhang, â€œMeta reinforcement learning-based lane change strategy for autonomous vehicles,â€ in Proc. IEEE Intell. Veh. Symposium (IV), pp.

223â€“230, 2021. <br><span id="ref-190" class="ref-anchor">[190]</span> Q. Deng, R. Li, Q. Hu, Y. Zhao, and R. Li, â€œContext-aware meta-rl with two-stage constrained adaptation for urban driving,â€ IEEE Trans. Veh. Technol., 2023. <br><span id="ref-191" class="ref-anchor">[191]</span> D. Abel, A. Barreto, B. Van Roy, D. Precup, H. P. van Hasselt, and S. Singh, â€œA definition of continual reinforcement learning,â€ Proc. Adv. Neural Inf. Proces. Syst., vol. 36, 2024. <br><span id="ref-192" class="ref-anchor">[192]</span> D. Wei, J. Xing, S. Yang, Y. Lu, and Y.

Huang, â€œContinual reinforcement learning for autonomous driving with application on velocity control under various environment,â€ in Proc. CAA Int. Conf. Veh. Control Intell. (CVCI), pp. 1â€“8, 2023. <br><span id="ref-193" class="ref-anchor">[193]</span> Z. Cao et al., â€œAutonomous driving policy continual learning with one-shot disengagement case,â€ IEEE Trans. Intell. Veh., vol. 8, no. 2, pp. 1380â€“1391, 2022. <br><span id="ref-194" class="ref-anchor">[194]</span> X. He and C.

Lv, â€œTowards safe autonomous driving: Decision making with observation-robust reinforcement learning,â€ Automotive Innovation, vol. 6, no. 4, pp. 509â€“520, 2023. <br><span id="ref-195" class="ref-anchor">[195]</span> B. LÂ¨utjens, M. Everett, and J. P. How, â€œSafe reinforcement learning with model uncertainty estimates,â€ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 8662â€“8668, 2019. <br><span id="ref-196" class="ref-anchor">[196]</span> J. Choi, C. Dance, J.-E. Kim, S. Hwang, and K.-s. Park, â€œRiskconditioned distributional soft actor-critic for risk-sensitive navigation,â€ in Proc. IEEE Int.

Conf. Robot. Autom. (ICRA), pp. 8337â€“8344, 2021. <br><span id="ref-197" class="ref-anchor">[197]</span> S. Li et al., â€œLearning locomotion for quadruped robots via distributional ensemble actor-critic,â€ IEEE Robot. Autom. Lett., vol. 9, no. 2, pp. 1811â€“1818, 2024.

<br><span id="ref-198" class="ref-anchor">[198]</span> J. Duan, D. Yu, S. E. Li, W. Wang, Y. Ren, Z. Lin, and B. Cheng, â€œFixed-dimensional and permutation invariant state representation of autonomous driving,â€ IEEE Trans. Intell. Transp. Syst., vol. 23, no. 7, pp. 9518â€“9528, 2021. <br><span id="ref-199" class="ref-anchor">[199]</span> W. X. Zhao et al., â€œA survey of large language models,â€ arXiv:2303.18223, 2023. <br><span id="ref-200" class="ref-anchor">[200]</span> C. Cui et al., â€œA survey on multimodal large language models for autonomous driving,â€ in Proc. IEEE/CVF Winter Conf. Appl. Comput. Vision Workshops, pp. 958â€“979, 2024. <br><span id="ref-201" class="ref-anchor">[201]</span> B.

Wang, Y. Qu, Y. Jiang, J. Shao, C. Liu, W. Yang, and X. Ji, â€œLlm-empowered state representation for reinforcement learning,â€ arXiv:2407.13237, 2024. <br><span id="ref-202" class="ref-anchor">[202]</span> W. Zhao, T. He, and C. Liu, â€œProbabilistic safeguard for reinforcement learning using safety index guided gaussian process models,â€ in Learning for Dynamics and Control Conference, pp. 783â€“796, 2023. <br><span id="ref-203" class="ref-anchor">[203]</span> K.-C. Hsu, H. Hu, and J. F. Fisac, â€œThe safety filter: A unified view of safety-critical control in autonomous systems,â€ Annu. Rev.

Control Robot. Auton. Syst., vol. 7, 2023. <br><span id="ref-204" class="ref-anchor">[204]</span> G. Swamy, C. Dann, R. Kidambi, Z. S. Wu, and A. Agarwal, â€œA minimaximalist approach to reinforcement learning from human feedback,â€ arXiv:2401.04056, 2024. <br><span id="ref-205" class="ref-anchor">[205]</span> E. Kargar and V. Kyrki, â€œIncreasing the efficiency of policy learning for autonomous vehicles by multi-task representation learning,â€ IEEE Trans. Intell. Veh., vol. 7, no. 3, pp. 701â€“710, 2022. <br><span id="ref-206" class="ref-anchor">[206]</span> Z. Yang, Y. Chen, J. Wang, S. Manivasagam, W.-C. Ma, A. J. Yang, and R.

Urtasun, â€œUnisim: A neural closed-loop sensor simulator,â€ in Proc. IEEE/CVF Conf. Comput. Vision and Pattern Recognit., pp. 1389â€“ 1399, 2023. <br><span id="ref-207" class="ref-anchor">[207]</span> Z. Deng, J. Jiang, G. Long, and C. Zhang, â€œCausal reinforcement learning: A survey,â€ arXiv:2307.01452, 2023. <br><span id="ref-208" class="ref-anchor">[208]</span> Y. Cao and Y. Li, â€œSurvey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods,â€ arXiv:2404.00282, 2024. <br><span id="ref-209" class="ref-anchor">[209]</span> A. Keysan, A. Look, E. Kosman, G. GÂ¨ursun, J. Wagner, Y. Yao, and B.

Rakitsch, â€œCan you text what is happening? integrating pre-trained language encoders into trajectory prediction models for autonomous driving,â€ arXiv:2309.05282, 2023. <br><span id="ref-210" class="ref-anchor">[210]</span> L. Wen et al., â€œDilu: A knowledge-driven approach to autonomous driving with large language models,â€ arXiv:2309.16292, 2023. <br><span id="ref-211" class="ref-anchor">[211]</span> M. Yildirim, B. Dagda, and S. Fallah, â€œHighwayllm: Decision-making and navigation in highway driving with rl-informed language model,â€ arXiv:2405.13547, 2024.</div></div></div>
    
    <script>
    const CURRENT_FILENAME = "A_Survey_of_Reinforcement_Learning-Based_Motion_Planning_for_Autonomous_Driving_Lessons_Learned_from_a_Driving_Task_Perspective";
    let isFeedbackMode = false;
    let isTranslating = false;
    let isStopping = false; // [çŠ¶æ€] æ ‡è®°æ˜¯å¦æ­£åœ¨åœæ­¢ä¸­
    let eventSource = null;

    function toggleFeedbackMode() {
        if (isTranslating || isStopping) return;
        isFeedbackMode = !isFeedbackMode;
        document.body.classList.toggle('feedback-mode');
        updateUI();
    }

    function updateUI() {
        const toggleBtn = document.getElementById('toggle-btn');
        const runBtn = document.getElementById('run-btn');
        const statusText = document.getElementById('status-text');
        
        if (isStopping) {
            // 1. åœæ­¢ä¸­çŠ¶æ€
            toggleBtn.style.display = 'none';
            runBtn.style.display = 'block';
            runBtn.textContent = "â³ æ­£åœ¨åœæ­¢ (Stopping)...";
            runBtn.className = 'btn btn:disabled';
            runBtn.onclick = null;
            statusText.textContent = "æ­£åœ¨ç­‰å¾…åå°ä¿å­˜è¿›åº¦...";
            disableClickHandlers();
        } 
        else if (isTranslating) {
            // 2. ç¿»è¯‘ä¸­çŠ¶æ€
            toggleBtn.style.display = 'none';
            runBtn.style.display = 'block';
            runBtn.textContent = "ğŸ›‘ åœæ­¢é‡è¯‘";
            runBtn.className = 'btn btn-danger';
            runBtn.onclick = stopRerun; 
            // statusText ç”± SSE æ›´æ–°
            disableClickHandlers();
        } 
        else if (isFeedbackMode) {
            // 3. çº é”™æ¨¡å¼
            toggleBtn.style.display = 'block';
            toggleBtn.textContent = "é€€å‡ºçº é”™æ¨¡å¼";
            toggleBtn.className = 'btn btn-danger';
            
            runBtn.style.display = 'block';
            runBtn.textContent = "ğŸš€ åº”ç”¨ä¿®æ”¹å¹¶é‡è¯‘";
            runBtn.className = 'btn btn-success';
            runBtn.onclick = triggerRerun;
            
            statusText.textContent = "âœï¸ çº é”™æ¨¡å¼ï¼šç‚¹å‡»é»„è‰²åŒºåŸŸå³å¯ä¿®æ”¹";
            enableClickHandlers();
        } 
        else {
            // 4. æµè§ˆæ¨¡å¼
            toggleBtn.style.display = 'block';
            toggleBtn.textContent = "è¿›å…¥çº é”™æ¨¡å¼";
            toggleBtn.className = 'btn btn-primary';
            runBtn.style.display = 'none';
            statusText.textContent = "æµè§ˆæ¨¡å¼";
            disableClickHandlers();
        }
    }

    function enableClickHandlers() {
        const rows = document.querySelectorAll('.row-container');
        rows.forEach(row => {
            const transCol = row.querySelector('.col-trans');
            if (transCol.getAttribute('data-bound')) return;
            transCol.setAttribute('data-bound', 'true');
            transCol.onclick = () => {
                if (!isFeedbackMode || isTranslating || isStopping) return;
                const panel = row.querySelector('.feedback-panel');
                const isHidden = (panel.style.display === 'none' || panel.style.display === '');
                panel.style.display = isHidden ? 'block' : 'none';
            };
        });
    }

    function disableClickHandlers() {
        const panels = document.querySelectorAll('.feedback-panel');
        panels.forEach(p => p.style.display = 'none');
    }

    // --- [æ ¸å¿ƒé€»è¾‘] å¼€å§‹é‡è¯‘ ---
    async function triggerRerun() {
        if (!confirm("ç¡®å®šè¦æ ¹æ®æ‚¨çš„ä¿®æ”¹å»ºè®®é‡æ–°ç¿»è¯‘å—ï¼Ÿ")) return;
        isTranslating = true;
        isStopping = false;
        updateUI();
        
        try {
            const response = await fetch('/api/feedback/rerun', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ filename: CURRENT_FILENAME })
            });
            startSSE();
        } catch (err) {
            alert("è¯·æ±‚é”™è¯¯: " + err);
            isTranslating = false;
            updateUI();
        }
    }

    // --- [æ ¸å¿ƒé€»è¾‘] åœæ­¢é‡è¯‘ (ä¼˜é›…åœæ­¢) ---
    async function stopRerun() {
        if(!confirm("ç¡®å®šè¦ç»ˆæ­¢åå°ä»»åŠ¡å—ï¼Ÿ")) return;
        
        isStopping = true; // è¿›å…¥åœæ­¢ç­‰å¾…çŠ¶æ€
        updateUI();
        
        try {
            await fetch('/api/workflow/stop/' + CURRENT_FILENAME, { method: 'POST' });
            // æ³¨æ„ï¼šæ­¤æ—¶ä¸æ–­å¼€ SSEï¼Œç­‰å¾…åç«¯å›æ»šæ•°æ®
        } catch(e) {
            alert("å‘é€åœæ­¢ä¿¡å·å¤±è´¥: " + e);
            isStopping = false;
            updateUI();
        }
    }

    // --- [æ ¸å¿ƒé€»è¾‘] SSE ç›‘å¬ ---
    function startSSE() {
        if(eventSource) eventSource.close();
        const url = '/api/stream/translation/' + CURRENT_FILENAME;
        eventSource = new EventSource(url);
        
        eventSource.onmessage = (e) => {
            const tasks = JSON.parse(e.data);
            const statusText = document.getElementById('status-text');
            
            const done = tasks.filter(t => t.status === 'success').length;
            const pending = tasks.filter(t => t.status === 'pending').length;
            const processing = tasks.filter(t => t.status === 'processing').length;
            
            // [Ack-based Stop] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·å·²ç”Ÿæ•ˆ (processingå½’é›¶)
            if (isStopping && processing === 0) {
                eventSource.close();
                isTranslating = false;
                isStopping = false;
                alert("âœ… åå°ä»»åŠ¡å·²å®‰å…¨åœæ­¢ï¼Œè¿›åº¦å·²ä¿å­˜ã€‚");
                location.reload(); // åˆ·æ–°ä»¥æ˜¾ç¤ºå›æ»šåçš„çŠ¶æ€
                return;
            }

            if (isTranslating && !isStopping) {
                statusText.textContent = `â³ é‡è¯‘ä¸­... (å‰©ä½™: ${pending} | è¿›è¡Œä¸­: ${processing})`;
            }
        };
        
        eventSource.addEventListener('close', () => {
            eventSource.close();
            isTranslating = false;
            isStopping = false;
            const statusText = document.getElementById('status-text');
            statusText.textContent = "âœ… å®Œæˆï¼æ­£åœ¨åˆ·æ–°...";
            setTimeout(() => { location.reload(); }, 1500);
        });
        
        eventSource.onerror = (err) => {
            console.warn("SSE è¿çº¿æ³¢åŠ¨", err);
        };
    }

    async function saveFeedback(taskId, btnElement) {
        const container = document.getElementById('task-' + taskId);
        const input = container.querySelector('.feedback-input');
        const hint = input.value.trim();
        const statusMsg = container.querySelector('.status-saved');
        const badge = document.getElementById('badge-' + taskId);
        
        if (!hint) { alert("è¯·è¾“å…¥æç¤º"); return; }
        
        const originalText = btnElement.textContent;
        btnElement.disabled = true;
        btnElement.textContent = "ä¿å­˜ä¸­...";
        
        try {
            const response = await fetch('/api/feedback/update', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    filename: CURRENT_FILENAME,
                    id: taskId, 
                    hint: hint 
                })
            });
            const data = await response.json();
            if (data.status === 'success') {
                statusMsg.style.display = 'inline';
                setTimeout(() => statusMsg.style.display = 'none', 2000);
                btnElement.textContent = "å·²ä¿å­˜ (å¾…é‡è¯‘)";
                
                if(badge) {
                    badge.innerHTML = `ğŸ’¡ ä¸Šæ¬¡æç¤º: ${hint} (â³ ç­‰å¾…é‡è¯‘)`;
                    badge.classList.add('has-hint');
                }
            } else {
                alert("ä¿å­˜å¤±è´¥: " + data.msg);
                btnElement.disabled = false;
                btnElement.textContent = originalText;
            }
        } catch (err) {
            alert("è¿æ¥é”™è¯¯: " + err);
            btnElement.disabled = false;
            btnElement.textContent = originalText;
        }
    }
    
    function highlightAsset(id) {
        const el = document.getElementById(id);
        if (el) {
            el.scrollIntoView({ behavior: 'smooth', block: 'center' });
            el.classList.remove('highlight-asset');
            void el.offsetWidth;
            el.classList.add('highlight-asset');
        }
    }
    </script>
    </body></html>