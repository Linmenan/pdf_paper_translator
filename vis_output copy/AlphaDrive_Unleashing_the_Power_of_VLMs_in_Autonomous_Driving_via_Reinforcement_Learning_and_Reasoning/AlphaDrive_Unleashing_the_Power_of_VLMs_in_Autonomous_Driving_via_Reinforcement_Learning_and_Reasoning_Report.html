<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><title>AlphaDrive_Unleashing_the_Power_of_VLMs_in_Autonomous_Driving_via_Reinforcement_Learning_and_Reasoning - Interactive Mode</title><style>:root { --primary: #2c3e50; --accent: #3498db; --bg: #f8f9fa; --border: #e0e0e0; }body { font-family: "Segoe UI", sans-serif; margin: 0; background: var(--bg); padding-bottom: 100px; }.container { max-width: 1200px; margin: 0 auto; background: #fff; box-shadow: 0 0 20px rgba(0,0,0,0.05); }.meta-section { padding: 40px; text-align: center; background: #fff; }.meta-title-en { font-size: 1.8em; color: #2c3e50; font-weight: 700; }.meta-title-zh { font-size: 1.6em; color: #34495e; font-weight: 400; }.meta-author-en { font-style: italic; color: #7f8c8d; }.meta-author-zh { color: #16a085; font-weight: bold; }.toolbar { position: fixed; top: 20px; right: 20px; background: #fff; padding: 10px 20px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border-radius: 8px; z-index: 999; display: flex; gap: 10px; align-items: center; }.btn { padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; font-weight: bold; transition: 0.2s; }.btn-primary { background: var(--accent); color: #fff; }.btn-danger { background: #e74c3c; color: #fff; }.btn-success { background: #27ae60; color: #fff; }.btn:disabled { background: #ccc; cursor: not-allowed; }#loading-mask { position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(255,255,255,0.8); z-index: 2000; display: none; justify-content: center; align-items: center; flex-direction: column; }.spinner { width: 40px; height: 40px; border: 4px solid #f3f3f3; border-top: 4px solid var(--accent); border-radius: 50%; animation: spin 1s linear infinite; }@keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }.row-container { border-bottom: 1px solid var(--border); }.row { display: flex; }.col-src, .col-trans { flex: 1; padding: 20px; }.col-src { border-right: 1px solid var(--border); color: #555; background: #fff; }body.feedback-mode .row:hover { background: #fdfdfd; }body.feedback-mode .col-trans { cursor: pointer; outline: 1px dashed #ccc; }.feedback-panel { background: #f1f8ff; padding: 15px 20px; border-top: 1px solid #d6eaf8; display: none; }.feedback-header { font-weight: bold; color: #2c3e50; margin-bottom: 5px; font-size: 0.9em; }.feedback-input { width: 100%; height: 60px; padding: 8px; border: 1px solid #bdc3c7; border-radius: 4px; font-family: inherit; margin-bottom: 5px; }.hint-badge { margin-top: 10px; padding: 5px 10px; background: #fff3cd; border: 1px solid #ffeeba; color: #856404; font-size: 0.85em; border-radius: 4px; }.status-saved { color: #27ae60; font-weight: bold; margin-left: 10px; display: none; }.asset-row { background: #f4f4f4; padding: 20px; display: block; }.asset-card { background: #fff; max-width: 90%; margin: 0 auto; border-radius: 8px; padding: 10px; text-align: center; }.asset-img { max-width: 100%; }</style></head><body><div id="loading-mask"><div class="spinner"></div><div style="margin-top: 15px; font-size: 1.2em; color: #555;">æ­£åœ¨åå°é‡è¯‘å¹¶ç”ŸæˆæŠ¥å‘Šï¼Œè¯·ç¨å€™...</div></div><div class="toolbar"><div id="status-text" style="margin-right: 10px; color: #666;">æµè§ˆæ¨¡å¼</div><button class="btn btn-primary" id="toggle-btn" onclick="toggleFeedbackMode()">è¿›å…¥çº é”™æ¨¡å¼</button><button class="btn btn-success" id="run-btn" onclick="triggerRerun()" style="display:none;">ğŸš€ åº”ç”¨ä¿®æ”¹å¹¶é‡è¯‘</button></div><div class="container"><div class="meta-section"><h1 class="meta-title-en">AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning</h1><h1 class="meta-title-zh">AlphaDriveï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­é‡Šæ”¾VLMsçš„å¼ºå¤§åŠŸèƒ½</h1><div class="meta-author-en">Bo Jiang1,â‹„ Shaoyu Chen1,2 Qian Zhang2 Wenyu Liu1 Xinggang Wang1,B 1 Huazhong University of Science and Technology 2 Horizon Robotics https://github.com/hustvl/AlphaDrive</div><div class="meta-author-zh">æ±Ÿæ³¢1ï¼Œâ‹… é™ˆå°‘å®‡1,2 å¼ è°¦2 æŸ³æ–‡å®‡1 ç‹å…´åˆš1,B 1åä¸­ç§‘æŠ€å¤§å­¦ 2åœ°å¹³çº¿æœºå™¨äºº https://github.com/hustvl/AlphaDrive</div></div><hr class="meta-divider"><div class="main-content"><div class="row-container" id="task-1"><div class="row text-row "><div class="col-src">[[HEADER: Abstract]]</div><div class="col-trans">æ‘˜è¦<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 1)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('1', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-2"><div class="row text-row "><div class="col-src">OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities.

Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving.</div><div class="col-trans">OpenAI o1å’ŒDeepSeek R1åœ¨æ•°å­¦å’Œç§‘å­¦ç­‰å¤æ‚é¢†åŸŸä¸­å®ç°äº†ç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶çº§çš„è¡¨ç°ï¼Œå…¶ä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚åœ¨è‡ªåŠ¨é©¾é©¶æ–¹é¢ï¼Œæœ€è¿‘çš„ç«¯åˆ°ç«¯æ¨¡å‹å¤§å¹…æé«˜äº†è§„åˆ’æ€§èƒ½ï¼Œä½†ä»éš¾ä»¥åº”å¯¹ç”±äºå¸¸è¯†å’Œæ¨ç†èƒ½åŠ›æœ‰é™è€Œäº§ç”Ÿçš„é•¿å°¾é—®é¢˜ã€‚

ä¸€äº›ç ”ç©¶å°†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºé€šè¿‡ç®€å•çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹é©¾é©¶æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œä¸è¿›ä¸€æ­¥æ¢ç´¢ä¸“é—¨é’ˆå¯¹è§„åˆ’çš„è®­ç»ƒç­–ç•¥æˆ–ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAlphaDriveçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ä¸æ¨ç†æŠ€æœ¯ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­åº”ç”¨è¿™äº›æŠ€æœ¯ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 2)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('2', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-3"><div class="row text-row "><div class="col-src">AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning.

Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.</div><div class="col-trans">AlphaDrive å¼•å…¥äº†å››ç§åŸºäºGRPOçš„RLå¥–åŠ±ï¼Œä¸“é—¨ç”¨äºè§„åˆ’ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§ç»“åˆSFTä¸RLçš„ä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒç­–ç•¥ã€‚å› æ­¤ï¼ŒAlphaDrive åœ¨è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨SFTæˆ–ä¸è¿›è¡Œæ¨ç†çš„æƒ…å†µã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¬£å–œåœ°å‘ç°ï¼Œåœ¨ç»è¿‡RLè®­ç»ƒåï¼ŒAlphaDrive å±•ç°å‡ºä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAlphaDrive æ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„RLä¸è§„åˆ’æ¨ç†é›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ä¸­çš„ç³»ç»Ÿã€‚ä»£ç å°†åœ¨æœªæ¥çš„ç ”ç©¶ä¸­äºˆä»¥å‘å¸ƒï¼Œä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 3)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('3', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-4"><div class="row text-row "><div class="col-src">[[HEADER: 1. Introduction]]</div><div class="col-trans"><b>1. å¼•è¨€</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 4)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('4', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_1</div><img src="./assets/Figure_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 1. Our planning-oriented RL and two-stage training strat- egy significantly boost planning accuracy. With just 20k samples, it outperforms SFT by 35.31%, showing strong performance even with limited data. As data increase, AlphaDrive consistently leads in planning performance.</div><div class="asset-desc-zh">å›¾ 1. æˆ‘ä»¬çš„é¢å‘è§„åˆ’çš„ RL å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†è§„åˆ’å‡†ç¡®æ€§ã€‚ä»…ä½¿ç”¨ 20k æ ·æœ¬ï¼Œå®ƒå°±æ¯” SFT é«˜å‡º 35.31% çš„æ€§èƒ½ï¼Œåœ¨æ•°æ®å¢åŠ çš„æƒ…å†µä¸‹ï¼ŒAlphaDrive åœ¨è§„åˆ’æ€§èƒ½ä¸Šå§‹ç»ˆé¢†å…ˆã€‚</div></div></div></div><div class="row-container" id="task-5"><div class="row text-row "><div class="col-src">Autonomous driving has witnessed rapid advances in recent years, with end-to-end autonomous driving emerging as one of the most representative models [8, 16, 17, 22, 29]. They take sensor data as input and leverage learnable neural networks to plan the vehicleâ€™s future trajectory. Benefiting from large-scale driving demonstrations, end-to-end models continuously improving their planning capabilities by

expanding training data and increasing model parameters. However, due to their black-box nature and lack of common sense, end-to-end models still face significant challenges when handling complex and long-tail driving scenarios. For instance, consider a situation where the vehicle ahead is carrying traffic cones while driving.</div><div class="col-trans">è‡ªä¸»é©¾é©¶åœ¨è¿‘å¹´æ¥å–å¾—äº†è¿…é€Ÿçš„è¿›æ­¥ï¼Œç«¯åˆ°ç«¯ï¼ˆend-to-endï¼‰è‡ªä¸»é©¾é©¶æ¨¡å¼ä½œä¸ºæœ€å…·ä»£è¡¨æ€§çš„æ¨¡å‹ä¹‹ä¸€ï¼Œå¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶å’Œå‘å±•[8, 16, 17, 22, 29]ã€‚è¿™äº›æ¨¡å‹å°†ä¼ æ„Ÿå™¨æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨å¯å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ¥è§„åˆ’è½¦è¾†çš„æœªæ¥è½¨è¿¹ã€‚å¾—ç›Šäºå¤§è§„æ¨¡çš„é©¾é©¶æ¼”ç¤ºï¼Œç«¯åˆ°ç«¯æ¨¡å‹é€šè¿‡æ‰©å±•è®­ç»ƒæ•°æ®å’Œå¢åŠ æ¨¡å‹å‚æ•°ä¸æ–­æ”¹è¿›å…¶è§„åˆ’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬å…·æœ‰é»‘ç›’æ€§è´¨ä¸”ç¼ºä¹å¸¸è¯†ï¼Œç«¯åˆ°ç«¯æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œé•¿å°¾ï¼ˆlong-tailï¼‰é©¾é©¶åœºæ™¯æ—¶ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œåœ¨å‰æ–¹è½¦è¾†æ­£åœ¨è¿é€äº¤é€šé”¥ç­’å¹¶è¡Œé©¶çš„æƒ…å†µä¸‹ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 5)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('5', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-6"><div class="row text-row "><div class="col-src">An end-to-end model may fail to comprehend the relationship between the leading vehicle and the traffic cones, mistakenly assuming that the road ahead is under construction and thus impassable, leading to an incorrect decision to brake. Therefore, relying solely on end-to-end models to achieve high-level autonomous driving remains challenging. With the success of GPT [6], large language models (LLMs) show remarkable comprehension and reasoning abilities [38, 48].

Furthermore, their capabilities have evolved from unimodal text understanding to multimodal vision-language processing. [3, 10, 24]. The commonsense and reasoning abilities of VLMs hold great potential to mitigate the limitations of end-to-end models. Recently, OpenAI o1 [25], which incorporates reasoning techniques, achieves performance comparable to or even surpassing that of human experts in fields such as programming.</div><div class="col-trans">ç«¯åˆ°ç«¯æ¨¡å‹å¯èƒ½æ— æ³•ç†è§£å‰è½¦ä¸äº¤é€šé”¥ä¹‹é—´çš„å…³ç³»ï¼Œé”™è¯¯åœ°å‡è®¾å‰æ–¹é“è·¯æ­£åœ¨æ–½å·¥ä¸”ä¸å¯é€šè¡Œï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„åˆ¹è½¦å†³ç­–ã€‚å› æ­¤ï¼Œä»…ä¾èµ–ç«¯åˆ°ç«¯æ¨¡å‹å®ç°é«˜çº§è‡ªåŠ¨é©¾é©¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚éšç€GPT [6]çš„æˆåŠŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç†è§£å’Œæ¨ç†èƒ½åŠ› [38, 48]ã€‚

æ­¤å¤–ï¼Œå®ƒä»¬çš„èƒ½åŠ›å·²ç»ä»å•ä¸€æ¨¡æ€æ–‡æœ¬ç†è§£å‘å±•åˆ°äº†å¤šæ¨¡æ€è§†è§‰-è¯­è¨€å¤„ç† [3, 10, 24]ã€‚VLMsçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥ç¼“è§£ç«¯åˆ°ç«¯æ¨¡å‹çš„å±€é™æ€§ã€‚æœ€è¿‘ï¼ŒOpenAI o1 [25] ç»“åˆäº†æ¨ç†æŠ€æœ¯ï¼Œåœ¨ç¼–ç¨‹ç­‰é¢†åŸŸå®ç°äº†ä¸ç”šè‡³è¶…è¶Šäººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 6)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('6', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-7"><div class="row text-row "><div class="col-src">Additionally, DeepSeek R1 [14], which leverages reinforcement learning, not only demonstrates â€œemergent abilitiesâ€and achieves top-tier performance but also requires significantly lower training costs compared to other models. These advances underscore the immense potential of reasoning techniques and RL in the development of large models. Existing research on applying VLMs to autonomous driving can be broadly categorized into two directions.

The first focuses on leveraging VLMs for the understanding of driving scenes [34, 49]. The second explores the use of VLMs for planning, where some studies treat VLMs as end-to-end systems that process driving images and other inputs to directly predict trajectories [7, 47]. However, unlike end-to-end models which are specifically designed for trajectory planning, VLMs operate in a language space and are not inherently suited for precise numerical predictions [12, 15].</div><div class="col-trans">æ­¤å¤–ï¼ŒDeepSeek R1 [14] åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸ä»…å±•ç¤ºäº†â€œæ¶Œç°èƒ½åŠ›â€å¹¶å–å¾—äº†é¡¶çº§æ€§èƒ½ï¼Œè€Œä¸”ä¸å…¶å®ƒæ¨¡å‹ç›¸æ¯”æ‰€éœ€çš„è®­ç»ƒæˆæœ¬æ˜¾è‘—é™ä½ã€‚è¿™äº›è¿›æ­¥çªæ˜¾äº†æ¨ç†æŠ€æœ¯å’Œå¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹æ¨¡å‹å¼€å‘ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚ç°æœ‰å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ç ”ç©¶å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ä¸ªæ–¹å‘ã€‚

ç¬¬ä¸€ä¸ªæ–¹å‘ä¾§é‡äºåˆ©ç”¨ VLMs ç†è§£é©¾é©¶åœºæ™¯ [34, 49]ã€‚ç¬¬äºŒä¸ªæ–¹å‘åˆ™æ¢ç´¢ä½¿ç”¨ VLMs è¿›è¡Œè§„åˆ’ï¼Œå…¶ä¸­ä¸€äº›ç ”ç©¶å°† VLMs è§†ä½œç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œå¤„ç†é©¾é©¶å›¾åƒå’Œå…¶ä»–è¾“å…¥ä»¥ç›´æ¥é¢„æµ‹è½¨è¿¹ [7, 47]ã€‚ç„¶è€Œï¼Œä¸ä¸“é—¨è®¾è®¡ç”¨äºè·¯å¾„è§„åˆ’çš„ç«¯åˆ°ç«¯æ¨¡å‹ä¸åŒï¼ŒVLMs åœ¨è¯­è¨€ç©ºé—´ä¸­è¿è¡Œï¼Œå¹¶ä¸å¤©ç”Ÿé€‚åˆè¿›è¡Œç²¾ç¡®çš„æ•°å€¼é¢„æµ‹ [12, 15]ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 7)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('7', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-8"><div class="row text-row "><div class="col-src">Consequently, directly employing VLMs for trajectory planning may result in suboptimal performance and even pose safety risks. Some studies leverage VLMs for high-level planning by formulating the ego vehicleâ€™s future actions in natural language, such as â€œslow down and turn rightâ€ [18]. Although this approach circumvents the aforementioned drawbacks, existing works still lack further exploration of training methodologies.

Most of them primarily rely on SFT, overlooking the impact of different training strategies on planning performance and the associated training costs. In this paper, we explore the following question: How can RL and reasoning â€” which achieves remarkable success in general large models â€” be applied to autonomous driving, particularly in planning, to enhance the performance of VLMs in autonomous driving while reducing training costs?</div><div class="col-trans">å› æ­¤ï¼Œç›´æ¥ä½¿ç”¨VLMè¿›è¡Œè½¨è¿¹è§„åˆ’å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ç”šè‡³å­˜åœ¨å®‰å…¨é£é™©ã€‚ä¸€äº›ç ”ç©¶é€šè¿‡å°†egoè½¦è¾†çš„æœªæ¥åŠ¨ä½œç”¨è‡ªç„¶è¯­è¨€ï¼ˆå¦‚â€œå‡é€Ÿå¹¶å³è½¬â€[[LINK: Eq. 1|Eq. 1]]ï¼‰æ¥åˆ¶å®šé«˜çº§è®¡åˆ’ï¼Œä»è€Œç»•è¿‡äº†ä¸Šè¿°é—®é¢˜ã€‚å°½ç®¡è¿™ç§æ–¹æ³•è§„é¿äº†å‰è¿°ç¼ºç‚¹ï¼Œä½†ç°æœ‰å·¥ä½œä»ç„¶ç¼ºä¹å¯¹ä¸åŒè®­ç»ƒç­–ç•¥åŠå…¶å¯¹è§„åˆ’æ€§èƒ½å’Œç›¸å…³è®­ç»ƒæˆæœ¬å½±å“çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚

å¤§å¤šæ•°ç ”ç©¶ä¸»è¦ä¾èµ–äºSFTï¼Œå¿½è§†äº†ä¸åŒè®­ç»ƒç­–ç•¥å¯¹è§„åˆ’æ€§èƒ½åŠç›¸åº”è®­ç»ƒæˆæœ¬çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä»¥ä¸‹é—®é¢˜ï¼šå¦‚ä½•å°†RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰å’Œæ¨ç†â€”â€”è¿™äº›æ–¹æ³•å·²ç»åœ¨é€šç”¨å¤§å‹æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸâ€”â€”åº”ç”¨äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è§„åˆ’æ–¹é¢ï¼Œä»¥æé«˜VLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ï¼Ÿ<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 8)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('8', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-9"><div class="row text-row "><div class="col-src">Through preliminary experiments, we find that directly applying existing RL and reasoning techniques to planning results in suboptimal performance. We attribute this to three main factors. First, the reward design in RL for general tasks is not well-suited for planning. For example, in visual object counting, the reward can be simply determined based on whether the model predicts the correct answer.

However, in autonomous driving, while high-level planning can be formulated as a multi-class classification problem, the varying significance of different driving behaviors makes it inappropriate to assign equal weights to all actions. Second, unlike mathematical or counting, the solution of planning are usually not unique. For instance, on an open, straight road, one may choose to maintain a constant speed or accelerate, both of which are valid decisions.</div><div class="col-trans">é€šè¿‡åˆæ­¥å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç›´æ¥å°†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æŠ€æœ¯åº”ç”¨äºè§„åˆ’ç»“æœä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸»è¦å½’å› äºä¸‰ä¸ªå› ç´ ã€‚é¦–å…ˆï¼Œåœ¨ä¸€èˆ¬ä»»åŠ¡ä¸­ï¼ŒRLä¸­çš„å¥–åŠ±è®¾è®¡å¹¶ä¸é€‚åˆè§„åˆ’ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†è§‰ç‰©ä½“è®¡æ•°ä»»åŠ¡ä¸­ï¼Œå¥–åŠ±å¯ä»¥æ ¹æ®æ¨¡å‹é¢„æµ‹çš„æ­£ç¡®ç­”æ¡ˆæ¥ç®€å•åœ°ç¡®å®šã€‚

ç„¶è€Œï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œè™½ç„¶é«˜å±‚æ¬¡çš„è§„åˆ’å¯ä»¥è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¤šç±»åˆ†ç±»é—®é¢˜ï¼Œä½†ç”±äºä¸åŒé©¾é©¶è¡Œä¸ºçš„é‡è¦æ€§å„å¼‚ï¼Œå°†æ‰€æœ‰åŠ¨ä½œèµ‹äºˆç›¸åŒæƒé‡æ˜¯ä¸åˆé€‚çš„ã€‚å…¶æ¬¡ï¼Œä¸æ•°å­¦æˆ–è®¡æ•°ä¸åŒï¼Œè§„åˆ’çš„è§£å†³æ–¹æ¡ˆé€šå¸¸ä¸æ˜¯å”¯ä¸€çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€æ¡å¼€æ”¾ä¸”ç¬”ç›´çš„è·¯ä¸Šï¼Œå¯ä»¥é€‰æ‹©ä¿æŒæ’å®šé€Ÿåº¦æˆ–åŠ é€Ÿï¼Œè¿™ä¸¤ç§å†³ç­–éƒ½æ˜¯åˆç†çš„ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 9)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('9', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-10"><div class="row text-row "><div class="col-src">Therefore, rigidly assessing whether the modelâ€™s planning output exactly matches the ground truth in the training data may not be the optimal approach. Finally, while domains such as mathematics have abundant reasoning data, including textbooks and solution manuals that can be easily utilized, autonomous driving lacks readily available datasets that capture the reasoning process. Collecting such data is highly costly and requires extensive manual annotation.

As a result, directly applying existing reasoning techniques to planning remains challenging. To address the aforementioned challenges, this paper introduces AlphaDrive, a VLM-based reinforcement learning and reasoning framework specifically designed for autonomous driving planning. In particular, AlphaDrive employs a RL strategy based on Group Relative Policy Optimization (GRPO) [33].</div><div class="col-trans">å› æ­¤ï¼Œä¸¥æ ¼è¯„ä¼°æ¨¡å‹çš„è§„åˆ’è¾“å‡ºæ˜¯å¦ä¸è®­ç»ƒæ•°æ®ä¸­çš„çœŸå®å€¼å®Œå…¨ä¸€è‡´å¯èƒ½å¹¶ä¸æ˜¯æœ€ä¼˜çš„æ–¹æ³•ã€‚æœ€åï¼Œå°½ç®¡è¯¸å¦‚æ•°å­¦è¿™æ ·çš„é¢†åŸŸæ‹¥æœ‰ä¸°å¯Œçš„æ¨ç†æ•°æ®ï¼ŒåŒ…æ‹¬æ˜“äºåˆ©ç”¨çš„æ•™ç§‘ä¹¦å’Œè§£é¢˜æ‰‹å†Œï¼Œä½†è‡ªåŠ¨é©¾é©¶ç¼ºä¹èƒ½å¤Ÿæ•æ‰æ¨ç†è¿‡ç¨‹çš„ç°æˆæ•°æ®é›†ã€‚æ”¶é›†æ­¤ç±»æ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ ‡æ³¨ã€‚

å› æ­¤ï¼Œç›´æ¥å°†ç°æœ‰çš„æ¨ç†æŠ€æœ¯åº”ç”¨äºè§„åˆ’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºåº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†AlphaDriveï¼Œè¿™æ˜¯ä¸€ç§åŸºäºVLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰çš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè‡ªåŠ¨é©¾é©¶è§„åˆ’ã€‚ç‰¹åˆ«æ˜¯ï¼ŒAlphaDrive ä½¿ç”¨äº†ä¸€ç§åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰[33] çš„RLç­–ç•¥ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 10)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('10', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-11"><div class="row text-row "><div class="col-src">Compared to Proximal Policy Optimization (PPO) [32] and Direct Preference Optimization (DPO) [31], GRPO exhibits better training stability and performance. Furthermore, the group relative optimization strategy in GRPO is well-suited for planning, as planning often involves multiple valid solutions, making relative optimization across multiple solutions a natural fit.

Our experiments show that AlphaDrive exhibits some emergent multimodal planning capabilities, which we think can be attributed to the use of GRPO. AlphaDrive introduces four GRPO rewards tailored for planning. The first is the planning accuracy reward, which evaluates the consistency between the modelâ€™s planning actions and the ground truth actions. The second is the actionweighted reward, which assigns different weights to various actions based on their importance to safety.</div><div class="col-trans">ä¸Proximal Policy Optimization (PPO) [32]å’ŒDirect Preference Optimization (DPO) [31]ç›¸æ¯”ï¼ŒGRPOåœ¨è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚æ­¤å¤–ï¼ŒGRPOä¸­çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŒ–ç­–ç•¥éå¸¸é€‚åˆç”¨äºè§„åˆ’ï¼Œå› ä¸ºè§„åˆ’é€šå¸¸æ¶‰åŠå¤šä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½¿å¾—åœ¨å¤šä¸ªæ–¹æ¡ˆä¹‹é—´è¿›è¡Œç›¸å¯¹ä¼˜åŒ–æˆä¸ºè‡ªç„¶çš„é€‰æ‹©ã€‚

æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAlphaDriveå±•ç°äº†ä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™å¯èƒ½æ˜¯ç”±äºä½¿ç”¨äº†GRPOã€‚AlphaDriveå¼•å…¥äº†å››ç§é’ˆå¯¹è§„åˆ’å®šåˆ¶çš„GRPOå¥–åŠ±ã€‚é¦–å…ˆæ˜¯è§„åˆ’å‡†ç¡®æ€§å¥–åŠ±ï¼Œå®ƒè¯„ä¼°æ¨¡å‹çš„è§„åˆ’åŠ¨ä½œä¸çœŸå®åŠ¨ä½œçš„ä¸€è‡´æ€§ã€‚å…¶æ¬¡æ˜¯åŠ¨ä½œåŠ æƒå¥–åŠ±ï¼Œæ ¹æ®å„ç§åŠ¨ä½œå¯¹å®‰å…¨çš„é‡è¦æ€§ä¸ºå…¶åˆ†é…ä¸åŒçš„æƒé‡ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 11)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('11', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-12"><div class="row text-row "><div class="col-src">For instance, actions such as braking and steering are critical for safety, so weighting them accordingly helps the model achieve better performance in planning key actions. The third is the planning diversity reward, which encourages the model to generate multiple diverse solutions. This prevents mode collapse and enhances overall planning performance. The last one is the planning format reward, where we define a specific output format and encourage the model to follow it.

This ensures more structured outputs and contributes to more stable training. In addition to RL, we propose a planning reasoning technique. Our approach employs a two-stage training strategy based on knowledge distillation, integrating SFT and RL. In the first stage, we leverage a large model, such as GPT-4o, to generate a small yet high-quality dataset containing planning reasoning processes derived from real driving actions.</div><div class="col-trans">ä¾‹å¦‚ï¼Œåˆ¶åŠ¨å’Œè½¬å‘ç­‰åŠ¨ä½œå¯¹äºå®‰å…¨è‡³å…³é‡è¦ï¼Œå› æ­¤å¯¹è¿™äº›åŠ¨ä½œè¿›è¡Œç›¸åº”çš„åŠ æƒæœ‰åŠ©äºæ¨¡å‹åœ¨è§„åˆ’å…³é”®åŠ¨ä½œæ—¶å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ç¬¬ä¸‰ç§æ˜¯è§„åˆ’å¤šæ ·æ€§å¥–åŠ±ï¼Œå®ƒé¼“åŠ±æ¨¡å‹ç”Ÿæˆå¤šç§å¤šæ ·çš„è§£å†³æ–¹æ¡ˆã€‚è¿™å¯ä»¥é˜²æ­¢æ¨¡å¼å´©æºƒå¹¶æé«˜æ•´ä½“çš„è§„åˆ’è¡¨ç°ã€‚æœ€åä¸€ç§æ˜¯è§„åˆ’æ ¼å¼å¥–åŠ±ï¼Œåœ¨è¿™ç§å¥–åŠ±æœºåˆ¶ä¸‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç‰¹å®šçš„è¾“å‡ºæ ¼å¼ï¼Œå¹¶é¼“åŠ±æ¨¡å‹éµå¾ªè¿™ä¸€æ ¼å¼ã€‚

è¿™æ ·å¯ä»¥ç¡®ä¿è¾“å‡ºæ›´åŠ ç»“æ„åŒ–ï¼Œå¹¶æœ‰åŠ©äºæ›´ç¨³å®šçš„è®­ç»ƒã€‚é™¤äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è§„åˆ’æ¨ç†æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºçŸ¥è¯†è’¸é¦çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†åºåˆ—å¡«å……è®­ç»ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ç”Ÿæˆä¸€ä¸ªåŒ…å«ä»å®é™…é©¾é©¶è¡Œä¸ºä¸­æå–å‡ºçš„è§„åˆ’æ¨ç†è¿‡ç¨‹çš„å°å‹é«˜è´¨é‡æ•°æ®é›†ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 12)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('12', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-13"><div class="row text-row "><div class="col-src">This dataset is then used to fine-tune our model via SFT, effectively distilling knowledge from the large model. In the second stage, we further refine the model using RL. Introducing the SFT stage as a warm-up step effectively mitigates hallucinations and instability commonly observed in the early stages of reinforcement learning, while also enhancing planning performance.

Our contributions are summarized as follows: â€¢ We propose AlphaDrive, a VLM tailored for high-level planning in autonomous driving. To the best of our knowledge, AlphaDrive is the first to integrate GRPObased RL with planning reasoning to autonomous driving, significantly boosting both performance and training efficiency. â€¢ AlphaDrive introduces four GRPO rewards for planning: planning accuracy reward, action-weighted reward, planning diversity reward, and planning format reward.</div><div class="col-trans">è¯¥æ•°æ®é›†éšåç”¨äºé€šè¿‡SFTå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæœ‰æ•ˆåœ°ä»å¤§å‹æ¨¡å‹ä¸­æç‚¼çŸ¥è¯†ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨RLç»†åŒ–æ¨¡å‹ã€‚å¼•å…¥SFTé˜¶æ®µä½œä¸ºé¢„çƒ­æ­¥éª¤ï¼Œæœ‰æ•ˆç¼“è§£äº†å¼ºåŒ–å­¦ä¹ æ—©æœŸé˜¶æ®µå¸¸è§çš„å¹»è§‰å’Œä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå¹¶ä¸”ä¹Ÿæå‡äº†è§„åˆ’æ€§èƒ½ã€‚

æˆ‘ä»¬çš„è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š
â€¢ æˆ‘ä»¬æå‡ºAlphaDriveï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è‡ªä¸»é©¾é©¶é«˜çº§è§„åˆ’çš„VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAlphaDriveæ˜¯ç¬¬ä¸€ä¸ªå°†åŸºäºGRPOçš„RLä¸è§„åˆ’æ¨ç†ç»“åˆåº”ç”¨äºè‡ªä¸»é©¾é©¶çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚
â€¢ AlphaDriveå¼•å…¥äº†å››ç§ç”¨äºè§„åˆ’çš„GRPOå¥–åŠ±ï¼šè§„åˆ’å‡†ç¡®æ€§å¥–åŠ±ã€åŠ¨ä½œåŠ æƒå¥–åŠ±ã€è§„åˆ’å¤šæ ·æ€§å¥–åŠ±ä»¥åŠè§„åˆ’æ ¼å¼å¥–åŠ±ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 13)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('13', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-14"><div class="row text-row "><div class="col-src">These optimized rewards make GRPO more suitable for autonomous driving. â€¢ We propose a two-stage reasoning training strategy based on knowledge distillation, integrating SFT and RL. Our approach achieves better planning performance compared to training with RL alone or without reasoning. â€¢ Experiments on a large-scale driving dataset validate the superiority of AlphaDrive.

Compared to the SFT-trained model, AlphaDrive significantly improves the planning accuracy by 25.52% and, with only 20% of the training data, outperforms the SFT-trained model by 35.31%. We are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is promising for improving driving safety and efficiency.</div><div class="col-trans">è¿™äº›ä¼˜åŒ–çš„å¥–åŠ±ä½¿GRPOæ›´åŠ é€‚åˆè‡ªåŠ¨é©¾é©¶ã€‚â€¢ æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„çŸ¥è¯†æ¨ç†è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†SFTå’ŒRLã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨RLè¿›è¡Œè®­ç»ƒæˆ–ä¸è¿›è¡Œæ¨ç†çš„æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚â€¢ åœ¨å¤§è§„æ¨¡é©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†AlphaDriveçš„ä¼˜è¶Šæ€§ã€‚

ä¸SFTè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼ŒAlphaDriveåœ¨è§„åˆ’å‡†ç¡®æ€§ä¸Šæé«˜äº†25.52%ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®å°±æ¯”SFTè®­ç»ƒçš„æ¨¡å‹é«˜å‡º35.31%ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨RLè®­ç»ƒä¹‹åï¼ŒAlphaDriveè¡¨ç°å‡ºä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡æ˜¯å¾ˆæœ‰å‰æ™¯çš„ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 14)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('14', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-15"><div class="row text-row "><div class="col-src">[[HEADER: 2. Related Work]]</div><div class="col-trans">2. ç›¸å…³å·¥ä½œ<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 15)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('15', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_2</div><img src="./assets/Figure_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 2. Overall training framework of AlphaDrive. AlphaDrive is trained using GRPO-based RL, and we design four planning rewards to help the model understand and learn planning. Besides, we propose a two-stage training paradigm, the first stage uses SFT to distill the planning reasoning process from a large model and serves as a warm-up, while the second stage employs RL to explore planning.</div><div class="asset-desc-zh">å›¾ 2. AlphaDrive çš„æ•´ä½“è®­ç»ƒæ¡†æ¶ã€‚AlphaDrive ä½¿ç”¨åŸºäº GRPO çš„ RL è¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¾è®¡äº†å››ç§è§„åˆ’å¥–åŠ±æ¥å¸®åŠ©æ¨¡å‹ç†è§£å’Œå­¦ä¹ è§„åˆ’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨ SFT æå–å‡ºè‡ªå¤§æ¨¡å‹çš„è§„åˆ’æ¨ç†è¿‡ç¨‹å¹¶ä½œä¸ºé¢„çƒ­ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨ RL æ¢ç´¢è§„åˆ’ã€‚</div></div></div></div><div class="row-container" id="task-16"><div class="row text-row "><div class="col-src">Vision Language Models. Since the release of GPT [1, 6], the capabilities of large models have gradually expanded from single modality to multi-modalities. Large vision language models [1, 3, 24, 38] now demonstrate superior abilities in visual understanding and reasoning. Early works attempt to integrate visual models with large language models (LLMs), Flamingo [2] uses a visual encoder to process visual signals and adds attention layers in the LLM decoder to interact with the visual features.

BLIP [20, 21] introduces the Q-Former architecture and cross-modal contrastive learning tasks to bridge the vision encoder with LLMs. LLaVA [23, 24] propose using vanilla MLP as the connector between the visual encoder and LLMs, which achieves impressive visual understanding capabilities with relatively limited data.</div><div class="col-trans">è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è‡ªGPT[1, 6]å‘å¸ƒä»¥æ¥ï¼Œå¤§å‹æ¨¡å‹çš„èƒ½åŠ›é€æ¸ä»å•ä¸€æ¨¡æ€æ‰©å±•åˆ°å¤šæ¨¡æ€ã€‚ç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹[1, 3, 24, 38]ç°åœ¨åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ—©æœŸçš„å·¥ä½œå°è¯•å°†è§†è§‰æ¨¡å‹ä¸å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ•´åˆï¼ŒFlamingo [2] ä½¿ç”¨è§†è§‰ç¼–ç å™¨å¤„ç†è§†è§‰ä¿¡å·ï¼Œå¹¶åœ¨LLMè§£ç å™¨ä¸­æ·»åŠ æ³¨æ„åŠ›å±‚ä»¥ä¸è§†è§‰ç‰¹å¾äº¤äº’ã€‚

BLIP [20, 21] å¼•å…¥äº†Q-Formeræ¶æ„å’Œè·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ ä»»åŠ¡ï¼Œå°†è§†è§‰ç¼–ç å™¨ä¸LLMsè¿æ¥èµ·æ¥ã€‚LLaVA [23, 24] æå‡ºä½¿ç”¨æ ‡å‡†çš„MLPä½œä¸ºè§†è§‰ç¼–ç å™¨ä¸LLMsä¹‹é—´çš„è¿æ¥å™¨ï¼Œåœ¨ç›¸å¯¹æœ‰é™çš„æ•°æ®ä¸‹å®ç°äº†å‡ºè‰²çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 16)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('16', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-17"><div class="row text-row "><div class="col-src">The QwenVL [3, 41] series continuously improve the visual module, offering better support for high-resolution and dynamic resolution images, while also demonstrate excellent performance in multilingual tasks and spatial perception.

Reinforcement Learning and Reasoning. Autoregressive learning [39] is currently the mainstream pre-training strategy for LLMs. Besides, RL and reasoning techniques further enhance the capabilities of large models [26, 31â€“ 33, 43]. For instance, GPT [1] employs RL with Human Feedback (RLHF) [26], which incorporates human feedback into the training process.

By integrating human intentions and behavioral preferences, RLHF enables LLMs to generate outputs that align more closely with human habits and preferences. Direct Preference Optimization (DPO) [31] enhances the modelâ€™s performance by directly optimizing preference feedback.</div><div class="col-trans">QwenVL [3, 41] ç³»åˆ—ä¸æ–­æ”¹è¿›è§†è§‰æ¨¡å—ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å’ŒåŠ¨æ€åˆ†è¾¨ç‡å›¾åƒæä¾›æ›´å¥½çš„æ”¯æŒï¼Œå¹¶ä¸”åœ¨å¤šè¯­è¨€ä»»åŠ¡å’Œç©ºé—´æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

å¼ºåŒ–å­¦ä¹ ä¸æ¨ç†ã€‚è‡ªå›å½’å­¦ä¹  [39] ç›®å‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„è®­ç»ƒçš„ä¸»è¦ç­–ç•¥ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºäº†å¤§æ¨¡å‹çš„èƒ½åŠ› [26, 31-33, 43]ã€‚ä¾‹å¦‚ï¼ŒGPT [1] ä½¿ç”¨äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) [26]ï¼Œå°†äººç±»åé¦ˆèå…¥åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚

é€šè¿‡æ•´åˆäººç±»æ„å›¾å’Œè¡Œä¸ºåå¥½ï¼ŒRLHF ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆäººç±»ä¹ æƒ¯å’Œåå¥½çš„è¾“å‡ºã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰[31] åˆ™é€šè¿‡ç›´æ¥ä¼˜åŒ–åå¥½åé¦ˆæ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 17)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('17', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-18"><div class="row text-row "><div class="col-src">Building on this, Group Relative Policy Optimization (GRPO) [33] introduces a strategy of group relative optimization, which considers the relative superiority or inferiority between multiple output groups, further improving the stability and effectiveness of the training process.

The recent DeepSeek R1 [14] experiences an â€œAha Momentâ€during training based on GRPO, where, without any explicit guidance, the model autonomously allocates more thinking to the problem and re-evaluates its initial approach. This highlights the potential of RL in enabling large models to evolve from mere imitation to emergent intelligence.</div><div class="col-trans">åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒGroup Relative Policy Optimization (GRPO) [33] å¼•å…¥äº†ä¸€ç§ç¾¤ä½“ç›¸å¯¹ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è€ƒè™‘äº†å¤šä¸ªè¾“å‡ºç¾¤ä½“ä¹‹é—´çš„ç›¸å¯¹ä¼˜è¶Šæ€§æˆ–åŠ£åŠ¿ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

æœ€è¿‘ï¼Œåœ¨ GRPO çš„æŒ‡å¯¼ä¸‹ï¼ŒDeepSeek R1 [14] åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»å†äº†â€œé¡¿æ‚Ÿâ€æ—¶åˆ»ã€‚åœ¨æ²¡æœ‰ä»»ä½•æ˜ç¡®æŒ‡å¯¼çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹è‡ªä¸»åœ°å°†æ›´å¤šçš„æ€è€ƒæŠ•å…¥åˆ°é—®é¢˜ä¸­ï¼Œå¹¶é‡æ–°è¯„ä¼°å…¶åˆå§‹æ–¹æ³•ã€‚è¿™çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ä½¿å¤§å‹æ¨¡å‹ä»ç®€å•çš„æ¨¡ä»¿è¿›åŒ–åˆ°æ¶Œç°æ™ºèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 18)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('18', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-19"><div class="row text-row "><div class="col-src">In our experients, we are also excited to discover that, after GRPO-based RL training, AlphaDrive demonstrates some emergent multimodal planning capabilities, enabling it to generate multiple reasonable driving plans. We believe it has great potential to improve driving safety and efficiency. In terms of reasoning, Chain-of-thought [43] has demonstrated great performance in solving complex problems by breaking them down and reasoning step by step.

OpenAI o1 [25], which is based on Chain-of-thought, introduces inference-time scaling. By increasing the computational cost during inference and combining search strategies such as Monte Carlo Tree Search (MCTS) [35] and Beam Search [46], significant improvements have been achieved in areas such as science and programming that require complex reasoning.

This also shows that, beyond scaling model parameters and training data, scaling the inference-time computation is also a promising direction for exploration.</div><div class="col-trans">åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¿˜æ¬£å–œåœ°å‘ç°ï¼Œåœ¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¹‹åï¼ŒAlphaDriveå±•ç¤ºå‡ºäº†ä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¤šä¸ªåˆç†çš„é©¾é©¶è®¡åˆ’ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›æ¥æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚ä»æ¨ç†çš„è§’åº¦æ¥çœ‹ï¼ŒChain-of-thought [43] é€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£å¹¶é€æ­¥æ¨ç†ï¼Œåœ¨è§£å†³å¤æ‚é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

OpenAI o1 [25] åŸºäº Chain-of-thoughtï¼Œå¼•å…¥äº†æ¨ç†æ—¶çš„æ‰©å±•ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¢åŠ è®¡ç®—æˆæœ¬ï¼Œå¹¶ç»“åˆè¯¸å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰[35] å’ŒæŸæœç´¢ï¼ˆBeam Searchï¼‰[46] ç­‰æœç´¢ç­–ç•¥ï¼Œå·²ç»åœ¨éœ€è¦å¤æ‚æ¨ç†çš„ç§‘å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚

è¿™è¡¨æ˜ï¼Œåœ¨æ¨¡å‹å‚æ•°å’Œè®­ç»ƒæ•°æ®ä¹‹å¤–ï¼Œæ‰©å±•æ¨ç†æ—¶çš„è®¡ç®—ä¹Ÿæ˜¯ä¸€ä¸ªå€¼å¾—æ¢ç´¢çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 19)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('19', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-20"><div class="row text-row "><div class="col-src">Autonomous Driving Planning. Planning is the ultimate task of autonomous driving. The earliest planning algorithms are rule-based [27, 36], which have significant limitations in terms of generalizability and efficiency. Recently, end-to-end models [8, 13, 16, 17, 22, 29] has gained popularity, where a unified neural network is used to directly output planning trajectories or control signals from sensor data.

By leveraging large-scale driving demonstrations, end-toend models are trained in a data-driven manner, achieving impressive planning performance. However, since end-toend models are black-box models that lack common-sense and reasoning capabilities, they still struggle to address the long-tailed problems in autonomous driving.

VLMs and Autonomous Driving. The common-sense and reasoning abilities of large models can effectively compensate for the limitations of end-to-end models in autonomous</div><div class="col-trans">è‡ªä¸»é©¾é©¶è§„åˆ’ã€‚è§„åˆ’æ˜¯è‡ªåŠ¨é©¾é©¶çš„æœ€ç»ˆä»»åŠ¡ã€‚æœ€æ—©çš„è§„åˆ’ç®—æ³•åŸºäºè§„åˆ™[27, 36]ï¼Œåœ¨é€šç”¨æ€§å’Œæ•ˆç‡æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚è¿‘å¹´æ¥ï¼Œç«¯åˆ°ç«¯æ¨¡å‹[8, 13, 16, 17, 22, 29]é€æ¸æµè¡Œèµ·æ¥ï¼Œåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œç»Ÿä¸€çš„ç¥ç»ç½‘ç»œå¯ä»¥ç›´æ¥ä»ä¼ æ„Ÿå™¨æ•°æ®è¾“å‡ºè§„åˆ’è½¨è¿¹æˆ–æ§åˆ¶ä¿¡å·ã€‚

é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡é©¾é©¶æ¼”ç¤ºæ•°æ®ï¼Œç«¯åˆ°ç«¯æ¨¡å‹ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è§„åˆ’æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºç«¯åˆ°ç«¯æ¨¡å‹æ˜¯é»‘ç›’æ¨¡å‹ï¼Œç¼ºä¹å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥åº”å¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„é•¿å°¾é—®é¢˜ã€‚

å¤§æ¨¡å‹ä¸è‡ªä¸»é©¾é©¶ã€‚å¤§å‹æ¨¡å‹çš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›å¯ä»¥æœ‰æ•ˆå¼¥è¡¥ç«¯åˆ°ç«¯æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶æ–¹é¢çš„å±€é™æ€§ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 20)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('20', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-21"><div class="row text-row "><div class="col-src">driving. In the field of robotics, Vision-Language-Action (VLA) models [5, 19, 44] have made significant progress in understanding language instructions and executing complex actions. A common approach is to use VLMs as the planning module to generate planning instructions, which are then translated into control signals through an action model. There have also been some works based on large models in the field of autonomous driving.

DriveGPT4 [47] utilizes a VLM that takes front-view videos as input, and the model directly predicts control signals. ELM [49] leverages largescale, cross-domain video training for VLMs, showing that using data from various domains can effectively enhance the performance of VLMs in driving-related tasks. OmniDrive [42] proposes the use of sparse 3D tokens to represent driving scenes, which are then input into VLMs for scene understanding and planning.</div><div class="col-trans">é©¾é©¶ã€‚åœ¨æœºå™¨äººé¢†åŸŸï¼ŒVision-Language-Action (VLA) æ¨¡å‹ [5, 19, 44] åœ¨ç†è§£å’Œæ‰§è¡Œå¤æ‚æŒ‡ä»¤æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨ VLM ä½œä¸ºè§„åˆ’æ¨¡å—ç”Ÿæˆè§„åˆ’æŒ‡ä»¤ï¼Œç„¶åé€šè¿‡åŠ¨ä½œæ¨¡å‹å°†è¿™äº›æŒ‡ä»¤è½¬æ¢ä¸ºæ§åˆ¶ä¿¡å·ã€‚æ­¤å¤–ï¼Œåœ¨è‡ªä¸»é©¾é©¶é¢†åŸŸä¹Ÿå‡ºç°äº†ä¸€äº›åŸºäºå¤§å‹æ¨¡å‹çš„ç ”ç©¶ã€‚

DriveGPT4 [47] åˆ©ç”¨ä¸€ä¸ªä»¥å‰æ–¹è§†é¢‘ä¸ºè¾“å…¥çš„ VLMï¼Œå¹¶ç›´æ¥é¢„æµ‹æ§åˆ¶ä¿¡å·ã€‚ELM [49] é€šè¿‡å¤§è§„æ¨¡è·¨åŸŸè§†é¢‘è®­ç»ƒæ¥æå‡ VLM çš„æ€§èƒ½ï¼Œè¡¨æ˜ä½¿ç”¨æ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®å¯ä»¥æœ‰æ•ˆå¢å¼º VLM åœ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚OmniDrive [42] æå‡ºäº†ä½¿ç”¨ç¨€ç–ä¸‰ç»´æ ‡è®°æ¥è¡¨ç¤ºé©¾é©¶åœºæ™¯çš„æ–¹æ³•ï¼Œç„¶åå°†è¿™äº›æ ‡è®°è¾“å…¥åˆ° VLM ä¸­è¿›è¡Œåœºæ™¯ç†è§£å’Œè§„åˆ’ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 21)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('21', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-22"><div class="row text-row "><div class="col-src">In addition to the above works that directly apply VLMs to driving, DriveVLM [37] combines VLMs with end-toend models for the first time, where VLMs predict lowfrequency trajectories and an end-to-end model generates high-frequency trajectories. Senna [18] proposes a framework where VLMs handle high-level planning, while endto-end models are responsible for low-level trajectory prediction.

Additionally, several datasets and benchmarks have been proposed [30, 34, 34, 45], which promote the application of VLMs in autonomous driving. However, most of the current works on VLMs in the field of autonomous driving involves directly using pre-trained models and then utilizing SFT on driving data, which lacks in-depth exploration on training strategies specifically designed for planning.

Further effort is needed to adapt the impressive RL and reasoning techniques from general tasks to autonomous driving.</div><div class="col-trans">é™¤äº†ä¸Šè¿°ç›´æ¥å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ç”¨äºé©¾é©¶çš„å·¥ä½œå¤–ï¼ŒDriveVLM [37] é¦–æ¬¡ç»“åˆäº† VLMs ä¸ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå…¶ä¸­ VLMs é¢„æµ‹ä½é¢‘è½¨è¿¹ï¼Œè€Œç«¯åˆ°ç«¯æ¨¡å‹ç”Ÿæˆé«˜é¢‘è½¨è¿¹ã€‚Senna [18] æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œåœ¨è¯¥æ¡†æ¶ä¸­ï¼ŒVLMs è´Ÿè´£é«˜çº§è§„åˆ’ï¼Œè€Œç«¯åˆ°ç«¯æ¨¡å‹åˆ™è´Ÿè´£ä½çº§è½¨è¿¹é¢„æµ‹ã€‚

æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¤šä¸ªæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯• [30, 34, 34, 45]ï¼Œè¿™äº›éƒ½ä¿ƒè¿›äº† VLMs åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°å…³äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„ VLMs å·¥ä½œä¸»è¦æ¶‰åŠç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨é©¾é©¶æ•°æ®ä¸Šè¿›è¡Œå°‘é‡çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç¼ºä¹é’ˆå¯¹è§„åˆ’ç­–ç•¥çš„å…·ä½“è®­ç»ƒæ–¹æ³•çš„æ·±å…¥æ¢ç´¢ã€‚

ä¸ºäº†è¿›ä¸€æ­¥å°†é€šç”¨ä»»åŠ¡ä¸­çš„å¼ºå¤§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æŠ€æœ¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ï¼Œè¿˜éœ€è¦åšå‡ºæ›´å¤šåŠªåŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 22)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('22', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-23"><div class="row text-row "><div class="col-src">[[HEADER: 3. AlphaDrive]]</div><div class="col-trans"><b>3. AlphaDrive</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 23)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('23', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-24"><div class="row text-row "><div class="col-src">[[HEADER: 3.1. Overview]]</div><div class="col-trans"><b>3.1. æ¦‚è¿°</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 24)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('24', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-25"><div class="row text-row "><div class="col-src">AlphaDrive is a VLM designed for autonomous driving planning. Unlike previous approaches that rely solely on SFT, we explore the incorporation of RL and reasoning techniques to better align with the unique characteristics of driving planning: (1) the varying importance of different driving behaviors; (2) the existence of multiple feasible solutions; and (3) the scarcity of readily available reasoning data for planning decisions.

We propose four GRPO-based RL rewards tailored for planning, along with a two-stage planning-reasoning training strategy that integrates SFT with RL. Our experiments demonstrate that, compared to using SFT alone or training without reasoning, AlphaDrive achieves significant improvements in both planning performance and training efficiency. In the following sections, we will detail the design of each component.</div><div class="col-trans">AlphaDrive æ˜¯ä¸€ç§ç”¨äºè‡ªä¸»é©¾é©¶è§„åˆ’çš„VLMã€‚ä¸ä¹‹å‰ä»…ä¾èµ–äºSFTçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†RLå’Œæ¨ç†æŠ€æœ¯ç»“åˆè¿›æ¥ï¼Œä»¥æ›´å¥½åœ°é€‚åº”é©¾é©¶è§„åˆ’çš„ç‹¬ç‰¹ç‰¹æ€§ï¼š(1) ä¸åŒé©¾é©¶è¡Œä¸ºçš„é‡è¦æ€§å„å¼‚ï¼›(2) å­˜åœ¨å¤šç§å¯è¡Œè§£å†³æ–¹æ¡ˆï¼›ä»¥åŠ (3) è§„åˆ’å†³ç­–ä¸­å¯ç”¨çš„æ¨ç†æ•°æ®ç¨€ç¼ºã€‚

æˆ‘ä»¬æå‡ºäº†å››ç§é’ˆå¯¹è§„åˆ’çš„GRPOåŸºRLå¥–åŠ±ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè§„åˆ’-æ¨ç†è®­ç»ƒç­–ç•¥ï¼Œå°†SFTä¸RLç›¸ç»“åˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨SFTæˆ–ä¸è¿›è¡Œæ¨ç†çš„è®­ç»ƒç›¸æ¯”ï¼ŒAlphaDrive åœ¨è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†æè¿°æ¯ä¸ªç»„ä»¶çš„è®¾è®¡ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 25)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('25', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-26"><div class="row text-row "><div class="col-src">[[HEADER: 3.2. Planning-oriented Reinforcement Learning]]</div><div class="col-trans"><b>3.2. è®¡åˆ’å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ </b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 26)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('26', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-27"><div class="row text-row "><div class="col-src">[[HEADER: 3.2.1. Reinforcement Learning Algorithm]]</div><div class="col-trans"><b>3.2.1. å¼ºåŒ–å­¦ä¹ ç®—æ³•</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 27)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('27', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Equation_1"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_1</div><img src="./assets/Equation_1.png" class="asset-img-raw" loading="lazy"></div></div><div class="row asset-row" id="Equation_2"><div class="asset-card placeholder-card"><div class="asset-header-mini">Equation_2</div><img src="./assets/Equation_2.png" class="asset-img-raw" loading="lazy"></div></div><div class="row-container" id="task-28"><div class="row text-row "><div class="col-src">Current commonly used RL algorithms include PPO [32], DPO [31], and GRPO [33]. Given a query q, GRPO samples a group of outputs {o1, o2, Â· Â· Â· , oG} from the old policy Ï€Î¸old and optimizes the new policy Ï€Î¸ by maximizing:

where wi = Ï€Î¸(oi|q) Ï€Î¸old(oi|q), Ïµ and Î² are hyper-parameters, and the advantage Ai is computed using the normalized reward within the group. We ultimately choose GRPO as the RL algorithm for AlphaDrive for two key reasons: (1) DeepSeek R1 [14] has demonstrated the effectiveness of GRPO in general domains.

Compared to other algorithms, GRPO provides higher training stability and efficiency; (2) Moreover, the group relative optimization strategy introduced by GRPO is particularly well-suited for planning, as planning often involves multiple valid solutions, making relative optimization across multiple solutions is a natural fit. Experimental results further confirm that models trained with GRPO exhibit strong planning capabilities.</div><div class="col-trans">å½“å‰å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åŒ…æ‹¬PPO [32]ã€DPO [31] å’ŒGRPO [33]ã€‚ç»™å®šä¸€ä¸ªæŸ¥è¯¢qï¼ŒGRPOä»æ—§ç­–ç•¥Ï€Î¸oldä¸­é‡‡æ ·ä¸€ç»„è¾“å‡º{o1, o2, Â·Â·Â·, oG}ï¼Œå¹¶é€šè¿‡æœ€å¤§åŒ–ä»¥ä¸‹ç›®æ ‡ä¼˜åŒ–æ–°ç­–ç•¥Ï€Î¸ï¼š

\[ \sum_{i=1}^{G} w_i A_i \]

å…¶ä¸­wi = Ï€Î¸(oi|q) Ï€Î¸old(oi|q)ï¼ŒÏµå’ŒÎ²æ˜¯è¶…å‚æ•°ï¼Œä¼˜åŠ¿Aié€šè¿‡ç»„å†…å½’ä¸€åŒ–å¥–åŠ±è®¡ç®—å¾—å‡ºã€‚æˆ‘ä»¬æœ€ç»ˆé€‰æ‹©GRPOä½œä¸ºAlphaDriveçš„RLç®—æ³•ï¼ŒåŸå› æœ‰ä¸¤ç‚¹ï¼šï¼ˆ1ï¼‰DeepSeek R1 [14] å·²ç»è¯æ˜äº†åœ¨é€šç”¨é¢†åŸŸä¸­GRPOçš„æœ‰æ•ˆæ€§ï¼›ï¼ˆ2ï¼‰æ­¤å¤–ï¼ŒGRPOå¼•å…¥çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŒ–ç­–ç•¥ç‰¹åˆ«é€‚åˆè§„åˆ’ï¼Œå› ä¸ºè§„åˆ’é€šå¸¸æ¶‰åŠå¤šä¸ªæœ‰æ•ˆè§£ï¼Œå› æ­¤åœ¨å¤šä¸ªè§£å†³æ–¹æ¡ˆä¹‹é—´è¿›è¡Œç›¸å¯¹ä¼˜åŒ–æ˜¯è‡ªç„¶çš„é€‰æ‹©ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œä½¿ç”¨GRPOè®­ç»ƒçš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§„åˆ’èƒ½åŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 28)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('28', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-29"><div class="row text-row "><div class="col-src">[[HEADER: 3.2.2. Planning Reward Modeling]]</div><div class="col-trans"><b>3.2.2 è®¡åˆ’å¥–åŠ±å»ºæ¨¡</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 29)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('29', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Algorithm_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Algorithm_1</div><img src="./assets/Algorithm_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">extrat ans will extract substrings that match the specified pattern from the given string. cal f1 score will calculate F1 score given the predictions and ground truth. check format will check whether the given string matches the provided pattern based on regular expression matching.</div><div class="asset-desc-zh">ç®—æ³•_1ï¼šæå– ans å°†ä»ç»™å®šå­—ç¬¦ä¸²ä¸­æå–åŒ¹é…æŒ‡å®šæ¨¡å¼çš„å­ä¸²ã€‚cal f1 score å°†æ ¹æ®é¢„æµ‹å’ŒçœŸå®å€¼è®¡ç®— F1 åˆ†æ•°ã€‚check format å°†æ£€æŸ¥ç»™å®šå­—ç¬¦ä¸²æ˜¯å¦ä¸æä¾›çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…ã€‚</div></div></div></div><div class="row asset-row" id="Table_1"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_1</div><img src="./assets/Table_1.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 1. High-level planning and reasoning evaluation results on the MetaAD dataset. Except for AlphaDrive, which utilizes our proposed training strategy, all other models are trained based on SFT. â€  denotes fine-tuned on the MetaAD dataset.</div><div class="asset-desc-zh">è¡¨ 1. åœ¨ MetaAD æ•°æ®é›†ä¸Šçš„é«˜çº§è§„åˆ’å’Œæ¨ç†è¯„ä¼°ç»“æœã€‚é™¤äº†ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„è®­ç»ƒç­–ç•¥è¿›è¡Œè®­ç»ƒçš„ AlphaDrive å¤–ï¼Œå…¶ä»–æ‰€æœ‰æ¨¡å‹å‡åŸºäº SFT è®­ç»ƒã€‚â€  è¡¨ç¤ºåœ¨ MetaAD æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚</div></div></div></div><div class="row asset-row" id="Table_2"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_2</div><img src="./assets/Table_2.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 2. Ablations on the effectiveness of our proposed planning GRPO rewards.</div><div class="asset-desc-zh">è¡¨ 2. å¯¹æˆ‘ä»¬æå‡ºè§„åˆ’ GRPO å¥–åŠ±æœ‰æ•ˆæ€§çš„æ¶ˆèç ”ç©¶ã€‚</div></div></div></div><div class="row-container" id="task-30"><div class="row text-row "><div class="col-src">Planning Accuracy Reward. In fields such as mathematics or programming, the reward in GRPO can be intuitively determined based on whether the final answer is correct. However, planning is more complex, as it involves both lateral (direction) and longitudinal (speed) components. Furthermore, the set of possible actions is constrained. As a result, we use the F1-Score to evaluate the accuracy of both lateral and longitudinal decisions separately, and assign rewards accordingly.

Initially, we evaluate accuracy by checking whether the modelâ€™s prediction exactly matches the ground truth. However, due to imperfect format in the modelâ€™s early training phase, such as discrepancies in case sensitivity or the presence of extraneous outputs, this approach results in poor stability during the early stages of training. We then attempt to extract all the words from the prediction and check whether the ground truth is included among the words.</div><div class="col-trans">è§„åˆ’å‡†ç¡®æ€§å¥–åŠ±ã€‚åœ¨æ•°å­¦æˆ–ç¼–ç¨‹ç­‰é¢†åŸŸä¸­ï¼ŒGRPOä¸­çš„å¥–åŠ±å¯ä»¥æ ¹æ®æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ç›´è§‚åœ°ç¡®å®šã€‚ç„¶è€Œï¼Œåœ¨è§„åˆ’æ–¹é¢æ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºå®ƒæ¶‰åŠæ¨ªå‘ï¼ˆæ–¹å‘ï¼‰å’Œçºµå‘ï¼ˆé€Ÿåº¦ï¼‰ä¸¤ä¸ªç»´åº¦ã€‚æ­¤å¤–ï¼Œå¯èƒ½çš„åŠ¨ä½œé›†å—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨F1-Scoreåˆ†åˆ«è¯„ä¼°æ¨ªå‘å’Œçºµå‘å†³ç­–çš„å‡†ç¡®æ€§ï¼Œå¹¶æ®æ­¤åˆ†é…å¥–åŠ±ã€‚

æœ€åˆï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ¨¡å‹é¢„æµ‹æ˜¯å¦å®Œå…¨ä¸çœŸå®å€¼åŒ¹é…æ¥è¯„ä¼°å‡†ç¡®æ€§ã€‚ä½†ç”±äºæ¨¡å‹æ—©æœŸè®­ç»ƒé˜¶æ®µæ ¼å¼ä¸Šçš„ä¸å®Œç¾ï¼Œä¾‹å¦‚å¤§å°å†™å·®å¼‚æˆ–å­˜åœ¨å¤šä½™è¾“å‡ºç­‰é—®é¢˜ï¼Œåœ¨è®­ç»ƒåˆæœŸä¼šå¯¼è‡´ç¨³å®šæ€§è¾ƒå·®ã€‚éšåï¼Œæˆ‘ä»¬å°è¯•ä»é¢„æµ‹ä¸­æå–æ‰€æœ‰å•è¯ï¼Œå¹¶æ£€æŸ¥çœŸå®å€¼æ˜¯å¦åŒ…å«åœ¨è¿™äº›å•è¯ä¹‹ä¸­ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 30)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('30', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-31"><div class="row text-row "><div class="col-src">This introduces a new issue where the model sometimes learns shortcut solutions, such as outputting all possible actions, which causes mode collapse. Ultimately, we adopt the F1-score for evaluation, as it not only prevents the model from learning shortcut solutions (where outputting all decisions could result in high recall but low accuracy) but also improves the stability during the early training phase.

Action-Weighted Reward. As mentioned above, the importance of different behaviors in planning varies. For instance, decelerating and stopping are more critical for safety than maintaining speed. Therefore, we assign different importance weights to various actions, incorporating them as weighted components in the final reward.</div><div class="col-trans">è¿™å¼•å…¥äº†ä¸€ä¸ªæ–°é—®é¢˜ï¼Œå³æ¨¡å‹æœ‰æ—¶ä¼šå­¦ä¹ æ·å¾„è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚è¾“å‡ºæ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œä»è€Œå¯¼è‡´æ¨¡å¼å´©æºƒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬é‡‡ç”¨F1åˆ†æ•°è¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºF1åˆ†æ•°ä¸ä»…èƒ½é˜²æ­¢æ¨¡å‹å­¦ä¹ æ·å¾„è§£å†³æ–¹æ¡ˆï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºæ‰€æœ‰å†³ç­–å¯èƒ½ä¼šå¯¼è‡´å¬å›ç‡é«˜ä½†å‡†ç¡®ç‡ä½ï¼‰ï¼Œè¿˜èƒ½åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µæé«˜ç¨³å®šæ€§ã€‚

åŠ¨ä½œåŠ æƒå¥–åŠ±ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œåœ¨è§„åˆ’ä¸­ä¸åŒè¡Œä¸ºçš„é‡è¦æ€§å„ä¸ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå‡é€Ÿå’Œåœæ­¢å¯¹äºå®‰å…¨æ¥è¯´æ¯”ä¿æŒé€Ÿåº¦æ›´ä¸ºå…³é”®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ºå„ç§åŠ¨ä½œåˆ†é…ä¸åŒçš„é‡è¦æ€§æƒé‡ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºæœ€ç»ˆå¥–åŠ±ä¸­çš„åŠ æƒç»„ä»¶è¿›è¡Œæ•´åˆã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 31)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('31', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-32"><div class="row text-row "><div class="col-src">Planning Diversity Reward. Since planning is inherently multimodal, during GRPO-based RL training, the model generates multiple solutions for group relative optimization. In the later stages of training, we observe that the modelâ€™s outputs tend to converge to the same solution. Our goal is to encourage the model to generate a variety of feasible solutions, rather than merely aligning with the ground truth

actions in the training data. To achieve this, we propose the Planning Diversity Reward. When the modelâ€™s outputs differ, we assign a higher reward; otherwise, we reduce the reward.</div><div class="col-trans">è§„åˆ’å¤šæ ·æ€§å¥–åŠ±ã€‚ç”±äºè§„åˆ’æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œåœ¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šç”Ÿæˆå¤šä¸ªè§£å†³æ–¹æ¡ˆä»¥å®ç°ç¾¤ä½“ç›¸å¯¹ä¼˜åŒ–ã€‚åœ¨è®­ç»ƒåæœŸï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹çš„è¾“å‡ºå€¾å‘äºæ”¶æ•›äºåŒä¸€ä¸ªè§£ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é¼“åŠ±æ¨¡å‹ç”Ÿæˆå¤šç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸è®­ç»ƒæ•°æ®ä¸­çš„çœŸå®åŠ¨ä½œå¯¹é½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†è§„åˆ’å¤šæ ·æ€§å¥–åŠ±ã€‚å½“æ¨¡å‹çš„è¾“å‡ºä¸åŒæ—¶ï¼Œæˆ‘ä»¬ä¼šç»™äºˆæ›´é«˜çš„å¥–åŠ±ï¼›å¦åˆ™ï¼Œä¼šé™ä½å¥–åŠ±ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 32)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('32', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-33"><div class="row text-row "><div class="col-src">Planning Format Reward. The last reward is used to regularize the output, making it easier to extract both the reasoning process and the final answer. This approach is inspired by R1. The reasoning process is encapsulated within the <think></think> tags, while the planning result is enclosed within the <answer></answer> tags. If the final output does not conform to this format, the format reward will be set to 0.

The Planning Accuracy Reward, the Action-Weighted Reward, and the Planning Diversity Reward are multiplied to compute the Planning Quality Reward. We calculate the Planning Quality Reward separately for speed planning and direction planning. Finally, the Planning Quality Reward and the Planning Format Reward are used to calculate the GRPO loss and update the model parameters. For details about Planning Reward Modeling, please refer to Alg. 1.</div><div class="col-trans">è§„åˆ’æ ¼å¼å¥–åŠ±ã€‚æœ€åä¸€ä¸ªå¥–åŠ±ç”¨äºæ­£åˆ™åŒ–è¾“å‡ºï¼Œä½¿å…¶æ›´å®¹æ˜“æå–æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•å—åˆ°R1çš„å¯å‘ã€‚æ¨ç†è¿‡ç¨‹è¢«å°è£…åœ¨<think></think>æ ‡ç­¾å†…ï¼Œè€Œè§„åˆ’ç»“æœåˆ™åŒ…å«åœ¨<answer></answer>æ ‡ç­¾å†…ã€‚å¦‚æœæœ€ç»ˆè¾“å‡ºä¸ç¬¦åˆæ­¤æ ¼å¼ï¼Œåˆ™æ ¼å¼å¥–åŠ±å°†è®¾ä¸º0ã€‚

è§„åˆ’å‡†ç¡®æ€§å¥–åŠ±ã€åŠ¨ä½œåŠ æƒå¥–åŠ±å’Œè§„åˆ’å¤šæ ·æ€§å¥–åŠ±ç›¸ä¹˜ä»¥è®¡ç®—è§„åˆ’è´¨é‡å¥–åŠ±ã€‚æˆ‘ä»¬åˆ†åˆ«å¯¹é€Ÿåº¦è§„åˆ’å’Œæ–¹å‘è§„åˆ’è®¡ç®—è§„åˆ’è´¨é‡å¥–åŠ±ã€‚æœ€åï¼Œä½¿ç”¨è§„åˆ’è´¨é‡å¥–åŠ±å’Œè§„åˆ’æ ¼å¼å¥–åŠ±æ¥è®¡ç®—GRPOæŸå¤±å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æœ‰å…³è§„åˆ’å¥–åŠ±å»ºæ¨¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§Algorithm 1ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 33)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('33', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-34"><div class="row text-row "><div class="col-src">[[HEADER: 3.3. Reasoning: Distillation from Large Models]]</div><div class="col-trans"><b>3.3. åŸå› ï¼šä»å¤§æ¨¡å‹ä¸­æç‚¼</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 34)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('34', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-35"><div class="row text-row "><div class="col-src">Unlike fields such as mathematics or science, which have abundant high-quality reasoning data available for training, the planning process in autonomous driving is difficult to record, and the cost of manual annotation is high. As a result, there is currently no large-scale, readily available planning reasoning dataset.

We initially attempt to incorporate reasoning steps directly into the RL training process, but the final results are suboptimal, mainly due to the following shortcomings: (1) insufficient perception of key elements, such as traffic lights; (2) disorganized reasoning process with weak causal relationships; (3) reasoning outputs that are overly lengthy and ineffective.</div><div class="col-trans">ä¸æ•°å­¦æˆ–ç§‘å­¦ç­‰å­¦ç§‘ç›¸æ¯”ï¼Œåè€…æ‹¥æœ‰å¤§é‡é«˜è´¨é‡çš„æ¨ç†æ•°æ®å¯ä¾›è®­ç»ƒï¼Œè€Œè‡ªåŠ¨é©¾é©¶è§„åˆ’è¿‡ç¨‹éš¾ä»¥è®°å½•ï¼Œäººå·¥æ ‡æ³¨çš„æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œç›®å‰å°šä¸å­˜åœ¨å¤§è§„æ¨¡ä¸”æ˜“äºè·å–çš„è§„åˆ’æ¨ç†æ•°æ®é›†ã€‚

æœ€åˆï¼Œæˆ‘ä»¬å°è¯•ç›´æ¥å°†æ¨ç†æ­¥éª¤çº³å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½†æœ€ç»ˆç»“æœä¸å°½å¦‚äººæ„ï¼Œä¸»è¦åŸå› å¦‚ä¸‹ï¼š(1) å¯¹å…³é”®å…ƒç´ ï¼ˆä¾‹å¦‚äº¤é€šç¯ï¼‰çš„æ„ŸçŸ¥ä¸è¶³ï¼›(2) æ¨ç†è¿‡ç¨‹æ‚ä¹±æ— ç« ï¼Œå› æœå…³ç³»è–„å¼±ï¼›(3) æ¨ç†è¾“å‡ºè¿‡äºå†—é•¿ä¸”æ•ˆæœä¸ä½³ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 35)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('35', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-36"><div class="row text-row "><div class="col-src">Therefore, we adopt a more capable cloud-based large model, such as GPT-4o, to generate high-quality planning reasoning data from a small set of driving clips. Specifically, we provide the model with prompts that include the real driving actions in a given scenario, along with the vehicleâ€™s current state and navigation information, prompting the model to generate a concise decision-making process. We find that the quality of the generated reasoning process is pretty good.

After conducting a manual quality check and filtering out samples with obvious errors, we obtain a batch of high-quality planning reasoning data. Subsequently, our model can improve its planning reasoning ability through knowledge distillation based on this data.</div><div class="col-trans">å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨æ›´å…·èƒ½åŠ›çš„åŸºäºäº‘çš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œä»å°‘é‡é©¾é©¶ç‰‡æ®µä¸­ç”Ÿæˆé«˜è´¨é‡çš„è§„åˆ’æ¨ç†æ•°æ®ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å‘æ¨¡å‹æä¾›åŒ…å«ç»™å®šåœºæ™¯ä¸­çš„çœŸå®é©¾é©¶åŠ¨ä½œã€è½¦è¾†å½“å‰çŠ¶æ€å’Œå¯¼èˆªä¿¡æ¯çš„æç¤ºï¼Œä¿ƒä½¿æ¨¡å‹ç”Ÿæˆç®€æ´çš„å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬å‘ç°ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹è´¨é‡ç›¸å½“ä¸é”™ã€‚

åœ¨è¿›è¡Œäººå·¥è´¨é‡æ£€æŸ¥å¹¶è¿‡æ»¤æ‰æ˜æ˜¾é”™è¯¯çš„æ ·æœ¬åï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€æ‰¹é«˜è´¨é‡çš„è§„åˆ’æ¨ç†æ•°æ®ã€‚éšåï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥é€šè¿‡åŸºäºè¿™äº›æ•°æ®çš„çŸ¥è¯†è’¸é¦æ¥æé«˜å…¶è§„åˆ’æ¨ç†èƒ½åŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 36)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('36', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-37"><div class="row text-row "><div class="col-src">[[HEADER: 3.4. Training: SFT Warm-Up, RL Exploration]]</div><div class="col-trans"><b>3.4 è®­ç»ƒï¼šSFT çƒ­èº«ï¼ŒRL æ¢ç´¢</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 37)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('37', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-38"><div class="row text-row "><div class="col-src">RL relies on sparse reward signals, whereas SFT is based on dense supervision, making it more suitable for knowledge distillation. Additionally, we find that relying solely on RL can lead to instability in the early stages of training. Therefore, we use a small amount of data for a warmup phase based on SFT, followed by RL training with the full dataset.

We discover that this approach improves stability in the early stages of training and enhances the modelâ€™s planning reasoning performance, ultimately leading to better overall planning capabilities.</div><div class="col-trans">RL ä¾èµ–ç¨€ç–çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œ SFT åˆ™åŸºäºå¯†é›†ç›‘ç£ï¼Œä½¿å…¶æ›´é€‚åˆçŸ¥è¯†è’¸é¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä»…ä¾é  RL å¯èƒ½åœ¨è®­ç»ƒåˆæœŸå¯¼è‡´ä¸ç¨³å®šæ€§ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨ SFT è¿›è¡Œé¢„çƒ­é˜¶æ®µåï¼Œå†ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œ RL è®­ç»ƒã€‚

æˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•åœ¨è®­ç»ƒåˆæœŸæé«˜äº†ç¨³å®šæ€§å’Œå¢å¼ºäº†æ¨¡å‹çš„è§„åˆ’æ¨ç†æ€§èƒ½ï¼Œæœ€ç»ˆæå‡äº†æ•´ä½“çš„è§„åˆ’èƒ½åŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 38)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('38', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-39"><div class="row text-row "><div class="col-src">[[HEADER: 4. Experiments]]</div><div class="col-trans">4. å®éªŒ<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 39)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('39', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-40"><div class="row text-row "><div class="col-src">[[HEADER: 4.1. Experimental Settings]]</div><div class="col-trans"><b>4.1 å®éªŒè®¾ç½®</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 40)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('40', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Table_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_3</div><img src="./assets/Table_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 3. Ablations on different reasoning training strategies.</div><div class="asset-desc-zh">è¡¨ 3. ä¸åŒæ¨ç†è®­ç»ƒç­–ç•¥çš„æ¶ˆèç ”ç©¶ã€‚</div></div></div></div><div class="row asset-row" id="Table_4"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Table_4</div><img src="./assets/Table_4.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Table 4. Ablations on the amount of training data.</div><div class="asset-desc-zh">è¡¨ 4. ä¸åŒè®­ç»ƒæ•°æ®é‡çš„æ¶ˆèç ”ç©¶ã€‚</div></div></div></div><div class="row-container" id="task-41"><div class="row text-row "><div class="col-src">Dataset. We adopt MetaAD, a large-scale real-world driving dataset, as our training and evaluation benchmark. This dataset consists of a total of 120k driving clips, each lasting three seconds. MetaAD is a high-quality dataset specifically designed for planning, supporting multi-sensor data and perception annotations. Furthermore, it maintains a well-balanced distribution across various driving environments and planning actions.

The dataset is divided into 110k clips for training and 10k clips for validation. As for reasoning, we sample 30k data from the training dataset to

generate the planning reasoning process. All reported results are obtained by training on the training set and evaluating on the validation set.</div><div class="col-trans">æ•°æ®é›†ã€‚æˆ‘ä»¬é‡‡ç”¨MetaADä½œä¸ºè®­ç»ƒå’Œè¯„ä¼°åŸºå‡†ï¼ŒMetaADæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ€»å…±åŒ…å«12ä¸‡æ¡é©¾é©¶ç‰‡æ®µï¼Œæ¯æ¡æŒç»­ä¸‰ç§’ã€‚MetaADæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè§„åˆ’è®¾è®¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œæ”¯æŒå¤šä¼ æ„Ÿå™¨æ•°æ®å’Œæ„ŸçŸ¥æ ‡æ³¨ï¼Œå¹¶ä¸”åœ¨å„ç§é©¾é©¶ç¯å¢ƒå’Œè§„åˆ’åŠ¨ä½œä¸Šä¿æŒäº†è‰¯å¥½çš„å¹³è¡¡åˆ†å¸ƒã€‚

æ•°æ®é›†è¢«åˆ’åˆ†ä¸ºç”¨äºè®­ç»ƒçš„11ä¸‡æ¡ç‰‡æ®µå’Œç”¨äºéªŒè¯çš„1ä¸‡æ¡ç‰‡æ®µã€‚ä¸ºäº†è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬ä»è®­ç»ƒæ•°æ®é›†ä¸­é‡‡æ ·äº†3ä¸‡æ¡æ•°æ®æ¥ç”Ÿæˆè§„åˆ’æ¨ç†è¿‡ç¨‹ã€‚æ‰€æœ‰æŠ¥å‘Šçš„ç»“æœéƒ½æ˜¯é€šè¿‡åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå¹¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¾—åˆ°çš„ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 41)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('41', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-42"><div class="row text-row "><div class="col-src">Training. We use Qwen2VL-2B [41] as the base model. Qwen2VL is currently one of the best-performing opensource models, and it offers a smaller 2B version that better meets the latency requirements for autonomous driving. Additionally, Qwen2VL provides better support for RL. The modelâ€™s inputs include a front-view image and a planning prompt, which contains the vehicleâ€™s current speed and navigation information.

The navigation data, consistent with real-world driving, is obtained from sparse navigation points via AMap (similar to Google Maps) and is converted into text form for inclusion in the prompt, such as â€œGo straight for 100m, then turn rightâ€. Training is conducted using 16 NVIDIA A800 GPUs.</div><div class="col-trans">è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨Qwen2VL-2B [41]ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚Qwen2VLç›®å‰æ˜¯è¡¨ç°æœ€ä½³çš„å¼€æºæ¨¡å‹ä¹‹ä¸€ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ›´ç¬¦åˆè‡ªåŠ¨é©¾é©¶å»¶è¿Ÿè¦æ±‚çš„å°å‹2Bç‰ˆæœ¬ã€‚æ­¤å¤–ï¼ŒQwen2VLåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹é¢æä¾›äº†æ›´å¥½çš„æ”¯æŒã€‚è¯¥æ¨¡å‹çš„è¾“å…¥åŒ…æ‹¬å‰è§†å›¾å›¾åƒå’Œä¸€ä¸ªè§„åˆ’æç¤ºï¼Œå…¶ä¸­åŒ…å«è½¦è¾†å½“å‰çš„é€Ÿåº¦å’Œå¯¼èˆªä¿¡æ¯ã€‚

å¯¼èˆªæ•°æ®ä¸å®é™…é©¾é©¶æƒ…å†µä¸€è‡´ï¼Œé€šè¿‡AMapï¼ˆç±»ä¼¼äºGoogle Mapsï¼‰ä»ç¨€ç–çš„å¯¼èˆªç‚¹è·å–ï¼Œå¹¶è½¬æ¢ä¸ºæ–‡æœ¬å½¢å¼ä»¥åŒ…å«åœ¨æç¤ºä¸­ï¼Œä¾‹å¦‚â€œç›´è¡Œ100ç±³åå³è½¬â€ã€‚è®­ç»ƒä½¿ç”¨äº†16å—NVIDIA A800 GPUè¿›è¡Œã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 42)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('42', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-43"><div class="row text-row "><div class="col-src">Evaluation. The evaluation metrics consist of two aspects. First, the accuracy of meta-action planning is measured by calculating the F1-Score for all categories of lateral and longitudinal meta-actions, followed by the overall planning accuracy. Additionally, for planning reasoning, we compute the similarity between the generated planning reasoning process and the annotated reasoning process in the dataset using BLEU-4 [28], CIDEr [40], and METEOR [4] scores.</div><div class="col-trans">è¯„ä»·ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¡ç®—æ‰€æœ‰ç±»åˆ«ï¼ˆæ¨ªå‘å’Œçºµå‘ï¼‰å…ƒåŠ¨ä½œçš„F1-Scoreæ¥è¡¡é‡å…ƒåŠ¨ä½œè§„åˆ’çš„å‡†ç¡®æ€§ï¼Œéšåè®¡ç®—æ€»ä½“è§„åˆ’å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œåœ¨è§„åˆ’æ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨BLEU-4[[LINK: Equation_2|Eq. 2]]ã€CIDEr[[LINK: Equation_3|Eq. 3]] å’Œ METEOR[[LINK: Equation_4|Eq. 4]] åˆ†æ•°æ¥è®¡ç®—ç”Ÿæˆçš„è§„åˆ’æ¨ç†è¿‡ç¨‹ä¸æ•°æ®é›†ä¸­æ ‡æ³¨çš„æ¨ç†è¿‡ç¨‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 43)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('43', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-44"><div class="row text-row "><div class="col-src">[[HEADER: 4.2. Main Results]]</div><div class="col-trans"><b>4.2 ä¸»è¦ç»“æœ</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 44)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('44', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-45"><div class="row text-row "><div class="col-src">Tab. 1 presents the performance of AlphaDrive in high-level planning. The first four rows show the results obtained by directly evaluating the corresponding pretrained models. It can be observed that, while these models demonstrate stronger general capabilities, their performance in planning is suboptimal, highlighting the need for further training with driving data. The subsequent five rows display the results of models fine-tuned on the MetaAD dataset.

As shown, AlphaDrive significantly outperforms the other models. Compared to Qwen2VL-7B, the second-best performing model after AlphaDrive, the planning accuracy significantly improves by 25.5%. There is a noticeable enhancement in key decisions such as steering and acceleration/deceleration. Additionally, the quality of planning reasoning is the best among all models, demonstrating the effectiveness of our proposed two-stage RL training and reasoning strategies.</div><div class="col-trans"><table_1>å±•ç¤ºäº†AlphaDriveåœ¨é«˜çº§è§„åˆ’ä¸­çš„æ€§èƒ½ã€‚å‰å››è¡Œæ˜¾ç¤ºäº†ç›´æ¥è¯„ä¼°ç›¸åº”é¢„è®­ç»ƒæ¨¡å‹çš„ç»“æœã€‚å¯ä»¥çœ‹å‡ºï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å±•ç°äº†æ›´å¼ºçš„é€šç”¨èƒ½åŠ›ï¼Œä½†åœ¨è§„åˆ’æ–¹é¢çš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ï¼Œçªæ˜¾äº†è¿›ä¸€æ­¥ä½¿ç”¨é©¾é©¶æ•°æ®è¿›è¡Œè®­ç»ƒçš„éœ€æ±‚ã€‚éšåçš„äº”è¡Œå±•ç¤ºäº†åœ¨MetaADæ•°æ®é›†ä¸Šå¾®è°ƒåçš„æ¨¡å‹ç»“æœã€‚

å¦‚è¡¨æ‰€ç¤ºï¼ŒAlphaDriveæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ä¸ç¬¬äºŒåæ¨¡å‹Qwen2VL-7Bç›¸æ¯”ï¼Œå…¶è§„åˆ’å‡†ç¡®æ€§æé«˜äº†25.5%ã€‚ç‰¹åˆ«æ˜¯åœ¨è½¬å‘å’ŒåŠ é€Ÿ/å‡é€Ÿç­‰å…³é”®å†³ç­–æ–¹é¢æœ‰æ˜æ˜¾çš„æå‡ã€‚æ­¤å¤–ï¼Œè§„åˆ’æ¨ç†çš„è´¨é‡åœ¨æ‰€æœ‰æ¨¡å‹ä¸­æœ€ä½³ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ä¸¤é˜¶æ®µRLè®­ç»ƒå’Œæ¨ç†ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 45)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('45', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-46"><div class="row text-row "><div class="col-src">[[HEADER: 4.3. Ablation Study]]</div><div class="col-trans"><b>4.3 å‰Šå‡å®éªŒ</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 46)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('46', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row asset-row" id="Figure_3"><div class="asset-card"><div class="asset-header"><span class="asset-tag">Resource</span> Figure_3</div><img src="./assets/Figure_3.png" class="asset-img" loading="lazy"><div class="asset-desc-box"><div class="asset-desc-en">Figure 3. Qualitative results of AlphaDirve. After RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which has great potential for improving driving safety and efficiency.</div><div class="asset-desc-zh">å›¾ 3. AlphaDrive çš„å®šæ€§ç»“æœã€‚ç»è¿‡ RL è®­ç»ƒåï¼ŒAlphaDrive å±•ç°å‡ºä¸€äº›æ–°å…´çš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ï¼Œè¿™åœ¨æé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</div></div></div></div><div class="row-container" id="task-47"><div class="row text-row "><div class="col-src">Planning Rewards. In Tab. 2, we validate the effectiveness of the four proposed GRPO planning rewards. The Base Accuracy reward directly determines the reward based on whether the response exactly matches the ground truth, a common approach in general domains. As shown, the model using the Base Accuracy reward lags significantly behind across all metrics (ID 1). The combination with the

Planning Format Reward yields a slight improvement. (ID 2). A significant improvement is seen with the adoption of our proposed Planning Accuracy Reward (ID 3). Further enhancement in acceleration/deceleration decisions is achieved by incorporating the Action-Weighted Reward (ID 4). Finally, by combining the Planning Diversity Reward, the best planning performance is achieved (ID 5-6).</div><div class="col-trans">è§„åˆ’å¥–åŠ±ã€‚åœ¨è¡¨2ä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†æ‰€æå‡ºçš„å››ç§GRPOè§„åˆ’å¥–åŠ±çš„æœ‰æ•ˆæ€§ã€‚åŸºç¡€å‡†ç¡®åº¦å¥–åŠ±ç›´æ¥æ ¹æ®å“åº”æ˜¯å¦å®Œå…¨åŒ¹é…çœŸå®å€¼æ¥ç¡®å®šå¥–åŠ±ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨é¢†åŸŸä¸­çš„å¸¸è§åšæ³•ï¼ˆID 1ï¼‰ã€‚ä¸åŸºç¡€å‡†ç¡®åº¦å¥–åŠ±ç»“åˆä½¿ç”¨æ ¼å¼åŒ–å¥–åŠ±å¸¦æ¥äº†è½»å¾®çš„æ”¹è¿›ï¼ˆID 2ï¼‰ã€‚é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„ç›®æ ‡è§„åˆ’å‡†ç¡®åº¦å¥–åŠ±æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ˆID 3ï¼‰ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œåŠ æƒå¥–åŠ±è¿›ä¸€æ­¥å¢å¼ºäº†åŠ é€Ÿ/å‡é€Ÿå†³ç­–ï¼ˆID 4ï¼‰ã€‚æœ€åï¼Œé€šè¿‡ç»“åˆè§„åˆ’å¤šæ ·æ€§å¥–åŠ±ï¼Œå®ç°äº†æœ€ä½³çš„è§„åˆ’è¡¨ç°ï¼ˆID 5-6ï¼‰ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 47)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('47', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-48"><div class="row text-row "><div class="col-src">Reasoning Training Strategies. The ablation study of the reasoning training strategies is shown in Tab. 3. As observed, introducing planning reasoning under different training strategies effectively enhances model performance. Notably, the improvement is especially significant for complex actions such as acceleration and deceleration, demonstrating that reasoning can greatly enhance decision-making in complex scenarios.

Furthermore, the model trained exclusively with RL performs worse in reasoning compared to the model trained with SFT. We attribute this to the limited parameter size of smaller models, which results in insufficient perception and reasoning capabilities. Therefore, incorporating SFT as a warm-up phase and using knowledge distillation to learn the reasoning process from a larger model can effectively address this issue.

By combining SFT and RL, the model achieves the best planning reasoning capabilities.</div><div class="col-trans">æ¨ç†è®­ç»ƒç­–ç•¥ã€‚è¡¨3å±•ç¤ºäº†æ¨ç†è®­ç»ƒç­–ç•¥çš„æ¶ˆèç ”ç©¶ç»“æœã€‚è§‚å¯Ÿå‘ç°ï¼Œåœ¨ä¸åŒè®­ç»ƒç­–ç•¥ä¸‹å¼•å…¥è§„åˆ’æ¨ç†èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ”¹è¿›åœ¨å¤æ‚çš„åŠ¨ä½œï¼ˆå¦‚åŠ é€Ÿå’Œå‡é€Ÿï¼‰ä¸­å°¤ä¸ºæ˜¾è‘—ï¼Œè¡¨æ˜æ¨ç†å¯ä»¥åœ¨å¤æ‚åœºæ™¯ä¸­çš„å†³ç­–åˆ¶å®šæ–¹é¢å‘æŒ¥å·¨å¤§ä½œç”¨ã€‚

æ­¤å¤–ï¼Œä»…ä½¿ç”¨RLè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨æ¨ç†æ–¹é¢çš„è¡¨ç°ä¸å¦‚ä½¿ç”¨SFTè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†å…¶å½’å› äºè¾ƒå°æ¨¡å‹å‚æ•°é‡æœ‰é™ï¼Œå¯¼è‡´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚å› æ­¤ï¼Œåœ¨å¼•å…¥SFTä½œä¸ºé¢„çƒ­é˜¶æ®µï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦ä»å¤§æ¨¡å‹ä¸­å­¦ä¹ æ¨ç†è¿‡ç¨‹å¯ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸€é—®é¢˜ã€‚

é€šè¿‡ç»“åˆSFTå’ŒRLï¼Œæ¨¡å‹èƒ½å¤Ÿå®ç°æœ€ä½³çš„è§„åˆ’æ¨ç†èƒ½åŠ›ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 48)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('48', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-49"><div class="row text-row "><div class="col-src">Amount of Training Data. Tab. 4 shows the impact of training data size on different training strategies. As observed, when the training data size decreases, SFT is more affected. With only 20k training samples, the model trained with RL reaches a planning accuracy of 46.08%, which is significantly higher than that of the SFT-trained model.

When using nearly half of the data, with 50k samples, AlphaDrive already achieves a planning accuracy of 70.83%, demonstrating the efficiency of our training strategy.</div><div class="col-trans">è®­ç»ƒæ•°æ®é‡çš„å½±å“ã€‚è¡¨4å±•ç¤ºäº†ä¸åŒè®­ç»ƒç­–ç•¥åœ¨è®­ç»ƒæ•°æ®è§„æ¨¡å˜åŒ–æ—¶çš„æ•ˆæœã€‚å¦‚è§‚å¯Ÿåˆ°çš„é‚£æ ·ï¼Œå½“è®­ç»ƒæ•°æ®é‡å‡å°‘æ—¶ï¼ŒSFTï¼ˆç›‘ç£ fine-tuningï¼‰å—åˆ°çš„å½±å“æ›´å¤§ã€‚ä»…ä½¿ç”¨20,000ä¸ªè®­ç»ƒæ ·æœ¬æ—¶ï¼Œé‡‡ç”¨RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰è®­ç»ƒçš„æ¨¡å‹è¾¾åˆ°äº†46.08%çš„è§„åˆ’å‡†ç¡®ç‡ï¼Œè¿™æ˜æ˜¾é«˜äºé€šè¿‡SFTè®­ç»ƒçš„æ¨¡å‹ã€‚

åœ¨ä½¿ç”¨è¿‘ä¸€åŠçš„æ•°æ®é‡ï¼Œå³50,000ä¸ªæ ·æœ¬æ—¶ï¼ŒAlphaDriveå·²ç»å®ç°äº†70.83%çš„è§„åˆ’å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜äº†æˆ‘ä»¬è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 49)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('49', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-50"><div class="row text-row "><div class="col-src">[[HEADER: 4.4. Emergence of Multimodal Planning Capability]]</div><div class="col-trans"><b>4.4. å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›çš„æ¶Œç°</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 50)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('50', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-51"><div class="row text-row "><div class="col-src">Fig. 3 illustrates the multimodal planning capability of AlphaDrive after RL training. In complex scenarios, it can effectively generate multiple feasible solutions, whereas the SFT-trained model can only produce a single planning decision. AlphaDrive can be integrated with a downstream action model to dynamically select the optimal solution from multiple options.</div><div class="col-trans">Figure 3 æ˜¾ç¤ºäº† AlphaDrive åœ¨å¼ºåŒ–å­¦ä¹  (RL) è®­ç»ƒåçš„å¤šæ¨¡æ€è§„åˆ’èƒ½åŠ›ã€‚åœ¨å¤æ‚åœºæ™¯ä¸­ï¼ŒAlphaDrive å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå¤šä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä»…ç»è¿‡ SFT è®­ç»ƒçš„æ¨¡å‹åªèƒ½äº§ç”Ÿä¸€ä¸ªå•ä¸€çš„è§„åˆ’å†³ç­–ã€‚AlphaDrive å¯ä»¥ä¸ä¸‹æ¸¸åŠ¨ä½œæ¨¡å‹é›†æˆï¼ŒåŠ¨æ€åœ°ä»å¤šç§é€‰é¡¹ä¸­é€‰æ‹©æœ€ä¼˜è§£ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 51)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('51', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-52"><div class="row text-row "><div class="col-src">[[HEADER: 5. Conclusions and Limitations]]</div><div class="col-trans"><b>5. ç»“è®ºä¸å±€é™æ€§</b><div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 52)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('52', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-53"><div class="row text-row "><div class="col-src">In this work, we propose AlphaDrive, a VLM for high-level planning in autonomous driving. Compared to previous models that solely employed the SFT, we explore the integration of advanced RL and reasoning in planning. Specifically, AlphaDrive introduces a planning-oriented RL strategy based on GRPO and further designs a two-stage planning reasoning training paradigm.

To the best of our knowledge, AlphaDrive is the first to introduce the RL and reasoning to autonomous driving planning, significantly boosting both performance and training efficiency. Currently, due to a lack of rich data annotation, AlphaDrive is still unable to output more complex driving behaviors such as lane changes or nudges.</div><div class="col-trans">åœ¨æœ¬é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaDriveï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶é«˜çº§è§„åˆ’çš„VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ã€‚ä¸ä¹‹å‰ä»…ä¾èµ–äºSFTï¼ˆç›‘ç£ fine-tuningï¼‰çš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†å…ˆè¿›çš„RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰å’Œæ¨ç†é›†æˆåˆ°è§„åˆ’ä¸­çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒAlphaDrive å¼•å…¥äº†ä¸€ç§åŸºäºGRPOï¼ˆGeneralized Reward-Optimized Policyï¼‰çš„é¢å‘è§„åˆ’çš„RLç­–ç•¥ï¼Œå¹¶è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒæ¡†æ¶ã€‚

æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAlphaDrive æ˜¯é¦–ä¸ªåœ¨è‡ªåŠ¨é©¾é©¶è§„åˆ’ä¸­å¼•å…¥RLå’Œæ¨ç†çš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ç›®å‰ï¼Œç”±äºç¼ºä¹ä¸°å¯Œçš„æ•°æ®æ ‡æ³¨ï¼ŒAlphaDrive ä»æ— æ³•è¾“å‡ºæ›´å¤æ‚çš„é©¾é©¶è¡Œä¸ºï¼Œå¦‚å˜é“æˆ–å¾®è°ƒæ–¹å‘ç­‰ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 53)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('53', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div><div class="row-container" id="task-54"><div class="row text-row "><div class="col-src">Additionally, the current planning reasoning data come from pseudo-labels generated by large models based on ground-truth driving actions, which still suffer from inaccurate perception and a failure to capture key factors. Therefore, further systematic validation is required to improve data quality and verify the performance upper bound of AlphaDrive.

Acknowledgments We sincerely thank Hao Gao, Tianheng Cheng, Bencheng Liao, Haoyi Jiang, and Dongli Hu for their valuable feedback on the draft.</div><div class="col-trans">æ­¤å¤–ï¼Œå½“å‰çš„è§„åˆ’æ¨ç†æ•°æ®æ¥æºäºåŸºäºçœŸå®é©¾é©¶åŠ¨ä½œç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼Œè¿™äº›æ•°æ®ä»ç„¶å—åˆ°ä¸å‡†ç¡®æ„ŸçŸ¥å’Œå…³é”®å› ç´ æ•æ‰ä¸è¶³çš„å½±å“ã€‚å› æ­¤ï¼Œéœ€è¦è¿›ä¸€æ­¥è¿›è¡Œç³»ç»Ÿçš„éªŒè¯ä»¥æé«˜æ•°æ®è´¨é‡å¹¶éªŒè¯AlphaDriveçš„æ€§èƒ½ä¸Šé™ã€‚

è‡´è°¢ æˆ‘ä»¬è¯šæŒšåœ°æ„Ÿè°¢é«˜æµ©ã€ç¨‹å¤©æ’ã€å»– Benchengã€æ±Ÿçš“æ¯… å’Œ èƒ¡ä¸œåˆ© åœ¨è‰ç¨¿ä¸­æä¾›çš„å®è´µåé¦ˆæ„è§ã€‚<div class="hint-badge" style="display: none">ğŸ’¡ ä¸Šæ¬¡æç¤º:  </div></div></div><div class="feedback-panel" style="display: none;"><div class="feedback-header">ğŸ› ï¸ äººå·¥çº é”™å‘å¯¼ (Task 54)</div><textarea class="feedback-input" placeholder="è¯·è¾“å…¥ç»™ AI çš„ç¿»è¯‘æç¤º..."></textarea><div style="margin-top:5px;"><button class="btn btn-primary" style="font-size:0.8em; padding:4px 10px;" onclick="saveFeedback('54', this)">ç¡®è®¤ä¿®æ”¹å¹¶æ ‡è®°</button><span class="status-saved">âœ… ä¿å­˜æˆåŠŸ</span></div></div></div></div><div class="ref-section"><pre>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.

Advances in neural information processing systems, 2022. 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 3 [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005. 7 [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.

4 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 1, 3 [7] Long Chen, Oleg Sinavski, Jan HÂ¨unermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. arXiv preprint arXiv:2310.01957, 2023.

2 [8] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 1, 3 [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al.

Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024.

1 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 [12] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. In NeurIPS, 2024.

2 [13] Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, et al. Rad: Training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning. arXiv preprint arXiv:2502.13144, 2025. 3 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.

Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3, 5 [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 2 [16] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al.

Planning-oriented autonomous driving. In CVPR, 2023. 1, 3 [17] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023. 1, 3 [18] Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang.

Senna: Bridging large vision-language models and end-to-end autonomous driving. arXiv preprint arXiv:2410.22313, 2024. 2, 4 [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 4 [20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.

Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. 3 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3 [22] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, et al.

Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. arXiv preprint arXiv:2411.15139, 2024. 1, 3 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 3 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 1, 3 [25] OpenAI. Learning to reason with llms, 2024.

2, 3 [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. 3 [27] Brian Paden, Michal Ë‡CÂ´ap, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and control techniques for self-driving urban vehicles.

IEEE Transactions on intelligent vehicles, 2016. 3 [28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 7 [29] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multimodal fusion transformer for end-to-end autonomous driving. In CVPR, 2021. 1, 3 [30] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang.

Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario. In AAAI, 2024. 4 [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:53728â€“53741, 2023. 2, 3, 4 [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 4 [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3, 4 [34] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li.

Drivelm: Driving with graph visual question answering. arXiv preprint arXiv:2312.14150, 2023. 2, 4 [35] Maciej Â´Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek MaÂ´ndziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 56(3):2497â€“2562, 2023. 3 [36] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al.

Stanley: The robot that won the darpa grand challenge. Journal of field Robotics, 2006. 3 [37] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024.

4 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÂ´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3 [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.

3 [40] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 7 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language modelâ€™s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.

3, 6, 7 [42] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024. 4 [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.

Advances in neural information processing systems, 35:24824â€“24837, 2022. 3 [44] Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et al. Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293, 2024. 4 [45] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen.

Language prompt for autonomous driving. arXiv preprint arXiv:2309.04379, 2023. 4 [46] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:41618â€“41650, 2023. 3 [47] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao.

Drivegpt4: Interpretable end-to-end autonomous driving via large language model. arXiv preprint arXiv:2310.01412, 2023. 2, 4 [48] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1 [49] Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, and Hongyang Li.

Embodied understanding of driving scenarios. arXiv preprint arXiv:2403.04593, 2024. 2, 4</pre></div></div><script>const API_BASE = "";let isFeedbackMode = false;function toggleFeedbackMode() { isFeedbackMode = !isFeedbackMode; document.body.classList.toggle('feedback-mode'); const toggleBtn = document.getElementById('toggle-btn'); const runBtn = document.getElementById('run-btn'); const statusText = document.getElementById('status-text'); if (isFeedbackMode) { toggleBtn.textContent = "é€€å‡ºçº é”™æ¨¡å¼"; toggleBtn.classList.replace('btn-primary', 'btn-danger'); runBtn.style.display = 'block'; statusText.textContent = "âœï¸ ç‚¹å‡»è¯‘æ–‡ä¿®æ”¹ï¼Œè‡ªåŠ¨ä¿å­˜"; enableClickHandlers(); } else { toggleBtn.textContent = "è¿›å…¥çº é”™æ¨¡å¼"; toggleBtn.classList.replace('btn-danger', 'btn-primary'); runBtn.style.display = 'none'; statusText.textContent = "æµè§ˆæ¨¡å¼"; disableClickHandlers(); } }function enableClickHandlers() { const rows = document.querySelectorAll('.row-container'); rows.forEach(row => { const transCol = row.querySelector('.col-trans'); if (transCol.getAttribute('data-bound')) return; transCol.setAttribute('data-bound', 'true'); transCol.onclick = () => { if (!isFeedbackMode) return; const panel = row.querySelector('.feedback-panel'); const isHidden = (panel.style.display === 'none' || panel.style.display === ''); panel.style.display = isHidden ? 'block' : 'none'; }; }); }function disableClickHandlers() { const panels = document.querySelectorAll('.feedback-panel'); panels.forEach(p => p.style.display = 'none'); }async function saveFeedback(taskId, btnElement) { const container = document.getElementById('task-' + taskId); const input = container.querySelector('.feedback-input'); const hint = input.value.trim(); const statusMsg = container.querySelector('.status-saved'); if (!hint) { alert("è¯·è¾“å…¥æç¤º"); return; } const originalText = btnElement.textContent; btnElement.disabled = true; btnElement.textContent = "ä¿å­˜ä¸­..."; try { const response = await fetch(API_BASE + '/update_task', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ id: taskId, hint: hint }) }); const data = await response.json(); if (data.status === 'success') { statusMsg.style.display = 'inline'; setTimeout(() => statusMsg.style.display = 'none', 2000); btnElement.textContent = "å·²ä¿å­˜ (å¾…é‡è¯‘)"; } else { alert("ä¿å­˜å¤±è´¥: " + data.msg); btnElement.textContent = originalText; btnElement.disabled = false; } } catch (err) { alert("è¿æ¥é”™è¯¯: " + err); btnElement.textContent = originalText; btnElement.disabled = false; } }async function triggerRerun() { if (!confirm("ç¡®å®šè¦é‡è¯‘å—ï¼Ÿ")) return; const mask = document.getElementById('loading-mask'); mask.style.display = 'flex'; try { const response = await fetch(API_BASE + '/trigger_rerun', { method: 'POST' }); const data = await response.json(); if (data.status === 'success') { alert(data.msg); location.reload(); } else { alert("å¤±è´¥: " + data.msg); mask.style.display = 'none'; } } catch (err) { alert("é”™è¯¯: " + err); mask.style.display = 'none'; } }</script></body></html>